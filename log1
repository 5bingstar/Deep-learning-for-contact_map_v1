nohup: ignoring input
/home/zhangqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/zhangqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2017-10-17 20:34:19.312405: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-17 20:34:19.312422: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-17 20:34:19.312426: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-10-17 20:34:19.312429: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-17 20:34:19.312431: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-10-17 20:34:20.447630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-10-17 20:34:20.447913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 10.91GiB
Free memory: 10.75GiB
2017-10-17 20:34:20.447923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-10-17 20:34:20.447927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-10-17 20:34:20.447932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
[None, None, None, 143]
step 0, loss = 52938.2
training 1, accuracy = 0.061404
step 1, loss = 46818.5
step 2, loss = 41824.7
step 3, loss = 37677.1
step 4, loss = 34180.2
step 5, loss = 31196.3
step 6, loss = 28624.4
step 7, loss = 26382.5
step 8, loss = 24420.4
step 9, loss = 22686.2
step 10, loss = 21140.3
training 11, accuracy = 0.000000
step 11, loss = 19763.1
step 12, loss = 18524.4
step 13, loss = 17406.6
step 14, loss = 16393.9
step 15, loss = 15472.2
step 16, loss = 14631.9
step 17, loss = 13860.6
step 18, loss = 13152.9
step 19, loss = 12501.1
step 20, loss = 11903.8
training 21, accuracy = 0.375000
step 21, loss = 11342.2
step 22, loss = 10824.7
step 23, loss = 10347.5
step 24, loss = 9895.83
step 25, loss = 9477.94
step 26, loss = 9087.38
step 27, loss = 8720.22
step 28, loss = 8376.66
step 29, loss = 8053.56
step 30, loss = 7749.86
training 31, accuracy = 0.082474
step 31, loss = 7464.84
step 32, loss = 7192.66
step 33, loss = 6937.59
step 34, loss = 6697.05
step 35, loss = 6469.36
step 36, loss = 6250.53
step 37, loss = 6045.91
step 38, loss = 5851.06
step 39, loss = 5664.09
step 40, loss = 5488.27
training 41, accuracy = 0.078431
step 41, loss = 5318.83
step 42, loss = 5158.59
step 43, loss = 5004.78
step 44, loss = 4858.74
step 45, loss = 4719.68
step 46, loss = 4585.34
step 47, loss = 4458.37
step 48, loss = 4336.46
step 49, loss = 4217.57
step 50, loss = 4105.41
training 51, accuracy = 0.030303
step 51, loss = 3997.86
step 52, loss = 3893.31
step 53, loss = 3793.92
step 54, loss = 3698.34
step 55, loss = 3607.24
step 56, loss = 3517.68
step 57, loss = 3432.16
step 58, loss = 3350.95
step 59, loss = 3270.79
step 60, loss = 3195.18
training 61, accuracy = 0.074627
step 61, loss = 3120.32
step 62, loss = 3049.58
step 63, loss = 2981.84
step 64, loss = 2915.58
step 65, loss = 2851.43
step 66, loss = 2788.59
step 67, loss = 2730.77
step 68, loss = 2671.7
step 69, loss = 2614.24
step 70, loss = 2560.41
training 71, accuracy = 0.042169
step 71, loss = 2507.78
step 72, loss = 2457.13
step 73, loss = 2407.45
step 74, loss = 2359.72
step 75, loss = 2314.24
step 76, loss = 2268.6
step 77, loss = 2224.33
step 78, loss = 2182.61
step 79, loss = 2140.61
step 80, loss = 2101.16
training 81, accuracy = 0.057971
step 81, loss = 2062.38
step 82, loss = 2024.02
step 83, loss = 1987.05
step 84, loss = 1950.97
step 85, loss = 1916.28
step 86, loss = 1883.63
step 87, loss = 1849.73
step 88, loss = 1817.38
step 89, loss = 1786.22
step 90, loss = 1757.48
training 91, accuracy = 0.161290
step 91, loss = 1725.84
step 92, loss = 1696.79
step 93, loss = 1670.49
step 94, loss = 1641.66
step 95, loss = 1617.23
step 96, loss = 1589.41
step 97, loss = 1563.25
step 98, loss = 1537.98
step 99, loss = 1514.28
step 100, loss = 1490.22
training 101, accuracy = 0.007812
step 101, loss = 1467.02
step 102, loss = 1444.96
step 103, loss = 1422.05
step 104, loss = 1401.04
step 105, loss = 1380.86
step 106, loss = 1359.56
step 107, loss = 1339.12
step 108, loss = 1319.2
step 109, loss = 1302.6
step 110, loss = 1281.22
training 111, accuracy = 0.041420
step 111, loss = 1263.14
step 112, loss = 1245.33
step 113, loss = 1226.96
step 114, loss = 1210.54
step 115, loss = 1192.93
step 116, loss = 1176.87
step 117, loss = 1160.66
step 118, loss = 1144.5
step 119, loss = 1129.21
step 120, loss = 1117.41
training 121, accuracy = 0.263158
step 121, loss = 1099.94
step 122, loss = 1084.43
step 123, loss = 1070.24
step 124, loss = 1055.89
step 125, loss = 1043.1
step 126, loss = 1029.95
step 127, loss = 1016.5
step 128, loss = 1003.5
step 129, loss = 990.02
step 130, loss = 977.677
training 131, accuracy = 0.070866
step 131, loss = 965.605
step 132, loss = 953.917
step 133, loss = 942.268
step 134, loss = 929.788
step 135, loss = 918.803
step 136, loss = 907.451
step 137, loss = 899.74
step 138, loss = 886.705
step 139, loss = 878.895
step 140, loss = 864.912
training 141, accuracy = 0.040230
step 141, loss = 855.198
step 142, loss = 845.221
step 143, loss = 836.133
step 144, loss = 825.444
step 145, loss = 815.709
step 146, loss = 807.563
step 147, loss = 798.062
step 148, loss = 788.509
step 149, loss = 780.816
step 150, loss = 771.793
training 151, accuracy = 0.088235
step 151, loss = 762.367
step 152, loss = 754.361
step 153, loss = 746.062
step 154, loss = 738.049
step 155, loss = 730.012
step 156, loss = 723.041
step 157, loss = 715.287
step 158, loss = 706.919
step 159, loss = 700.89
step 160, loss = 691.666
training 161, accuracy = 0.013423
step 161, loss = 685.036
step 162, loss = 677.221
step 163, loss = 671.372
step 164, loss = 664.98
step 165, loss = 657.257
step 166, loss = 650.816
step 167, loss = 643.475
step 168, loss = 636.745
step 169, loss = 630.307
step 170, loss = 624.177
training 171, accuracy = 0.169014
step 171, loss = 618.023
step 172, loss = 611.465
step 173, loss = 605.515
step 174, loss = 600.58
step 175, loss = 594.778
step 176, loss = 588.679
step 177, loss = 582.417
step 178, loss = 577.391
step 179, loss = 572.06
step 180, loss = 565.667
training 181, accuracy = 0.011527
step 181, loss = 560.299
step 182, loss = 555.375
step 183, loss = 549.95
step 184, loss = 546.71
step 185, loss = 540.716
step 186, loss = 536.664
step 187, loss = 530.176
step 188, loss = 526.222
step 189, loss = 520.258
step 190, loss = 515.693
training 191, accuracy = 0.061350
step 191, loss = 511.494
step 192, loss = 506.532
step 193, loss = 502.389
step 194, loss = 497.427
step 195, loss = 493.339
step 196, loss = 489.448
step 197, loss = 485.017
step 198, loss = 480.314
step 199, loss = 477.142
step 200, loss = 472.537
training 201, accuracy = 0.028169
step 201, loss = 470.157
step 202, loss = 470.099
step 203, loss = 460.053
step 204, loss = 456.105
step 205, loss = 452.523
step 206, loss = 448.221
step 207, loss = 444.949
step 208, loss = 441.848
step 209, loss = 437.417
step 210, loss = 434.468
training 211, accuracy = 0.000000
step 211, loss = 429.965
step 212, loss = 426.383
step 213, loss = 423.02
step 214, loss = 420.516
step 215, loss = 418.553
step 216, loss = 414.063
step 217, loss = 410.72
step 218, loss = 406.538
step 219, loss = 402.516
step 220, loss = 400.927
training 221, accuracy = 0.148148
step 221, loss = 396.408
step 222, loss = 393.577
step 223, loss = 390.945
step 224, loss = 387.396
step 225, loss = 384.201
step 226, loss = 381.335
step 227, loss = 378.471
step 228, loss = 375.217
step 229, loss = 372.807
step 230, loss = 370.33
training 231, accuracy = 0.102941
step 231, loss = 366.529
step 232, loss = 363.832
step 233, loss = 361.913
step 234, loss = 359.801
step 235, loss = 359.356
step 236, loss = 353.198
step 237, loss = 351.218
step 238, loss = 349.625
step 239, loss = 346.066
step 240, loss = 342.861
training 241, accuracy = 0.034286
step 241, loss = 340.419
step 242, loss = 338.658
step 243, loss = 335.257
step 244, loss = 333.21
step 245, loss = 330.594
step 246, loss = 328.354
step 247, loss = 326.442
step 248, loss = 325.652
step 249, loss = 321.794
step 250, loss = 319.247
training 251, accuracy = 0.039216
step 251, loss = 316.733
step 252, loss = 314.548
step 253, loss = 312.142
step 254, loss = 309.966
step 255, loss = 309.114
step 256, loss = 306.027
step 257, loss = 303.624
step 258, loss = 301.777
step 259, loss = 299.948
step 260, loss = 298.234
training 261, accuracy = 0.214286
step 261, loss = 297.021
step 262, loss = 293.761
step 263, loss = 291.327
step 264, loss = 290.105
step 265, loss = 290.103
step 266, loss = 286.708
step 267, loss = 285.06
step 268, loss = 282.75
step 269, loss = 279.802
step 270, loss = 278.721
training 271, accuracy = 0.079365
step 271, loss = 276.219
step 272, loss = 275.69
step 273, loss = 273.164
step 274, loss = 271.151
step 275, loss = 269.923
step 276, loss = 267.761
step 277, loss = 265.92
step 278, loss = 263.967
step 279, loss = 262.447
step 280, loss = 260.856
training 281, accuracy = 0.158537
step 281, loss = 258.643
step 282, loss = 257.665
step 283, loss = 256.131
step 284, loss = 255.199
step 285, loss = 253.766
step 286, loss = 250.818
step 287, loss = 249.458
step 288, loss = 247.669
step 289, loss = 246.946
step 290, loss = 248.991
training 291, accuracy = 0.333333
step 291, loss = 243.538
step 292, loss = 241.608
step 293, loss = 239.999
step 294, loss = 238.757
step 295, loss = 236.796
step 296, loss = 236.169
step 297, loss = 234.187
step 298, loss = 233.022
step 299, loss = 231.376
step 300, loss = 230.526
training 301, accuracy = 0.188889
step 301, loss = 229.67
step 302, loss = 227.62
step 303, loss = 226.567
step 304, loss = 229.845
step 305, loss = 224.226
step 306, loss = 222.827
step 307, loss = 221.146
step 308, loss = 219.77
step 309, loss = 218.864
step 310, loss = 216.879
training 311, accuracy = 0.180645
step 311, loss = 215.674
step 312, loss = 214.692
step 313, loss = 213.808
step 314, loss = 212.231
step 315, loss = 210.718
step 316, loss = 209.679
step 317, loss = 208.173
step 318, loss = 207.803
step 319, loss = 205.828
step 320, loss = 204.76
training 321, accuracy = 0.073034
step 321, loss = 203.42
step 322, loss = 202.335
step 323, loss = 201.671
step 324, loss = 202.915
step 325, loss = 199.245
step 326, loss = 198.178
step 327, loss = 196.896
step 328, loss = 196.342
step 329, loss = 194.762
step 330, loss = 194.277
training 331, accuracy = 0.346939
step 331, loss = 192.871
step 332, loss = 191.48
step 333, loss = 190.606
step 334, loss = 189.46
step 335, loss = 189.19
step 336, loss = 187.903
step 337, loss = 186.877
step 338, loss = 185.506
step 339, loss = 185.054
step 340, loss = 186.728
training 341, accuracy = 0.136364
step 341, loss = 182.857
step 342, loss = 182.08
step 343, loss = 180.405
step 344, loss = 179.946
step 345, loss = 178.775
step 346, loss = 177.786
step 347, loss = 177.312
step 348, loss = 175.875
step 349, loss = 174.995
step 350, loss = 174.141
training 351, accuracy = 0.061404
step 351, loss = 173.73
step 352, loss = 172.821
step 353, loss = 171.569
step 354, loss = 170.408
step 355, loss = 169.545
step 356, loss = 168.924
step 357, loss = 169.368
step 358, loss = 167.562
step 359, loss = 166.511
step 360, loss = 165.156
training 361, accuracy = 0.016043
step 361, loss = 164.84
step 362, loss = 163.613
step 363, loss = 162.75
step 364, loss = 162.012
step 365, loss = 161.056
step 366, loss = 160.66
step 367, loss = 159.442
step 368, loss = 158.578
step 369, loss = 157.855
step 370, loss = 157.557
training 371, accuracy = 0.034483
step 371, loss = 156.363
step 372, loss = 155.882
step 373, loss = 154.994
step 374, loss = 154.065
step 375, loss = 154.562
step 376, loss = 152.475
step 377, loss = 152.109
step 378, loss = 152.877
step 379, loss = 150.47
step 380, loss = 150.142
training 381, accuracy = 0.040816
step 381, loss = 149.102
step 382, loss = 148.65
step 383, loss = 147.37
step 384, loss = 146.805
step 385, loss = 146.25
step 386, loss = 145.343
step 387, loss = 145.202
step 388, loss = 144.667
step 389, loss = 143.685
step 390, loss = 142.841
training 391, accuracy = 0.129032
step 391, loss = 142.363
step 392, loss = 141.842
step 393, loss = 140.958
step 394, loss = 140.287
step 395, loss = 145.721
step 396, loss = 138.762
step 397, loss = 138.114
step 398, loss = 138.147
step 399, loss = 137.119
step 400, loss = 136.842
training 401, accuracy = 0.090909
step 401, loss = 135.72
step 402, loss = 135.063
step 403, loss = 134.393
step 404, loss = 133.786
step 405, loss = 133.069
step 406, loss = 133.018
step 407, loss = 132.38
step 408, loss = 131.359
step 409, loss = 130.625
step 410, loss = 130.187
training 411, accuracy = 0.625000
step 411, loss = 129.845
step 412, loss = 128.927
step 413, loss = 128.256
step 414, loss = 128.217
step 415, loss = 127.272
step 416, loss = 126.777
step 417, loss = 126.502
step 418, loss = 126.022
step 419, loss = 125.002
step 420, loss = 124.419
training 421, accuracy = 0.247788
step 421, loss = 123.952
step 422, loss = 123.503
step 423, loss = 122.857
step 424, loss = 122.195
step 425, loss = 122.088
step 426, loss = 124.84
step 427, loss = 121.024
step 428, loss = 120.322
step 429, loss = 119.889
step 430, loss = 119.594
training 431, accuracy = 0.536585
step 431, loss = 118.641
step 432, loss = 118.5
step 433, loss = 117.882
step 434, loss = 117.058
step 435, loss = 116.599
step 436, loss = 116.158
step 437, loss = 115.647
step 438, loss = 116.069
step 439, loss = 114.92
step 440, loss = 114.113
training 441, accuracy = 0.115556
step 441, loss = 113.676
step 442, loss = 113.449
step 443, loss = 112.911
step 444, loss = 112.194
step 445, loss = 111.835
step 446, loss = 111.297
step 447, loss = 110.902
step 448, loss = 110.55
step 449, loss = 110.089
step 450, loss = 109.413
training 451, accuracy = 0.076687
step 451, loss = 109.425
step 452, loss = 108.739
step 453, loss = 108.835
step 454, loss = 107.696
step 455, loss = 108.406
step 456, loss = 107.106
step 457, loss = 106.52
step 458, loss = 106.278
step 459, loss = 106.262
step 460, loss = 105.447
training 461, accuracy = 0.094595
step 461, loss = 104.815
step 462, loss = 104.541
step 463, loss = 104.452
step 464, loss = 103.605
step 465, loss = 103.104
step 466, loss = 102.69
step 467, loss = 102.297
step 468, loss = 102.315
step 469, loss = 101.623
step 470, loss = 101.155
training 471, accuracy = 0.560345
step 471, loss = 100.879
step 472, loss = 100.232
step 473, loss = 99.8318
step 474, loss = 99.5137
step 475, loss = 100.551
step 476, loss = 98.7708
step 477, loss = 98.686
step 478, loss = 97.9121
step 479, loss = 97.6135
step 480, loss = 97.5724
training 481, accuracy = 0.155172
step 481, loss = 96.9798
step 482, loss = 96.8418
step 483, loss = 96.2682
step 484, loss = 95.7452
step 485, loss = 96.1348
step 486, loss = 95.1273
step 487, loss = 99.5721
step 488, loss = 94.3963
step 489, loss = 94.1017
step 490, loss = 93.6537
training 491, accuracy = 0.015385
step 491, loss = 93.2746
step 492, loss = 92.7858
step 493, loss = 92.4756
step 494, loss = 92.506
step 495, loss = 91.9599
step 496, loss = 91.4361
step 497, loss = 91.2044
step 498, loss = 91.126
step 499, loss = 90.4063
step 500, loss = 90.4417
training 501, accuracy = 0.295082
step 501, loss = 90.0051
step 502, loss = 89.4694
step 503, loss = 89.4069
step 504, loss = 88.7256
step 505, loss = 88.5349
step 506, loss = 93.4152
step 507, loss = 88.8926
step 508, loss = 88.2125
step 509, loss = 87.0944
step 510, loss = 87.6319
training 511, accuracy = 0.306122
step 511, loss = 87.2641
step 512, loss = 86.2454
step 513, loss = 86.0106
step 514, loss = 85.9542
step 515, loss = 85.3373
step 516, loss = 84.8481
step 517, loss = 84.896
step 518, loss = 84.5399
step 519, loss = 84.34
step 520, loss = 84.0693
training 521, accuracy = 0.169231
step 521, loss = 83.2925
step 522, loss = 85.6699
step 523, loss = 82.9987
step 524, loss = 82.4982
step 525, loss = 82.5924
step 526, loss = 81.9746
step 527, loss = 81.9434
step 528, loss = 81.6039
step 529, loss = 81.062
step 530, loss = 80.8997
training 531, accuracy = 0.000000
step 531, loss = 80.8512
step 532, loss = 81.4223
step 533, loss = 80.3629
step 534, loss = 79.9063
step 535, loss = 79.3261
step 536, loss = 79.2981
step 537, loss = 78.7363
step 538, loss = 78.523
step 539, loss = 78.5739
step 540, loss = 83.5227
training 541, accuracy = 0.600000
step 541, loss = 78.8018
step 542, loss = 77.6371
step 543, loss = 77.5866
step 544, loss = 77.3056
step 545, loss = 77.2409
step 546, loss = 76.8639
step 547, loss = 76.3295
step 548, loss = 76.2484
step 549, loss = 75.5238
step 550, loss = 75.6156
training 551, accuracy = 0.014286
step 551, loss = 75.3506
step 552, loss = 75.3083
step 553, loss = 74.6062
step 554, loss = 74.6079
step 555, loss = 74.0538
step 556, loss = 74.1692
step 557, loss = 74.4763
step 558, loss = 73.9004
step 559, loss = 73.1665
step 560, loss = 72.9754
training 561, accuracy = 0.354167
step 561, loss = 72.8525
step 562, loss = 72.3573
step 563, loss = 72.5422
step 564, loss = 71.9362
step 565, loss = 71.7738
step 566, loss = 71.6111
step 567, loss = 71.9028
step 568, loss = 70.9001
step 569, loss = 71.0041
step 570, loss = 70.4037
training 571, accuracy = 0.273469
step 571, loss = 70.4723
step 572, loss = 70.1683
step 573, loss = 70.1531
step 574, loss = 69.9389
step 575, loss = 69.7774
step 576, loss = 69.1246
step 577, loss = 69.2119
step 578, loss = 68.5957
step 579, loss = 68.6041
step 580, loss = 68.2495
training 581, accuracy = 0.096000
step 581, loss = 67.9796
step 582, loss = 67.8545
step 583, loss = 67.56
step 584, loss = 68.9536
step 585, loss = 67.2886
step 586, loss = 67.1707
step 587, loss = 67.1088
step 588, loss = 66.4723
step 589, loss = 67.2581
step 590, loss = 66.5011
training 591, accuracy = 0.000000
step 591, loss = 65.909
step 592, loss = 65.9235
step 593, loss = 66.3622
step 594, loss = 65.4661
step 595, loss = 65.1312
step 596, loss = 64.7171
step 597, loss = 64.9011
step 598, loss = 64.5482
step 599, loss = 64.409
step 600, loss = 64.2607
training 601, accuracy = 0.025000
step 601, loss = 63.8151
step 602, loss = 63.5412
step 603, loss = 63.3525
step 604, loss = 63.1084
step 605, loss = 62.96
step 606, loss = 62.8065
step 607, loss = 62.6739
step 608, loss = 62.2936
step 609, loss = 62.1204
step 610, loss = 62.1114
training 611, accuracy = 0.442748
step 611, loss = 61.7498
step 612, loss = 61.9289
step 613, loss = 61.434
step 614, loss = 61.2602
step 615, loss = 61.0755
step 616, loss = 60.8554
step 617, loss = 60.9403
step 618, loss = 60.6303
step 619, loss = 60.5345
step 620, loss = 60.99
training 621, accuracy = 0.230769
step 621, loss = 59.9303
step 622, loss = 59.7374
step 623, loss = 59.6263
step 624, loss = 59.4001
step 625, loss = 59.2648
step 626, loss = 59.4446
step 627, loss = 58.9768
step 628, loss = 58.6338
step 629, loss = 58.8658
step 630, loss = 58.3012
training 631, accuracy = 0.101961
step 631, loss = 58.5196
step 632, loss = 58.161
step 633, loss = 58.5257
step 634, loss = 57.9843
step 635, loss = 57.4554
step 636, loss = 57.4367
step 637, loss = 57.1758
step 638, loss = 57.3055
step 639, loss = 57.4236
step 640, loss = 57.2129
training 641, accuracy = 0.160000
step 641, loss = 57.0236
step 642, loss = 56.6969
step 643, loss = 56.6287
step 644, loss = 56.2674
step 645, loss = 55.9598
step 646, loss = 55.6974
step 647, loss = 55.6999
step 648, loss = 55.2382
step 649, loss = 55.2138
step 650, loss = 55.1549
training 651, accuracy = 0.200000
step 651, loss = 54.9224
step 652, loss = 54.7399
step 653, loss = 54.6109
step 654, loss = 54.4194
step 655, loss = 54.5282
step 656, loss = 55.3852
step 657, loss = 54.1104
step 658, loss = 54.0081
step 659, loss = 53.7936
step 660, loss = 53.7689
training 661, accuracy = 0.014286
step 661, loss = 53.3293
step 662, loss = 53.3214
step 663, loss = 52.9827
step 664, loss = 52.8602
step 665, loss = 52.7498
step 666, loss = 52.8116
step 667, loss = 52.6583
step 668, loss = 52.2761
step 669, loss = 52.618
step 670, loss = 51.9668
training 671, accuracy = 0.300971
step 671, loss = 51.9421
step 672, loss = 51.7122
step 673, loss = 51.5011
step 674, loss = 51.5699
step 675, loss = 51.2972
step 676, loss = 56.0422
step 677, loss = 51.0121
step 678, loss = 51.3551
step 679, loss = 51.1986
step 680, loss = 50.8033
training 681, accuracy = 0.321429
step 681, loss = 50.5584
step 682, loss = 50.194
step 683, loss = 50.1867
step 684, loss = 49.9714
step 685, loss = 49.8894
step 686, loss = 49.8631
step 687, loss = 49.5382
step 688, loss = 49.4604
step 689, loss = 49.5768
step 690, loss = 49.7191
training 691, accuracy = 0.470588
step 691, loss = 49.4861
step 692, loss = 48.8369
step 693, loss = 48.8939
step 694, loss = 48.9234
step 695, loss = 48.5818
step 696, loss = 48.5603
step 697, loss = 48.6825
step 698, loss = 48.1131
step 699, loss = 48.5074
step 700, loss = 47.9676
training 701, accuracy = 0.138889
step 701, loss = 47.7041
step 702, loss = 47.7725
step 703, loss = 47.6061
step 704, loss = 47.636
step 705, loss = 47.3001
step 706, loss = 47.1245
step 707, loss = 47.584
step 708, loss = 47.3243
step 709, loss = 46.7852
step 710, loss = 46.7081
training 711, accuracy = 0.373832
step 711, loss = 46.7492
step 712, loss = 46.7864
step 713, loss = 46.3437
step 714, loss = 47.9389
step 715, loss = 46.1718
step 716, loss = 46.0375
step 717, loss = 45.8105
step 718, loss = 45.5102
step 719, loss = 45.3928
step 720, loss = 45.3683
training 721, accuracy = 0.059880
step 721, loss = 45.3326
step 722, loss = 45.1805
step 723, loss = 45.0927
step 724, loss = 45.028
step 725, loss = 44.7409
step 726, loss = 44.7496
step 727, loss = 44.8229
step 728, loss = 44.8053
step 729, loss = 49.0868
step 730, loss = 44.3397
training 731, accuracy = 0.044000
step 731, loss = 44.1395
step 732, loss = 44.4792
step 733, loss = 43.885
step 734, loss = 44.2908
step 735, loss = 43.844
step 736, loss = 43.8354
step 737, loss = 43.5989
step 738, loss = 43.8142
step 739, loss = 43.2129
step 740, loss = 43.2633
training 741, accuracy = 0.472973
step 741, loss = 43.1462
step 742, loss = 43.074
step 743, loss = 42.8881
step 744, loss = 43.3825
step 745, loss = 42.5559
step 746, loss = 43.8708
step 747, loss = 42.5139
step 748, loss = 42.443
step 749, loss = 42.3266
step 750, loss = 41.9513
training 751, accuracy = 0.078818
step 751, loss = 42.0488
step 752, loss = 41.7341
step 753, loss = 41.6381
step 754, loss = 41.8455
step 755, loss = 44.0852
step 756, loss = 41.9397
step 757, loss = 41.506
step 758, loss = 41.1965
step 759, loss = 41.0687
step 760, loss = 40.9909
training 761, accuracy = 0.362903
step 761, loss = 41.0216
step 762, loss = 40.8572
step 763, loss = 40.6476
step 764, loss = 40.5202
step 765, loss = 41.452
step 766, loss = 40.3894
step 767, loss = 40.4946
step 768, loss = 40.21
step 769, loss = 40.1222
step 770, loss = 40.0259
training 771, accuracy = 0.493506
step 771, loss = 39.9012
step 772, loss = 40.7881
step 773, loss = 39.6383
step 774, loss = 39.8651
step 775, loss = 39.5451
step 776, loss = 39.3727
step 777, loss = 40.4621
step 778, loss = 40.8477
step 779, loss = 39.2428
step 780, loss = 39.6881
training 781, accuracy = 0.189189
step 781, loss = 39.0299
step 782, loss = 38.9297
step 783, loss = 38.7511
step 784, loss = 38.5849
step 785, loss = 38.746
step 786, loss = 38.3938
step 787, loss = 38.3061
step 788, loss = 38.2053
step 789, loss = 38.665
step 790, loss = 38.0166
training 791, accuracy = 0.113122
step 791, loss = 37.9157
step 792, loss = 38.3187
step 793, loss = 37.9134
step 794, loss = 37.7239
step 795, loss = 37.792
step 796, loss = 37.7454
step 797, loss = 37.6844
step 798, loss = 37.4422
step 799, loss = 37.8129
step 800, loss = 37.0343
training 801, accuracy = 0.383562
step 801, loss = 37.093
step 802, loss = 36.9684
step 803, loss = 36.9657
step 804, loss = 37.0171
step 805, loss = 36.6975
step 806, loss = 37.0664
step 807, loss = 36.8834
step 808, loss = 36.6089
step 809, loss = 36.3487
step 810, loss = 36.4752
training 811, accuracy = 0.507246
step 811, loss = 36.2444
step 812, loss = 36.1063
step 813, loss = 36.7356
step 814, loss = 36.3448
step 815, loss = 35.8897
step 816, loss = 35.6861
step 817, loss = 35.7473
step 818, loss = 35.639
step 819, loss = 35.4763
step 820, loss = 35.726
training 821, accuracy = 0.095238
step 821, loss = 35.4688
step 822, loss = 35.527
step 823, loss = 35.3284
step 824, loss = 35.9762
step 825, loss = 35.1273
step 826, loss = 36.0713
step 827, loss = 34.9498
step 828, loss = 35.2042
step 829, loss = 35.0766
step 830, loss = 34.7892
training 831, accuracy = 0.060976
step 831, loss = 34.6814
step 832, loss = 34.7048
step 833, loss = 38.9017
step 834, loss = 34.7399
step 835, loss = 34.7895
step 836, loss = 34.2633
step 837, loss = 34.1661
step 838, loss = 34.3626
step 839, loss = 34.0404
step 840, loss = 33.8747
training 841, accuracy = 0.176471
step 841, loss = 33.8557
step 842, loss = 34.1399
step 843, loss = 33.9469
step 844, loss = 33.5828
step 845, loss = 33.6361
step 846, loss = 33.4567
step 847, loss = 33.5025
step 848, loss = 33.3835
step 849, loss = 33.7029
step 850, loss = 33.2994
training 851, accuracy = 0.308642
step 851, loss = 33.0362
step 852, loss = 35.1463
step 853, loss = 33.6941
step 854, loss = 33.2602
step 855, loss = 33.429
step 856, loss = 32.9643
step 857, loss = 32.8684
step 858, loss = 32.6031
step 859, loss = 32.8787
step 860, loss = 32.5024
training 861, accuracy = 0.228070
step 861, loss = 32.5121
step 862, loss = 32.6592
step 863, loss = 33.088
step 864, loss = 32.2878
step 865, loss = 32.6483
step 866, loss = 31.9975
step 867, loss = 31.9091
step 868, loss = 31.8857
step 869, loss = 31.8929
step 870, loss = 31.8333
training 871, accuracy = 0.129870
step 871, loss = 31.5277
step 872, loss = 32.7702
step 873, loss = 31.5852
step 874, loss = 32.0491
step 875, loss = 31.6577
step 876, loss = 31.3005
step 877, loss = 31.8272
step 878, loss = 31.6898
step 879, loss = 31.2422
step 880, loss = 31.3358
training 881, accuracy = 0.086420
step 881, loss = 31.0985
step 882, loss = 30.8344
step 883, loss = 32.9366
step 884, loss = 30.7162
step 885, loss = 30.6724
step 886, loss = 30.5678
step 887, loss = 30.5536
step 888, loss = 30.4327
step 889, loss = 30.6063
step 890, loss = 30.2397
training 891, accuracy = 0.073099
step 891, loss = 30.3999
step 892, loss = 30.2963
step 893, loss = 29.9989
step 894, loss = 30.0637
step 895, loss = 30.0245
step 896, loss = 31.6804
step 897, loss = 31.0451
step 898, loss = 29.937
step 899, loss = 29.9489
step 900, loss = 29.7516
training 901, accuracy = 0.172840
step 901, loss = 30.9087
step 902, loss = 29.5739
step 903, loss = 29.69
step 904, loss = 29.4415
step 905, loss = 29.528
step 906, loss = 29.3846
step 907, loss = 29.4938
step 908, loss = 29.3395
step 909, loss = 29.0965
step 910, loss = 29.0619
training 911, accuracy = 0.149606
step 911, loss = 29.2105
step 912, loss = 28.9951
step 913, loss = 29.1616
step 914, loss = 28.9595
step 915, loss = 28.7448
step 916, loss = 28.6634
step 917, loss = 28.6852
step 918, loss = 28.6369
step 919, loss = 28.5256
step 920, loss = 28.4568
training 921, accuracy = 0.263566
step 921, loss = 28.567
step 922, loss = 28.3722
step 923, loss = 28.3663
step 924, loss = 28.3182
step 925, loss = 28.1294
step 926, loss = 28.055
step 927, loss = 27.9787
step 928, loss = 28.1015
step 929, loss = 31.4006
step 930, loss = 27.9508
training 931, accuracy = 0.107570
step 931, loss = 28.1286
step 932, loss = 27.8217
step 933, loss = 28.3806
step 934, loss = 27.6746
step 935, loss = 27.7582
step 936, loss = 27.6225
step 937, loss = 28.2272
step 938, loss = 27.8362
step 939, loss = 27.5157
step 940, loss = 27.5757
training 941, accuracy = 0.216216
step 941, loss = 27.6145
step 942, loss = 27.4008
step 943, loss = 27.1885
step 944, loss = 27.3451
step 945, loss = 27.0645
step 946, loss = 27.0329
step 947, loss = 27.1608
step 948, loss = 26.817
step 949, loss = 26.8381
step 950, loss = 26.7632
training 951, accuracy = 0.011111
step 951, loss = 26.9358
step 952, loss = 28.0257
step 953, loss = 26.9924
step 954, loss = 28.7361
step 955, loss = 26.6858
step 956, loss = 26.7582
step 957, loss = 26.7502
step 958, loss = 26.499
step 959, loss = 26.5655
step 960, loss = 26.6076
training 961, accuracy = 0.127660
step 961, loss = 26.6571
step 962, loss = 26.1176
step 963, loss = 26.0573
step 964, loss = 26.0969
step 965, loss = 25.9984
step 966, loss = 26.073
step 967, loss = 26.0493
step 968, loss = 26.0935
step 969, loss = 25.7975
step 970, loss = 25.7695
training 971, accuracy = 0.131783
step 971, loss = 25.8414
step 972, loss = 25.7446
step 973, loss = 25.752
step 974, loss = 25.4786
step 975, loss = 25.4247
step 976, loss = 25.5089
step 977, loss = 25.3643
step 978, loss = 25.3544
step 979, loss = 25.3052
step 980, loss = 25.1994
training 981, accuracy = 0.369128
step 981, loss = 25.3157
step 982, loss = 25.2736
step 983, loss = 25.3196
step 984, loss = 25.1343
step 985, loss = 25.1302
step 986, loss = 25.1241
step 987, loss = 25.6464
step 988, loss = 24.9474
step 989, loss = 25.0885
step 990, loss = 25.9287
training 991, accuracy = 0.312500
step 991, loss = 24.6993
step 992, loss = 24.6707
step 993, loss = 24.4943
step 994, loss = 24.4968
step 995, loss = 24.6359
step 996, loss = 24.4272
step 997, loss = 24.5272
step 998, loss = 24.3475
step 999, loss = 24.4261
step 1000, loss = 24.2501
training 1001, accuracy = 0.391304
step 1001, loss = 24.4042
step 1002, loss = 24.3664
step 1003, loss = 24.6964
step 1004, loss = 24.0319
step 1005, loss = 23.967
step 1006, loss = 24.0263
step 1007, loss = 23.9714
step 1008, loss = 23.8956
step 1009, loss = 24.0163
step 1010, loss = 27.7568
training 1011, accuracy = 0.285714
step 1011, loss = 24.9971
step 1012, loss = 23.8865
step 1013, loss = 24.5829
step 1014, loss = 24.0545
step 1015, loss = 23.601
step 1016, loss = 23.4602
step 1017, loss = 23.567
step 1018, loss = 23.6109
step 1019, loss = 23.3989
step 1020, loss = 26.4062
training 1021, accuracy = 0.437500
step 1021, loss = 23.4628
step 1022, loss = 23.4017
step 1023, loss = 23.2355
step 1024, loss = 24.6759
step 1025, loss = 23.2764
step 1026, loss = 23.1569
step 1027, loss = 23.2006
step 1028, loss = 23.2119
step 1029, loss = 23.126
step 1030, loss = 23.1219
training 1031, accuracy = 0.341176
step 1031, loss = 23.0979
step 1032, loss = 24.3465
step 1033, loss = 24.3059
step 1034, loss = 22.6799
step 1035, loss = 23.0348
step 1036, loss = 22.9512
step 1037, loss = 22.6071
step 1038, loss = 22.6417
step 1039, loss = 22.757
step 1040, loss = 22.4491
training 1041, accuracy = 0.028777
step 1041, loss = 22.4919
step 1042, loss = 22.4686
step 1043, loss = 22.3492
step 1044, loss = 22.4308
step 1045, loss = 22.3169
step 1046, loss = 22.3267
step 1047, loss = 24.7585
step 1048, loss = 22.5118
step 1049, loss = 22.2641
step 1050, loss = 22.2944
training 1051, accuracy = 0.818182
step 1051, loss = 22.2741
step 1052, loss = 22.0931
step 1053, loss = 21.9058
step 1054, loss = 22.1515
step 1055, loss = 22.0148
step 1056, loss = 21.8789
step 1057, loss = 22.1078
step 1058, loss = 22.0229
step 1059, loss = 21.9323
step 1060, loss = 21.7355
training 1061, accuracy = 0.195652
step 1061, loss = 21.8073
step 1062, loss = 21.8213
step 1063, loss = 21.6825
step 1064, loss = 21.7512
step 1065, loss = 21.5825
step 1066, loss = 23.5651
step 1067, loss = 22.283
step 1068, loss = 21.4842
step 1069, loss = 21.6488
step 1070, loss = 21.5475
training 1071, accuracy = 0.213483
step 1071, loss = 21.371
step 1072, loss = 21.5644
step 1073, loss = 21.2901
step 1074, loss = 22.3688
step 1075, loss = 21.9272
step 1076, loss = 21.3397
step 1077, loss = 20.9921
step 1078, loss = 20.9395
step 1079, loss = 21.1566
step 1080, loss = 20.9597
training 1081, accuracy = 0.064327
step 1081, loss = 20.8871
step 1082, loss = 20.9639
step 1083, loss = 21.2087
step 1084, loss = 20.8467
step 1085, loss = 20.8535
step 1086, loss = 20.7558
step 1087, loss = 20.7848
step 1088, loss = 20.6578
step 1089, loss = 20.7007
step 1090, loss = 20.5261
training 1091, accuracy = 0.267490
step 1091, loss = 20.5086
step 1092, loss = 20.6665
step 1093, loss = 20.4077
step 1094, loss = 20.5231
step 1095, loss = 20.4427
step 1096, loss = 20.8113
step 1097, loss = 20.5825
step 1098, loss = 20.4625
step 1099, loss = 20.3269
step 1100, loss = 24.8971
training 1101, accuracy = 0.571429
step 1101, loss = 21.3295
step 1102, loss = 22.1619
step 1103, loss = 20.3091
step 1104, loss = 20.1452
step 1105, loss = 20.0251
step 1106, loss = 20.3913
step 1107, loss = 19.9789
step 1108, loss = 20.0166
step 1109, loss = 19.8752
step 1110, loss = 19.937
training 1111, accuracy = 0.115789
step 1111, loss = 20.1115
step 1112, loss = 20.0572
step 1113, loss = 19.9731
step 1114, loss = 20.0713
step 1115, loss = 19.7494
step 1116, loss = 19.7483
step 1117, loss = 19.6532
step 1118, loss = 19.6196
step 1119, loss = 19.694
step 1120, loss = 19.6955
training 1121, accuracy = 0.022727
step 1121, loss = 19.6257
step 1122, loss = 19.7324
step 1123, loss = 19.6459
step 1124, loss = 19.4163
step 1125, loss = 20.4349
step 1126, loss = 19.4928
step 1127, loss = 22.3273
step 1128, loss = 19.5146
step 1129, loss = 19.6036
step 1130, loss = 19.5093
training 1131, accuracy = 0.111111
step 1131, loss = 19.4741
step 1132, loss = 19.4667
step 1133, loss = 19.3222
step 1134, loss = 19.2711
step 1135, loss = 19.0455
step 1136, loss = 19.0188
step 1137, loss = 19.1432
step 1138, loss = 19.1861
step 1139, loss = 19.0604
step 1140, loss = 18.8774
training 1141, accuracy = 0.077844
step 1141, loss = 19.0584
step 1142, loss = 18.757
step 1143, loss = 19.0184
step 1144, loss = 18.7078
step 1145, loss = 18.6928
step 1146, loss = 18.552
step 1147, loss = 19.4762
step 1148, loss = 18.5696
step 1149, loss = 18.8048
step 1150, loss = 18.7255
training 1151, accuracy = 0.204082
step 1151, loss = 18.6219
step 1152, loss = 19.0402
step 1153, loss = 19.7309
step 1154, loss = 18.7068
step 1155, loss = 18.4421
step 1156, loss = 18.4642
step 1157, loss = 18.5063
step 1158, loss = 18.405
step 1159, loss = 18.3708
step 1160, loss = 18.4982
training 1161, accuracy = 0.129032
step 1161, loss = 18.3981
step 1162, loss = 18.3323
step 1163, loss = 18.2855
step 1164, loss = 18.1012
step 1165, loss = 18.2965
step 1166, loss = 18.0914
step 1167, loss = 17.971
step 1168, loss = 18.0859
step 1169, loss = 18.1181
step 1170, loss = 17.9736
training 1171, accuracy = 0.077419
step 1171, loss = 17.9007
step 1172, loss = 18.283
step 1173, loss = 17.9881
step 1174, loss = 18.1818
step 1175, loss = 17.8518
step 1176, loss = 19.1912
step 1177, loss = 17.7919
step 1178, loss = 17.8673
step 1179, loss = 17.921
step 1180, loss = 17.9045
training 1181, accuracy = 0.065574
step 1181, loss = 17.6674
step 1182, loss = 17.5877
step 1183, loss = 17.6143
step 1184, loss = 17.7087
step 1185, loss = 17.58
step 1186, loss = 17.737
step 1187, loss = 17.5673
step 1188, loss = 18.1805
step 1189, loss = 17.5827
step 1190, loss = 17.3665
training 1191, accuracy = 0.122047
step 1191, loss = 17.3702
step 1192, loss = 18.1567
step 1193, loss = 19.4549
step 1194, loss = 20.7161
step 1195, loss = 17.6298
step 1196, loss = 17.5267
step 1197, loss = 17.2692
step 1198, loss = 17.3745
step 1199, loss = 17.3507
step 1200, loss = 17.1644
training 1201, accuracy = 0.140000
step 1201, loss = 17.3133
step 1202, loss = 17.2496
step 1203, loss = 16.9635
step 1204, loss = 17.679
step 1205, loss = 17.0973
step 1206, loss = 17.0796
step 1207, loss = 16.9841
step 1208, loss = 16.9983
step 1209, loss = 16.9383
step 1210, loss = 16.7857
training 1211, accuracy = 0.660920
step 1211, loss = 17.0067
step 1212, loss = 16.7429
step 1213, loss = 16.7289
step 1214, loss = 16.7689
step 1215, loss = 16.7642
step 1216, loss = 16.953
step 1217, loss = 17.042
step 1218, loss = 16.5833
step 1219, loss = 16.6339
step 1220, loss = 16.7177
training 1221, accuracy = 0.097222
step 1221, loss = 16.7035
step 1222, loss = 16.458
step 1223, loss = 16.4441
step 1224, loss = 16.8763
step 1225, loss = 16.5568
step 1226, loss = 16.5597
step 1227, loss = 16.5616
step 1228, loss = 16.5001
step 1229, loss = 16.3023
step 1230, loss = 16.3275
training 1231, accuracy = 0.493333
step 1231, loss = 16.5883
step 1232, loss = 16.3106
step 1233, loss = 16.3856
step 1234, loss = 16.3544
step 1235, loss = 16.1859
step 1236, loss = 19.5316
step 1237, loss = 16.6303
step 1238, loss = 16.9544
step 1239, loss = 16.2627
step 1240, loss = 16.6223
training 1241, accuracy = 0.240741
step 1241, loss = 16.7275
step 1242, loss = 16.7265
step 1243, loss = 16.7445
step 1244, loss = 16.3849
step 1245, loss = 16.1818
step 1246, loss = 15.9125
step 1247, loss = 16.0098
step 1248, loss = 15.9379
step 1249, loss = 15.9702
step 1250, loss = 16.0419
training 1251, accuracy = 0.024691
step 1251, loss = 16.3028
step 1252, loss = 15.8906
step 1253, loss = 16.0321
step 1254, loss = 15.6879
step 1255, loss = 15.722
step 1256, loss = 15.7023
step 1257, loss = 15.7451
step 1258, loss = 15.7778
step 1259, loss = 15.7246
step 1260, loss = 15.5835
training 1261, accuracy = 0.559524
step 1261, loss = 15.6302
step 1262, loss = 15.5482
step 1263, loss = 15.6016
step 1264, loss = 15.5748
step 1265, loss = 15.4882
step 1266, loss = 15.4765
step 1267, loss = 16.7325
step 1268, loss = 15.6138
step 1269, loss = 15.5788
step 1270, loss = 16.3752
training 1271, accuracy = 0.260870
step 1271, loss = 15.4116
step 1272, loss = 15.5203
step 1273, loss = 15.2609
step 1274, loss = 15.4883
step 1275, loss = 15.4302
step 1276, loss = 15.2115
step 1277, loss = 15.2588
step 1278, loss = 15.2319
step 1279, loss = 15.3313
step 1280, loss = 15.2455
training 1281, accuracy = 0.571429
step 1281, loss = 17.6407
step 1282, loss = 16.1918
step 1283, loss = 15.3034
step 1284, loss = 15.215
step 1285, loss = 15.5953
step 1286, loss = 15.4708
step 1287, loss = 15.2525
step 1288, loss = 15.2111
step 1289, loss = 14.9923
step 1290, loss = 15.0877
training 1291, accuracy = 0.127660
step 1291, loss = 14.8873
step 1292, loss = 15.0936
step 1293, loss = 14.7484
step 1294, loss = 14.7778
step 1295, loss = 15.7764
step 1296, loss = 15.0085
step 1297, loss = 15.0119
step 1298, loss = 15.8379
step 1299, loss = 14.7148
step 1300, loss = 14.6873
training 1301, accuracy = 0.066667
step 1301, loss = 14.8175
step 1302, loss = 14.666
step 1303, loss = 14.7349
step 1304, loss = 14.6449
step 1305, loss = 14.5664
step 1306, loss = 15.8914
step 1307, loss = 14.669
step 1308, loss = 15.005
step 1309, loss = 14.5089
step 1310, loss = 14.702
training 1311, accuracy = 0.083333
step 1311, loss = 14.4151
step 1312, loss = 14.9588
step 1313, loss = 14.6502
step 1314, loss = 14.6218
step 1315, loss = 14.6895
step 1316, loss = 14.7686
step 1317, loss = 14.3023
step 1318, loss = 14.9083
step 1319, loss = 14.4858
step 1320, loss = 14.2957
training 1321, accuracy = 0.261538
step 1321, loss = 14.3463
step 1322, loss = 14.214
step 1323, loss = 14.1448
step 1324, loss = 14.1957
step 1325, loss = 14.6334
step 1326, loss = 16.9918
step 1327, loss = 14.3149
step 1328, loss = 14.0736
step 1329, loss = 14.0825
step 1330, loss = 14.2605
training 1331, accuracy = 0.161765
step 1331, loss = 14.6754
step 1332, loss = 14.0181
step 1333, loss = 13.9852
step 1334, loss = 14.1845
step 1335, loss = 15.2174
step 1336, loss = 14.8591
step 1337, loss = 14.4463
step 1338, loss = 14.0268
step 1339, loss = 13.9914
step 1340, loss = 14.4445
training 1341, accuracy = 0.195122
step 1341, loss = 13.8956
step 1342, loss = 14.2931
step 1343, loss = 13.8322
step 1344, loss = 13.9941
step 1345, loss = 13.6935
step 1346, loss = 13.9322
step 1347, loss = 13.7693
step 1348, loss = 13.862
step 1349, loss = 13.7376
step 1350, loss = 13.9323
training 1351, accuracy = 0.050000
step 1351, loss = 13.6213
step 1352, loss = 13.7509
step 1353, loss = 13.5358
step 1354, loss = 13.6462
step 1355, loss = 13.5824
step 1356, loss = 13.6996
step 1357, loss = 14.1873
step 1358, loss = 13.4823
step 1359, loss = 14.3597
step 1360, loss = 13.543
training 1361, accuracy = 0.019802
step 1361, loss = 13.3986
step 1362, loss = 13.4246
step 1363, loss = 13.4187
step 1364, loss = 15.8918
step 1365, loss = 16.7031
step 1366, loss = 14.0809
step 1367, loss = 15.2804
step 1368, loss = 13.336
step 1369, loss = 14.9529
step 1370, loss = 15.5676
training 1371, accuracy = 0.133333
step 1371, loss = 13.5503
step 1372, loss = 13.4552
step 1373, loss = 13.2724
step 1374, loss = 13.3577
step 1375, loss = 13.1285
step 1376, loss = 13.229
step 1377, loss = 13.0285
step 1378, loss = 16.5821
step 1379, loss = 13.9354
step 1380, loss = 13.6464
training 1381, accuracy = 0.100000
step 1381, loss = 13.9066
step 1382, loss = 13.81
step 1383, loss = 13.4352
step 1384, loss = 13.2712
step 1385, loss = 13.3433
step 1386, loss = 13.3348
step 1387, loss = 13.3696
step 1388, loss = 13.1645
step 1389, loss = 13.0234
step 1390, loss = 13.198
training 1391, accuracy = 0.085106
step 1391, loss = 12.9531
step 1392, loss = 12.8984
step 1393, loss = 12.9747
step 1394, loss = 12.8567
step 1395, loss = 12.7953
step 1396, loss = 12.8981
step 1397, loss = 12.8775
step 1398, loss = 12.7976
step 1399, loss = 12.7591
step 1400, loss = 12.7608
training 1401, accuracy = 0.280303
step 1401, loss = 12.7644
step 1402, loss = 12.7048
step 1403, loss = 12.8428
step 1404, loss = 13.16
step 1405, loss = 12.8821
step 1406, loss = 12.862
step 1407, loss = 12.7788
step 1408, loss = 12.682
step 1409, loss = 12.633
step 1410, loss = 12.5672
training 1411, accuracy = 0.030120
step 1411, loss = 12.6217
step 1412, loss = 12.6303
step 1413, loss = 12.6128
step 1414, loss = 12.6039
step 1415, loss = 12.5324
step 1416, loss = 12.6556
step 1417, loss = 12.6304
step 1418, loss = 12.3837
step 1419, loss = 12.5223
step 1420, loss = 12.3125
training 1421, accuracy = 0.375415
step 1421, loss = 12.4747
step 1422, loss = 13.0249
step 1423, loss = 12.3593
step 1424, loss = 12.1723
step 1425, loss = 12.2924
step 1426, loss = 14.9772
step 1427, loss = 14.0206
step 1428, loss = 12.8638
step 1429, loss = 12.3675
step 1430, loss = 12.5258
training 1431, accuracy = 0.161290
step 1431, loss = 12.3369
step 1432, loss = 12.2052
step 1433, loss = 12.169
step 1434, loss = 12.3091
step 1435, loss = 12.0641
step 1436, loss = 12.1989
step 1437, loss = 12.1004
step 1438, loss = 12.3397
step 1439, loss = 12.068
step 1440, loss = 12.082
training 1441, accuracy = 0.152542
step 1441, loss = 12.1121
step 1442, loss = 12.1047
step 1443, loss = 12.2185
step 1444, loss = 12.1047
step 1445, loss = 12.1055
step 1446, loss = 12.0779
step 1447, loss = 12.092
step 1448, loss = 12.957
step 1449, loss = 12.0537
step 1450, loss = 12.1606
training 1451, accuracy = 0.092105
step 1451, loss = 11.9266
step 1452, loss = 12.2374
step 1453, loss = 11.9648
step 1454, loss = 12.1684
step 1455, loss = 11.9836
step 1456, loss = 11.9026
step 1457, loss = 12.0477
step 1458, loss = 11.9311
step 1459, loss = 11.8461
step 1460, loss = 11.86
training 1461, accuracy = 0.580645
step 1461, loss = 11.8979
step 1462, loss = 11.6437
step 1463, loss = 11.681
step 1464, loss = 11.7948
step 1465, loss = 11.8506
step 1466, loss = 11.5537
step 1467, loss = 11.8259
step 1468, loss = 11.732
step 1469, loss = 11.6006
step 1470, loss = 11.6255
training 1471, accuracy = 0.355932
step 1471, loss = 11.594
step 1472, loss = 11.5987
step 1473, loss = 11.799
step 1474, loss = 11.8884
step 1475, loss = 11.3716
step 1476, loss = 11.6724
step 1477, loss = 11.8444
step 1478, loss = 11.4439
step 1479, loss = 11.6266
step 1480, loss = 11.5273
training 1481, accuracy = 0.323232
step 1481, loss = 11.5831
step 1482, loss = 11.4462
step 1483, loss = 11.4896
step 1484, loss = 11.3853
step 1485, loss = 11.5206
step 1486, loss = 11.3448
step 1487, loss = 11.2606
step 1488, loss = 11.3278
step 1489, loss = 11.3259
step 1490, loss = 11.3447
training 1491, accuracy = 0.022472
step 1491, loss = 11.2987
step 1492, loss = 11.3149
step 1493, loss = 11.2479
step 1494, loss = 11.2119
step 1495, loss = 11.4928
step 1496, loss = 11.3168
step 1497, loss = 11.3129
step 1498, loss = 11.3102
step 1499, loss = 11.2795
step 1500, loss = 11.1336
training 1501, accuracy = 0.482759
step 1501, loss = 11.1442
step 1502, loss = 13.2117
step 1503, loss = 11.818
step 1504, loss = 11.3502
step 1505, loss = 11.1922
step 1506, loss = 11.0665
step 1507, loss = 11.5576
step 1508, loss = 11.349
step 1509, loss = 11.1398
step 1510, loss = 11.3384
training 1511, accuracy = 0.024390
step 1511, loss = 11.0297
step 1512, loss = 11.0687
step 1513, loss = 11.0652
step 1514, loss = 10.8904
step 1515, loss = 13.3689
step 1516, loss = 11.7182
step 1517, loss = 12.0023
step 1518, loss = 11.3403
step 1519, loss = 11.0783
step 1520, loss = 11.2397
training 1521, accuracy = 0.385417
step 1521, loss = 11.3879
step 1522, loss = 10.9909
step 1523, loss = 11.2472
step 1524, loss = 11.2997
step 1525, loss = 11.0156
step 1526, loss = 10.9458
step 1527, loss = 10.7934
step 1528, loss = 10.7439
step 1529, loss = 10.7687
step 1530, loss = 10.7131
training 1531, accuracy = 0.433526
step 1531, loss = 10.9197
step 1532, loss = 10.7574
step 1533, loss = 10.9393
step 1534, loss = 10.821
step 1535, loss = 10.8393
step 1536, loss = 10.8481
step 1537, loss = 10.7077
step 1538, loss = 10.5578
step 1539, loss = 10.7465
step 1540, loss = 10.7006
training 1541, accuracy = 0.038462
step 1541, loss = 11.3111
step 1542, loss = 10.6959
step 1543, loss = 10.8949
step 1544, loss = 10.7575
step 1545, loss = 10.7059
step 1546, loss = 10.6552
step 1547, loss = 10.6559
step 1548, loss = 10.6236
step 1549, loss = 10.6915
step 1550, loss = 10.5
training 1551, accuracy = 0.835052
step 1551, loss = 10.6727
step 1552, loss = 10.452
step 1553, loss = 10.4256
step 1554, loss = 10.6362
step 1555, loss = 10.4372
step 1556, loss = 10.6514
step 1557, loss = 10.5923
step 1558, loss = 10.3513
step 1559, loss = 10.3847
step 1560, loss = 10.354
training 1561, accuracy = 0.075269
step 1561, loss = 10.3313
step 1562, loss = 10.4907
step 1563, loss = 10.2806
step 1564, loss = 10.2739
step 1565, loss = 10.3495
step 1566, loss = 10.239
step 1567, loss = 10.2295
step 1568, loss = 10.368
step 1569, loss = 10.2108
step 1570, loss = 10.2885
training 1571, accuracy = 0.600000
step 1571, loss = 10.2757
step 1572, loss = 10.1876
step 1573, loss = 10.1786
step 1574, loss = 10.2814
step 1575, loss = 10.26
step 1576, loss = 10.3154
step 1577, loss = 11.2274
step 1578, loss = 10.2034
step 1579, loss = 10.1658
step 1580, loss = 10.4246
training 1581, accuracy = 0.000000
step 1581, loss = 10.1194
step 1582, loss = 10.2501
step 1583, loss = 10.0985
step 1584, loss = 10.0909
step 1585, loss = 10.1323
step 1586, loss = 10.1325
step 1587, loss = 9.95729
step 1588, loss = 10.0888
step 1589, loss = 10.0061
step 1590, loss = 10.0432
training 1591, accuracy = 0.160000
step 1591, loss = 9.9857
step 1592, loss = 10.1086
step 1593, loss = 10.1981
step 1594, loss = 9.95042
step 1595, loss = 9.86306
step 1596, loss = 9.979
step 1597, loss = 11.3973
step 1598, loss = 10.0099
step 1599, loss = 10.1127
step 1600, loss = 10.2686
training 1601, accuracy = 0.172414
step 1601, loss = 9.83399
step 1602, loss = 9.88505
step 1603, loss = 9.82746
step 1604, loss = 9.79004
step 1605, loss = 9.78668
step 1606, loss = 9.82414
step 1607, loss = 9.8121
step 1608, loss = 9.76944
step 1609, loss = 9.78155
step 1610, loss = 9.82629
training 1611, accuracy = 0.075949
step 1611, loss = 9.75932
step 1612, loss = 11.268
step 1613, loss = 10.1387
step 1614, loss = 10.0459
step 1615, loss = 9.60896
step 1616, loss = 9.62301
step 1617, loss = 9.67667
step 1618, loss = 10.2632
step 1619, loss = 9.6982
step 1620, loss = 9.72804
training 1621, accuracy = 0.251748
step 1621, loss = 9.63422
step 1622, loss = 9.64334
step 1623, loss = 9.94835
step 1624, loss = 9.6166
step 1625, loss = 9.59882
step 1626, loss = 9.57762
step 1627, loss = 9.75967
step 1628, loss = 9.53372
step 1629, loss = 9.69908
step 1630, loss = 9.61237
training 1631, accuracy = 0.037594
step 1631, loss = 9.68104
step 1632, loss = 9.48518
step 1633, loss = 9.62936
step 1634, loss = 9.51652
step 1635, loss = 9.42734
step 1636, loss = 9.41892
step 1637, loss = 9.42851
step 1638, loss = 9.51853
step 1639, loss = 9.40594
step 1640, loss = 9.4873
training 1641, accuracy = 0.481013
step 1641, loss = 9.37222
step 1642, loss = 9.41968
step 1643, loss = 9.43414
step 1644, loss = 9.41352
step 1645, loss = 9.45786
step 1646, loss = 9.32249
step 1647, loss = 9.46341
step 1648, loss = 9.45098
step 1649, loss = 9.38337
step 1650, loss = 9.44696
training 1651, accuracy = 0.245614
step 1651, loss = 9.29305
step 1652, loss = 9.27935
step 1653, loss = 9.22019
step 1654, loss = 9.29586
step 1655, loss = 9.5121
step 1656, loss = 9.83023
step 1657, loss = 9.59791
step 1658, loss = 9.2695
step 1659, loss = 9.16918
step 1660, loss = 9.37561
training 1661, accuracy = 0.092593
step 1661, loss = 9.14335
step 1662, loss = 11.0161
step 1663, loss = 9.72968
step 1664, loss = 9.87066
step 1665, loss = 9.34806
step 1666, loss = 9.59692
step 1667, loss = 9.46465
step 1668, loss = 9.23831
step 1669, loss = 9.05629
step 1670, loss = 9.26789
training 1671, accuracy = 0.031746
step 1671, loss = 9.40595
step 1672, loss = 9.13086
step 1673, loss = 9.15651
step 1674, loss = 9.00647
step 1675, loss = 9.14201
step 1676, loss = 9.7649
step 1677, loss = 8.94791
step 1678, loss = 9.04538
step 1679, loss = 9.13055
step 1680, loss = 9.86561
training 1681, accuracy = 0.375000
step 1681, loss = 9.13984
step 1682, loss = 9.07074
step 1683, loss = 9.22445
step 1684, loss = 9.06456
step 1685, loss = 8.90631
step 1686, loss = 9.40687
step 1687, loss = 9.02584
step 1688, loss = 8.86429
step 1689, loss = 8.97121
step 1690, loss = 8.99277
training 1691, accuracy = 0.250000
step 1691, loss = 8.98052
step 1692, loss = 8.81947
step 1693, loss = 9.0974
step 1694, loss = 8.9064
step 1695, loss = 8.78053
step 1696, loss = 9.04355
step 1697, loss = 8.853
step 1698, loss = 8.95628
step 1699, loss = 8.76713
step 1700, loss = 8.9986
training 1701, accuracy = 0.095238
step 1701, loss = 8.76899
step 1702, loss = 8.71666
step 1703, loss = 8.8147
step 1704, loss = 8.75304
step 1705, loss = 9.23423
step 1706, loss = 8.91305
step 1707, loss = 9.36892
step 1708, loss = 8.7347
step 1709, loss = 8.6607
step 1710, loss = 8.62328
training 1711, accuracy = 0.455782
step 1711, loss = 8.81351
step 1712, loss = 8.87828
step 1713, loss = 8.81147
step 1714, loss = 8.78416
step 1715, loss = 8.61488
step 1716, loss = 8.62522
step 1717, loss = 8.60865
step 1718, loss = 8.68941
step 1719, loss = 8.54215
step 1720, loss = 8.69276
training 1721, accuracy = 0.089552
step 1721, loss = 8.53786
step 1722, loss = 8.55251
step 1723, loss = 8.60385
step 1724, loss = 8.48713
step 1725, loss = 8.57691
step 1726, loss = 8.72057
step 1727, loss = 8.72283
step 1728, loss = 8.5284
step 1729, loss = 8.57983
step 1730, loss = 8.52633
training 1731, accuracy = 0.354839
step 1731, loss = 8.55414
step 1732, loss = 9.21431
step 1733, loss = 9.11492
step 1734, loss = 8.57557
step 1735, loss = 8.58951
step 1736, loss = 8.35859
step 1737, loss = 8.47365
step 1738, loss = 8.53122
step 1739, loss = 8.45611
step 1740, loss = 8.39643
training 1741, accuracy = 0.206897
step 1741, loss = 8.42944
step 1742, loss = 8.36998
step 1743, loss = 8.89856
step 1744, loss = 8.53111
step 1745, loss = 8.42599
step 1746, loss = 8.3921
step 1747, loss = 8.28302
step 1748, loss = 8.30042
step 1749, loss = 8.29345
step 1750, loss = 9.604
training 1751, accuracy = 0.214286
step 1751, loss = 8.57122
step 1752, loss = 8.68543
step 1753, loss = 8.38436
step 1754, loss = 8.68029
step 1755, loss = 8.52689
step 1756, loss = 8.65644
step 1757, loss = 8.53584
step 1758, loss = 8.672
step 1759, loss = 8.25088
step 1760, loss = 8.46453
training 1761, accuracy = 0.052632
step 1761, loss = 8.27818
step 1762, loss = 8.55117
step 1763, loss = 8.38687
step 1764, loss = 8.56064
step 1765, loss = 8.3921
step 1766, loss = 8.4398
step 1767, loss = 8.36433
step 1768, loss = 8.20508
step 1769, loss = 8.19565
step 1770, loss = 8.0892
training 1771, accuracy = 0.702970
step 1771, loss = 8.32252
step 1772, loss = 8.13418
step 1773, loss = 8.17285
step 1774, loss = 8.04995
step 1775, loss = 8.12845
step 1776, loss = 8.06716
step 1777, loss = 8.00582
step 1778, loss = 10.1998
step 1779, loss = 8.55179
step 1780, loss = 8.27352
training 1781, accuracy = 0.362637
step 1781, loss = 8.40003
step 1782, loss = 7.98255
step 1783, loss = 8.01254
step 1784, loss = 8.1744
step 1785, loss = 8.18999
step 1786, loss = 8.08341
step 1787, loss = 8.10375
step 1788, loss = 7.92434
step 1789, loss = 8.06393
step 1790, loss = 7.91711
training 1791, accuracy = 0.523077
step 1791, loss = 7.87339
step 1792, loss = 8.08837
step 1793, loss = 7.88397
step 1794, loss = 8.36698
step 1795, loss = 8.04147
step 1796, loss = 7.85887
step 1797, loss = 7.89523
step 1798, loss = 7.80289
step 1799, loss = 7.90865
step 1800, loss = 7.85593
training 1801, accuracy = 0.116564
step 1801, loss = 7.85153
step 1802, loss = 7.97876
step 1803, loss = 7.79364
step 1804, loss = 7.8371
step 1805, loss = 7.72493
step 1806, loss = 8.17813
step 1807, loss = 8.19343
step 1808, loss = 8.05666
step 1809, loss = 7.90952
step 1810, loss = 7.94244
training 1811, accuracy = 0.855263
step 1811, loss = 7.86388
step 1812, loss = 7.88879
step 1813, loss = 7.84706
step 1814, loss = 7.83183
step 1815, loss = 7.72017
step 1816, loss = 7.89104
step 1817, loss = 7.87708
step 1818, loss = 8.00169
step 1819, loss = 7.66869
step 1820, loss = 7.79383
training 1821, accuracy = 0.070423
step 1821, loss = 7.65732
step 1822, loss = 8.14394
step 1823, loss = 7.78859
step 1824, loss = 7.71706
step 1825, loss = 7.70732
step 1826, loss = 7.77507
step 1827, loss = 7.59743
step 1828, loss = 7.58787
step 1829, loss = 7.73946
step 1830, loss = 7.58531
training 1831, accuracy = 0.641618
step 1831, loss = 7.68269
step 1832, loss = 7.5959
step 1833, loss = 7.62653
step 1834, loss = 7.60103
step 1835, loss = 7.51135
step 1836, loss = 7.55077
step 1837, loss = 7.82873
step 1838, loss = 7.65161
step 1839, loss = 7.57652
step 1840, loss = 7.55819
training 1841, accuracy = 0.171429
step 1841, loss = 7.48871
step 1842, loss = 7.55456
step 1843, loss = 7.54922
step 1844, loss = 7.46613
step 1845, loss = 7.68112
step 1846, loss = 7.67374
step 1847, loss = 7.73183
step 1848, loss = 7.47495
step 1849, loss = 7.44318
step 1850, loss = 7.46023
training 1851, accuracy = 0.238462
step 1851, loss = 7.42972
step 1852, loss = 10.6996
step 1853, loss = 8.63987
step 1854, loss = 9.73679
step 1855, loss = 8.2402
step 1856, loss = 7.54319
step 1857, loss = 7.62052
step 1858, loss = 7.41162
step 1859, loss = 7.60737
step 1860, loss = 7.8396
training 1861, accuracy = 0.237500
step 1861, loss = 7.41726
step 1862, loss = 7.56877
step 1863, loss = 7.56172
step 1864, loss = 7.34055
step 1865, loss = 7.51441
step 1866, loss = 7.42822
step 1867, loss = 7.34411
step 1868, loss = 7.60037
step 1869, loss = 7.66465
step 1870, loss = 7.40461
training 1871, accuracy = 0.112150
step 1871, loss = 8.41814
step 1872, loss = 7.30492
step 1873, loss = 7.26314
step 1874, loss = 7.4339
step 1875, loss = 7.25704
step 1876, loss = 7.29665
step 1877, loss = 7.2475
step 1878, loss = 7.29503
step 1879, loss = 7.28408
step 1880, loss = 7.3613
training 1881, accuracy = 0.818182
step 1881, loss = 7.29434
step 1882, loss = 7.32171
step 1883, loss = 7.34022
step 1884, loss = 7.33076
step 1885, loss = 7.46978
step 1886, loss = 7.48718
step 1887, loss = 7.38359
step 1888, loss = 7.21391
step 1889, loss = 7.48802
step 1890, loss = 7.81627
training 1891, accuracy = 0.100000
step 1891, loss = 7.23228
step 1892, loss = 7.2498
step 1893, loss = 7.25833
step 1894, loss = 7.20182
step 1895, loss = 7.16882
step 1896, loss = 7.17072
step 1897, loss = 7.24623
step 1898, loss = 7.07272
step 1899, loss = 7.25747
step 1900, loss = 7.41831
training 1901, accuracy = 0.000000
step 1901, loss = 7.42519
step 1902, loss = 6.98788
step 1903, loss = 7.13962
step 1904, loss = 7.05721
step 1905, loss = 7.02889
step 1906, loss = 7.10386
step 1907, loss = 7.03205
step 1908, loss = 7.37844
step 1909, loss = 7.04562
step 1910, loss = 7.12991
training 1911, accuracy = 0.130000
step 1911, loss = 7.2051
step 1912, loss = 7.1533
step 1913, loss = 6.98999
step 1914, loss = 6.99904
step 1915, loss = 7.15259
step 1916, loss = 6.95561
step 1917, loss = 7.01102
step 1918, loss = 7.09138
step 1919, loss = 6.90338
step 1920, loss = 6.98487
training 1921, accuracy = 0.086538
step 1921, loss = 7.11051
step 1922, loss = 8.52246
step 1923, loss = 7.42805
step 1924, loss = 7.12399
step 1925, loss = 7.02148
step 1926, loss = 6.92309
step 1927, loss = 7.0813
step 1928, loss = 7.03617
step 1929, loss = 6.8634
step 1930, loss = 7.01148
training 1931, accuracy = 0.030769
step 1931, loss = 6.83418
step 1932, loss = 6.79786
step 1933, loss = 7.13705
step 1934, loss = 6.98315
step 1935, loss = 6.78352
step 1936, loss = 6.85385
step 1937, loss = 6.94391
step 1938, loss = 6.80408
step 1939, loss = 7.04295
step 1940, loss = 7.00628
training 1941, accuracy = 0.746835
step 1941, loss = 6.92233
step 1942, loss = 6.79231
step 1943, loss = 6.85663
step 1944, loss = 6.86306
step 1945, loss = 7.37821
step 1946, loss = 7.03087
step 1947, loss = 7.11582
step 1948, loss = 6.78819
step 1949, loss = 6.98929
step 1950, loss = 6.68708
training 1951, accuracy = 0.156997
step 1951, loss = 6.76512
step 1952, loss = 6.8251
step 1953, loss = 9.38797
step 1954, loss = 8.35226
step 1955, loss = 6.98107
step 1956, loss = 7.19265
step 1957, loss = 7.08795
step 1958, loss = 6.78253
step 1959, loss = 6.94959
step 1960, loss = 7.34989
training 1961, accuracy = 0.090909
step 1961, loss = 7.15902
step 1962, loss = 6.8536
step 1963, loss = 6.82856
step 1964, loss = 6.94419
step 1965, loss = 6.72847
step 1966, loss = 6.63956
step 1967, loss = 6.6492
step 1968, loss = 6.69072
step 1969, loss = 6.80227
step 1970, loss = 6.57048
training 1971, accuracy = 0.694118
step 1971, loss = 6.67753
step 1972, loss = 6.58863
step 1973, loss = 6.59207
step 1974, loss = 6.69532
step 1975, loss = 6.61534
step 1976, loss = 6.64776
step 1977, loss = 6.63635
step 1978, loss = 6.67569
step 1979, loss = 6.76479
step 1980, loss = 6.63259
training 1981, accuracy = 0.150000
step 1981, loss = 6.55805
step 1982, loss = 6.50964
step 1983, loss = 6.59333
step 1984, loss = 6.60025
step 1985, loss = 6.46386
step 1986, loss = 7.50233
step 1987, loss = 6.74536
step 1988, loss = 6.49161
step 1989, loss = 6.7935
step 1990, loss = 6.6076
training 1991, accuracy = 0.380435
step 1991, loss = 6.45786
step 1992, loss = 6.74582
step 1993, loss = 6.64241
step 1994, loss = 6.61786
step 1995, loss = 6.47343
step 1996, loss = 6.57434
step 1997, loss = 6.57918
step 1998, loss = 6.55176
step 1999, loss = 6.4353
step 2000, loss = 8.78349
training 2001, accuracy = 0.578947
step 2001, loss = 6.9318
step 2002, loss = 7.94532
step 2003, loss = 6.6086
step 2004, loss = 6.66049
step 2005, loss = 7.72368
step 2006, loss = 6.51925
step 2007, loss = 6.43515
step 2008, loss = 6.41838
step 2009, loss = 6.58443
step 2010, loss = 6.34431
training 2011, accuracy = 0.201220
step 2011, loss = 6.43503
step 2012, loss = 6.41465
step 2013, loss = 6.32992
step 2014, loss = 6.61003
step 2015, loss = 6.61547
step 2016, loss = 6.59937
step 2017, loss = 6.39216
step 2018, loss = 6.58813
step 2019, loss = 6.29996
step 2020, loss = 6.34445
training 2021, accuracy = 0.483607
step 2021, loss = 6.34004
step 2022, loss = 6.39622
step 2023, loss = 6.32658
step 2024, loss = 6.24844
step 2025, loss = 6.23445
step 2026, loss = 6.24845
step 2027, loss = 6.23419
step 2028, loss = 6.34317
step 2029, loss = 6.23399
step 2030, loss = 6.35266
training 2031, accuracy = 0.761194
step 2031, loss = 6.25396
step 2032, loss = 6.42787
step 2033, loss = 6.29429
step 2034, loss = 6.43142
step 2035, loss = 6.21747
step 2036, loss = 6.21998
step 2037, loss = 6.44732
step 2038, loss = 6.5368
step 2039, loss = 6.15895
step 2040, loss = 6.15171
training 2041, accuracy = 0.698925
step 2041, loss = 6.40833
step 2042, loss = 6.12408
step 2043, loss = 6.1442
step 2044, loss = 6.34096
step 2045, loss = 6.16752
step 2046, loss = 6.32606
step 2047, loss = 6.41317
step 2048, loss = 6.46747
step 2049, loss = 6.16345
step 2050, loss = 6.21599
training 2051, accuracy = 0.362319
step 2051, loss = 6.10185
step 2052, loss = 6.27701
step 2053, loss = 7.47021
step 2054, loss = 6.48672
step 2055, loss = 6.43254
step 2056, loss = 6.20827
step 2057, loss = 6.28977
step 2058, loss = 6.1344
step 2059, loss = 6.10903
step 2060, loss = 6.04081
training 2061, accuracy = 0.274112
step 2061, loss = 6.01165
step 2062, loss = 6.12133
step 2063, loss = 6.43952
step 2064, loss = 6.12126
step 2065, loss = 6.16107
step 2066, loss = 6.56457
step 2067, loss = 6.23669
step 2068, loss = 6.08423
step 2069, loss = 5.99596
step 2070, loss = 6.09296
training 2071, accuracy = 0.050000
step 2071, loss = 6.16492
step 2072, loss = 6.04467
step 2073, loss = 5.9343
step 2074, loss = 5.96233
step 2075, loss = 5.97422
step 2076, loss = 7.09504
step 2077, loss = 6.85059
step 2078, loss = 6.03925
step 2079, loss = 5.95833
step 2080, loss = 5.97486
training 2081, accuracy = 0.085366
step 2081, loss = 5.96223
step 2082, loss = 6.01404
step 2083, loss = 6.01326
step 2084, loss = 6.17232
step 2085, loss = 6.15558
step 2086, loss = 5.98865
step 2087, loss = 5.93743
step 2088, loss = 7.11129
step 2089, loss = 6.79488
step 2090, loss = 6.18173
training 2091, accuracy = 0.055556
step 2091, loss = 6.04985
step 2092, loss = 5.87163
step 2093, loss = 6.09294
step 2094, loss = 6.31872
step 2095, loss = 5.86788
step 2096, loss = 6.15616
step 2097, loss = 5.88101
step 2098, loss = 6.09863
step 2099, loss = 5.83513
step 2100, loss = 6.03332
training 2101, accuracy = 0.022727
step 2101, loss = 5.99851
step 2102, loss = 5.93696
step 2103, loss = 6.03432
step 2104, loss = 6.34172
step 2105, loss = 6.22433
step 2106, loss = 6.09898
step 2107, loss = 5.86893
step 2108, loss = 5.85152
step 2109, loss = 5.83688
step 2110, loss = 5.77646
training 2111, accuracy = 0.073593
step 2111, loss = 5.88415
step 2112, loss = 5.92209
step 2113, loss = 6.48056
step 2114, loss = 5.98005
step 2115, loss = 5.81309
step 2116, loss = 5.84781
step 2117, loss = 7.39816
step 2118, loss = 6.62086
step 2119, loss = 6.54758
step 2120, loss = 6.38949
training 2121, accuracy = 0.000000
step 2121, loss = 5.82907
step 2122, loss = 6.58278
step 2123, loss = 6.46665
step 2124, loss = 6.50613
step 2125, loss = 5.75201
step 2126, loss = 5.72062
step 2127, loss = 5.92415
step 2128, loss = 5.9284
step 2129, loss = 5.80325
step 2130, loss = 5.73292
training 2131, accuracy = 0.675439
step 2131, loss = 5.67589
step 2132, loss = 5.68207
step 2133, loss = 5.85238
step 2134, loss = 5.65068
step 2135, loss = 5.70806
step 2136, loss = 5.66979
step 2137, loss = 5.78411
step 2138, loss = 5.63818
step 2139, loss = 6.36523
step 2140, loss = 5.78686
training 2141, accuracy = 0.400000
step 2141, loss = 5.82171
step 2142, loss = 5.87494
step 2143, loss = 5.68622
step 2144, loss = 5.68911
step 2145, loss = 5.66021
step 2146, loss = 5.60841
step 2147, loss = 5.65112
step 2148, loss = 5.77527
step 2149, loss = 5.72111
step 2150, loss = 5.77542
training 2151, accuracy = 0.357143
step 2151, loss = 5.51242
step 2152, loss = 5.58527
step 2153, loss = 5.73066
step 2154, loss = 5.51073
step 2155, loss = 5.57211
step 2156, loss = 5.71243
step 2157, loss = 5.47782
step 2158, loss = 5.51807
step 2159, loss = 5.58787
step 2160, loss = 5.70824
training 2161, accuracy = 0.653846
step 2161, loss = 5.48264
step 2162, loss = 5.54886
step 2163, loss = 5.80265
step 2164, loss = 6.40996
step 2165, loss = 5.62497
step 2166, loss = 5.68678
step 2167, loss = 5.82812
step 2168, loss = 5.65799
step 2169, loss = 5.48694
step 2170, loss = 5.49307
training 2171, accuracy = 0.659864
step 2171, loss = 5.43154
step 2172, loss = 5.64487
step 2173, loss = 5.57848
step 2174, loss = 5.55464
step 2175, loss = 5.59567
step 2176, loss = 5.44299
step 2177, loss = 5.47623
step 2178, loss = 5.4705
step 2179, loss = 5.57666
step 2180, loss = 5.89743
training 2181, accuracy = 0.083333
step 2181, loss = 5.73218
step 2182, loss = 5.50846
step 2183, loss = 5.40119
step 2184, loss = 5.51764
step 2185, loss = 6.15105
step 2186, loss = 5.41492
step 2187, loss = 5.57569
step 2188, loss = 5.47938
step 2189, loss = 5.48299
step 2190, loss = 5.41638
training 2191, accuracy = 0.162602
step 2191, loss = 5.38265
step 2192, loss = 5.37464
step 2193, loss = 5.5009
step 2194, loss = 5.44004
step 2195, loss = 5.34519
step 2196, loss = 5.42179
step 2197, loss = 5.32938
step 2198, loss = 5.42885
step 2199, loss = 5.91544
step 2200, loss = 5.41995
training 2201, accuracy = 0.590426
step 2201, loss = 5.40766
step 2202, loss = 5.50398
step 2203, loss = 5.52904
step 2204, loss = 5.52471
step 2205, loss = 5.40067
step 2206, loss = 5.27523
step 2207, loss = 5.27309
step 2208, loss = 5.87732
step 2209, loss = 5.41395
step 2210, loss = 5.52458
training 2211, accuracy = 0.257143
step 2211, loss = 5.55536
step 2212, loss = 5.44165
step 2213, loss = 5.27835
step 2214, loss = 5.36137
step 2215, loss = 5.37698
step 2216, loss = 5.44007
step 2217, loss = 5.23275
step 2218, loss = 5.30334
step 2219, loss = 5.27893
step 2220, loss = 5.35837
training 2221, accuracy = 0.250000
step 2221, loss = 5.94462
step 2222, loss = 5.27054
step 2223, loss = 5.31877
step 2224, loss = 5.54734
step 2225, loss = 5.25337
step 2226, loss = 5.22421
step 2227, loss = 5.28614
step 2228, loss = 5.31381
step 2229, loss = 5.26052
step 2230, loss = 5.25232
training 2231, accuracy = 0.118182
step 2231, loss = 5.40047
step 2232, loss = 5.27516
step 2233, loss = 6.00858
step 2234, loss = 5.48423
step 2235, loss = 5.5099
step 2236, loss = 5.37771
step 2237, loss = 5.2306
step 2238, loss = 5.15593
step 2239, loss = 5.14919
step 2240, loss = 5.22395
training 2241, accuracy = 0.450549
step 2241, loss = 5.37389
step 2242, loss = 5.14306
step 2243, loss = 5.19879
step 2244, loss = 5.1027
step 2245, loss = 5.08274
step 2246, loss = 5.30264
step 2247, loss = 5.1128
step 2248, loss = 5.16727
step 2249, loss = 5.22026
step 2250, loss = 5.32361
training 2251, accuracy = 0.276596
step 2251, loss = 5.32602
step 2252, loss = 5.28891
step 2253, loss = 5.29663
step 2254, loss = 6.02118
step 2255, loss = 5.17713
step 2256, loss = 5.09669
step 2257, loss = 5.139
step 2258, loss = 5.22113
step 2259, loss = 5.09514
step 2260, loss = 5.0664
training 2261, accuracy = 0.805882
step 2261, loss = 5.14086
step 2262, loss = 5.09473
step 2263, loss = 5.97944
step 2264, loss = 6.20661
step 2265, loss = 5.50896
step 2266, loss = 5.1598
step 2267, loss = 6.37023
step 2268, loss = 5.11654
step 2269, loss = 5.13177
step 2270, loss = 5.15293
training 2271, accuracy = 0.229885
step 2271, loss = 5.06918
step 2272, loss = 5.09514
step 2273, loss = 5.02875
step 2274, loss = 5.1421
step 2275, loss = 5.00534
step 2276, loss = 5.27551
step 2277, loss = 5.14517
step 2278, loss = 5.03073
step 2279, loss = 4.94814
step 2280, loss = 5.00294
training 2281, accuracy = 0.448485
step 2281, loss = 5.19667
step 2282, loss = 5.19938
step 2283, loss = 5.40883
step 2284, loss = 5.35069
step 2285, loss = 5.10877
step 2286, loss = 4.98363
step 2287, loss = 5.30605
step 2288, loss = 5.15376
step 2289, loss = 5.13796
step 2290, loss = 5.0087
training 2291, accuracy = 0.116788
step 2291, loss = 4.92812
step 2292, loss = 5.58418
step 2293, loss = 5.40611
step 2294, loss = 4.99177
step 2295, loss = 5.35766
step 2296, loss = 5.05019
step 2297, loss = 5.01629
step 2298, loss = 5.21852
step 2299, loss = 4.95393
step 2300, loss = 4.90203
training 2301, accuracy = 0.219388
step 2301, loss = 5.11316
step 2302, loss = 4.85595
step 2303, loss = 4.94461
step 2304, loss = 4.92881
step 2305, loss = 4.94241
step 2306, loss = 4.9302
step 2307, loss = 4.8882
step 2308, loss = 4.92862
step 2309, loss = 4.94408
step 2310, loss = 4.84226
training 2311, accuracy = 0.140351
step 2311, loss = 4.91664
step 2312, loss = 4.91857
step 2313, loss = 4.8469
step 2314, loss = 4.99388
step 2315, loss = 5.05484
step 2316, loss = 4.95327
step 2317, loss = 4.80692
step 2318, loss = 4.90965
step 2319, loss = 4.89747
step 2320, loss = 4.77293
training 2321, accuracy = 0.757895
step 2321, loss = 5.08861
step 2322, loss = 4.83231
step 2323, loss = 4.85958
step 2324, loss = 5.04482
step 2325, loss = 5.01495
step 2326, loss = 5.54605
step 2327, loss = 5.07607
step 2328, loss = 5.01873
step 2329, loss = 4.77948
step 2330, loss = 5.04356
training 2331, accuracy = 0.238095
step 2331, loss = 5.01601
step 2332, loss = 4.73598
step 2333, loss = 4.90077
step 2334, loss = 4.85443
step 2335, loss = 4.79289
step 2336, loss = 4.74683
step 2337, loss = 5.06135
step 2338, loss = 5.1063
step 2339, loss = 4.93318
step 2340, loss = 4.77015
training 2341, accuracy = 0.452555
step 2341, loss = 4.83709
step 2342, loss = 4.70163
step 2343, loss = 4.86634
step 2344, loss = 4.77036
step 2345, loss = 4.83354
step 2346, loss = 4.89915
step 2347, loss = 4.68914
step 2348, loss = 4.83218
step 2349, loss = 4.77841
step 2350, loss = 4.67864
training 2351, accuracy = 0.285088
step 2351, loss = 8.28853
step 2352, loss = 5.33972
step 2353, loss = 5.5319
step 2354, loss = 4.84279
step 2355, loss = 5.10858
step 2356, loss = 4.90155
step 2357, loss = 4.79673
step 2358, loss = 5.15351
step 2359, loss = 4.76513
step 2360, loss = 5.22485
training 2361, accuracy = 0.045455
step 2361, loss = 4.97504
step 2362, loss = 4.64724
step 2363, loss = 4.80411
step 2364, loss = 4.63145
step 2365, loss = 4.6752
step 2366, loss = 4.63948
step 2367, loss = 4.70463
step 2368, loss = 4.62751
step 2369, loss = 4.90919
step 2370, loss = 4.60189
training 2371, accuracy = 0.200000
step 2371, loss = 4.6862
step 2372, loss = 4.68819
step 2373, loss = 4.5954
step 2374, loss = 4.59822
step 2375, loss = 4.80059
step 2376, loss = 4.64402
step 2377, loss = 4.80423
step 2378, loss = 4.6111
step 2379, loss = 4.61046
step 2380, loss = 4.67194
training 2381, accuracy = 0.567901
step 2381, loss = 4.59116
step 2382, loss = 4.57334
step 2383, loss = 4.56772
step 2384, loss = 5.11126
step 2385, loss = 4.75423
step 2386, loss = 4.55024
step 2387, loss = 4.62
step 2388, loss = 4.65101
step 2389, loss = 4.63152
step 2390, loss = 4.52491
training 2391, accuracy = 0.126866
step 2391, loss = 4.67234
step 2392, loss = 4.51681
step 2393, loss = 4.60484
step 2394, loss = 4.50273
step 2395, loss = 4.71748
step 2396, loss = 4.6454
step 2397, loss = 5.85725
step 2398, loss = 4.82957
step 2399, loss = 4.79592
step 2400, loss = 5.45238
training 2401, accuracy = 0.261905
step 2401, loss = 4.67354
step 2402, loss = 4.69514
step 2403, loss = 5.02497
step 2404, loss = 4.7558
step 2405, loss = 4.63721
step 2406, loss = 4.70683
step 2407, loss = 4.62448
step 2408, loss = 4.59818
step 2409, loss = 4.47166
step 2410, loss = 4.6554
training 2411, accuracy = 0.085106
step 2411, loss = 4.59413
step 2412, loss = 4.67423
step 2413, loss = 4.57243
step 2414, loss = 4.44044
step 2415, loss = 4.45922
step 2416, loss = 4.49707
step 2417, loss = 4.43718
step 2418, loss = 4.48991
step 2419, loss = 4.54101
step 2420, loss = 4.59477
training 2421, accuracy = 0.061538
step 2421, loss = 4.57206
step 2422, loss = 4.54188
step 2423, loss = 4.4618
step 2424, loss = 4.49502
step 2425, loss = 5.07591
step 2426, loss = 4.46071
step 2427, loss = 4.71009
step 2428, loss = 4.59286
step 2429, loss = 4.51487
step 2430, loss = 4.37438
training 2431, accuracy = 0.329154
step 2431, loss = 4.43467
step 2432, loss = 4.40702
step 2433, loss = 4.36226
step 2434, loss = 4.48971
step 2435, loss = 4.5562
step 2436, loss = 4.43062
step 2437, loss = 4.42355
step 2438, loss = 4.38418
step 2439, loss = 4.40228
step 2440, loss = 4.53648
training 2441, accuracy = 0.640000
step 2441, loss = 4.42231
step 2442, loss = 4.40293
step 2443, loss = 4.56431
step 2444, loss = 4.56984
step 2445, loss = 4.38568
step 2446, loss = 4.35787
step 2447, loss = 4.41656
step 2448, loss = 4.40096
step 2449, loss = 4.39014
step 2450, loss = 4.33487
training 2451, accuracy = 0.114583
step 2451, loss = 4.41907
step 2452, loss = 4.39669
step 2453, loss = 4.39303
step 2454, loss = 4.40882
step 2455, loss = 4.35051
step 2456, loss = 4.29423
step 2457, loss = 4.66017
step 2458, loss = 4.44174
step 2459, loss = 4.35975
step 2460, loss = 4.45475
training 2461, accuracy = 0.340426
step 2461, loss = 4.49425
step 2462, loss = 4.42846
step 2463, loss = 4.20531
step 2464, loss = 4.37476
step 2465, loss = 4.32402
step 2466, loss = 4.37955
step 2467, loss = 4.39166
step 2468, loss = 4.46866
step 2469, loss = 4.25492
step 2470, loss = 4.37073
training 2471, accuracy = 0.595745
step 2471, loss = 4.41209
step 2472, loss = 4.22255
step 2473, loss = 4.23957
step 2474, loss = 4.40291
step 2475, loss = 4.41618
step 2476, loss = 4.32331
step 2477, loss = 4.21541
step 2478, loss = 4.24186
step 2479, loss = 4.27343
step 2480, loss = 4.46703
training 2481, accuracy = 0.173077
step 2481, loss = 4.26763
step 2482, loss = 4.58667
step 2483, loss = 4.49678
step 2484, loss = 4.32449
step 2485, loss = 4.19478
step 2486, loss = 4.24201
step 2487, loss = 4.27611
step 2488, loss = 4.42968
step 2489, loss = 4.23577
step 2490, loss = 4.29703
training 2491, accuracy = 0.611111
step 2491, loss = 4.35489
step 2492, loss = 4.6069
step 2493, loss = 4.8343
step 2494, loss = 4.33632
step 2495, loss = 4.33685
step 2496, loss = 4.22887
step 2497, loss = 4.43499
step 2498, loss = 4.30912
step 2499, loss = 4.3137
step 2500, loss = 4.23003
training 2501, accuracy = 0.030928
step 2501, loss = 4.25903
step 2502, loss = 4.25919
step 2503, loss = 4.11838
step 2504, loss = 4.49423
step 2505, loss = 4.40578
step 2506, loss = 4.21095
step 2507, loss = 4.19962
step 2508, loss = 4.53502
step 2509, loss = 4.47835
step 2510, loss = 4.21718
training 2511, accuracy = 0.032967
step 2511, loss = 4.26798
step 2512, loss = 4.31225
step 2513, loss = 4.19247
step 2514, loss = 4.26812
step 2515, loss = 4.16355
step 2516, loss = 4.13842
step 2517, loss = 4.65175
step 2518, loss = 4.15921
step 2519, loss = 4.30157
step 2520, loss = 4.33739
training 2521, accuracy = 0.000000
step 2521, loss = 4.24673
step 2522, loss = 4.19989
step 2523, loss = 4.52358
step 2524, loss = 4.12861
step 2525, loss = 4.14863
step 2526, loss = 4.14669
step 2527, loss = 4.18691
step 2528, loss = 4.10346
step 2529, loss = 4.14952
step 2530, loss = 4.16999
training 2531, accuracy = 0.586207
step 2531, loss = 4.05834
step 2532, loss = 4.15734
step 2533, loss = 4.1671
step 2534, loss = 4.42013
step 2535, loss = 4.45171
step 2536, loss = 4.1273
step 2537, loss = 4.28835
step 2538, loss = 4.13887
step 2539, loss = 4.15507
step 2540, loss = 4.18395
training 2541, accuracy = 0.225352
step 2541, loss = 4.23581
step 2542, loss = 4.12409
step 2543, loss = 4.16722
step 2544, loss = 4.03983
step 2545, loss = 4.20623
step 2546, loss = 4.10232
step 2547, loss = 4.03337
step 2548, loss = 4.01071
step 2549, loss = 4.18112
step 2550, loss = 3.99592
training 2551, accuracy = 0.547945
step 2551, loss = 4.03108
step 2552, loss = 4.04974
step 2553, loss = 4.09665
step 2554, loss = 4.05577
step 2555, loss = 4.08055
step 2556, loss = 4.02999
step 2557, loss = 3.99677
step 2558, loss = 3.97628
step 2559, loss = 4.02347
step 2560, loss = 4.05154
training 2561, accuracy = 0.118280
step 2561, loss = 3.96585
step 2562, loss = 4.31803
step 2563, loss = 4.01016
step 2564, loss = 3.95859
step 2565, loss = 4.15286
step 2566, loss = 3.977
step 2567, loss = 4.23054
step 2568, loss = 5.21943
step 2569, loss = 5.39178
step 2570, loss = 4.42978
training 2571, accuracy = 0.631579
step 2571, loss = 4.23314
step 2572, loss = 4.21964
step 2573, loss = 4.14657
step 2574, loss = 3.93216
step 2575, loss = 4.10303
step 2576, loss = 4.49273
step 2577, loss = 4.11987
step 2578, loss = 3.92189
step 2579, loss = 4.08306
step 2580, loss = 4.04187
training 2581, accuracy = 0.555556
step 2581, loss = 4.06118
step 2582, loss = 5.09843
step 2583, loss = 4.13795
step 2584, loss = 4.0508
step 2585, loss = 3.98194
step 2586, loss = 4.08898
step 2587, loss = 3.91726
step 2588, loss = 3.98922
step 2589, loss = 3.94204
step 2590, loss = 4.01842
training 2591, accuracy = 0.566265
step 2591, loss = 3.87269
step 2592, loss = 4.26135
step 2593, loss = 3.91428
step 2594, loss = 3.93439
step 2595, loss = 3.93473
step 2596, loss = 3.87411
step 2597, loss = 3.90941
step 2598, loss = 4.06281
step 2599, loss = 3.96744
step 2600, loss = 4.19151
training 2601, accuracy = 0.076923
step 2601, loss = 3.96647
step 2602, loss = 3.99599
step 2603, loss = 3.92285
step 2604, loss = 3.89707
step 2605, loss = 3.92271
step 2606, loss = 3.87576
step 2607, loss = 4.45161
step 2608, loss = 3.98315
step 2609, loss = 3.95889
step 2610, loss = 3.93484
training 2611, accuracy = 0.162162
step 2611, loss = 3.91379
step 2612, loss = 3.8292
step 2613, loss = 3.84658
step 2614, loss = 4.01575
step 2615, loss = 3.83097
step 2616, loss = 3.98785
step 2617, loss = 3.91789
step 2618, loss = 3.89538
step 2619, loss = 3.94696
step 2620, loss = 3.81096
training 2621, accuracy = 0.035000
step 2621, loss = 3.84657
step 2622, loss = 3.82543
step 2623, loss = 3.79594
step 2624, loss = 4.05819
step 2625, loss = 3.7684
step 2626, loss = 3.88249
step 2627, loss = 3.78726
step 2628, loss = 3.87682
step 2629, loss = 3.86927
step 2630, loss = 3.88461
training 2631, accuracy = 0.430108
step 2631, loss = 3.76776
step 2632, loss = 3.76587
step 2633, loss = 3.81825
step 2634, loss = 3.95296
step 2635, loss = 4.5485
step 2636, loss = 3.84731
step 2637, loss = 5.93336
step 2638, loss = 4.5071
step 2639, loss = 4.59286
step 2640, loss = 4.30805
training 2641, accuracy = 0.177083
step 2641, loss = 4.09869
step 2642, loss = 4.55876
step 2643, loss = 4.13945
step 2644, loss = 4.6118
step 2645, loss = 3.78869
step 2646, loss = 3.82506
step 2647, loss = 3.96776
step 2648, loss = 4.16963
step 2649, loss = 4.04726
step 2650, loss = 3.79235
training 2651, accuracy = 0.801527
step 2651, loss = 3.79692
step 2652, loss = 3.81322
step 2653, loss = 3.88467
step 2654, loss = 3.79375
step 2655, loss = 3.82747
step 2656, loss = 3.70702
step 2657, loss = 3.96096
step 2658, loss = 3.80014
step 2659, loss = 3.70972
step 2660, loss = 3.89813
training 2661, accuracy = 0.098039
step 2661, loss = 3.70508
step 2662, loss = 3.74455
step 2663, loss = 4.78887
step 2664, loss = 4.21741
step 2665, loss = 3.84225
step 2666, loss = 3.76137
step 2667, loss = 3.7645
step 2668, loss = 3.93233
step 2669, loss = 3.97361
step 2670, loss = 3.84033
training 2671, accuracy = 0.339286
step 2671, loss = 3.72052
step 2672, loss = 3.68813
step 2673, loss = 3.71142
step 2674, loss = 3.86615
step 2675, loss = 3.6704
step 2676, loss = 3.8115
step 2677, loss = 3.81616
step 2678, loss = 3.85912
step 2679, loss = 4.16202
step 2680, loss = 3.77778
training 2681, accuracy = 0.392405
step 2681, loss = 3.67835
step 2682, loss = 3.69452
step 2683, loss = 3.97017
step 2684, loss = 3.86909
step 2685, loss = 3.78987
step 2686, loss = 3.73196
step 2687, loss = 3.64983
step 2688, loss = 3.75684
step 2689, loss = 4.17347
step 2690, loss = 3.87764
training 2691, accuracy = 0.584416
step 2691, loss = 3.71793
step 2692, loss = 3.99394
step 2693, loss = 3.68005
step 2694, loss = 3.8563
step 2695, loss = 3.76704
step 2696, loss = 3.68017
step 2697, loss = 3.6028
step 2698, loss = 3.62368
step 2699, loss = 3.75666
step 2700, loss = 3.72222
training 2701, accuracy = 0.397436
step 2701, loss = 3.73685
step 2702, loss = 3.78727
step 2703, loss = 3.62705
step 2704, loss = 3.69012
step 2705, loss = 3.62892
step 2706, loss = 3.61913
step 2707, loss = 3.72287
step 2708, loss = 4.0675
step 2709, loss = 3.64634
step 2710, loss = 3.78878
training 2711, accuracy = 0.074627
step 2711, loss = 4.17711
step 2712, loss = 3.60147
step 2713, loss = 3.72234
step 2714, loss = 3.65259
step 2715, loss = 3.76099
step 2716, loss = 3.60816
step 2717, loss = 3.71336
step 2718, loss = 3.51163
step 2719, loss = 3.72629
step 2720, loss = 3.77027
training 2721, accuracy = 0.293103
step 2721, loss = 3.54895
step 2722, loss = 3.60308
step 2723, loss = 3.70932
step 2724, loss = 3.66883
step 2725, loss = 3.52241
step 2726, loss = 3.50122
step 2727, loss = 3.56685
step 2728, loss = 3.53886
step 2729, loss = 3.81029
step 2730, loss = 4.11715
training 2731, accuracy = 0.130435
step 2731, loss = 3.91058
step 2732, loss = 3.76218
step 2733, loss = 3.7663
step 2734, loss = 3.73112
step 2735, loss = 3.63674
step 2736, loss = 4.16278
step 2737, loss = 4.14137
step 2738, loss = 3.74513
step 2739, loss = 3.53691
step 2740, loss = 3.79875
training 2741, accuracy = 0.212500
step 2741, loss = 3.73539
step 2742, loss = 3.55379
step 2743, loss = 3.6212
step 2744, loss = 3.53894
step 2745, loss = 3.6139
step 2746, loss = 3.64598
step 2747, loss = 4.18088
step 2748, loss = 3.5014
step 2749, loss = 3.50173
step 2750, loss = 3.4532
training 2751, accuracy = 0.147799
step 2751, loss = 3.5262
step 2752, loss = 3.74586
step 2753, loss = 3.56189
step 2754, loss = 3.53093
step 2755, loss = 3.49052
step 2756, loss = 3.47837
step 2757, loss = 3.46206
step 2758, loss = 3.64505
step 2759, loss = 3.50698
step 2760, loss = 3.46155
training 2761, accuracy = 0.670270
step 2761, loss = 3.43104
step 2762, loss = 3.57773
step 2763, loss = 3.50167
step 2764, loss = 3.55789
step 2765, loss = 3.59941
step 2766, loss = 3.55287
step 2767, loss = 3.72187
step 2768, loss = 3.49601
step 2769, loss = 3.50814
step 2770, loss = 3.47585
training 2771, accuracy = 0.137681
step 2771, loss = 3.50957
step 2772, loss = 3.59236
step 2773, loss = 3.43025
step 2774, loss = 3.48807
step 2775, loss = 3.47493
step 2776, loss = 3.62378
step 2777, loss = 3.68726
step 2778, loss = 3.50556
step 2779, loss = 3.55551
step 2780, loss = 3.45992
training 2781, accuracy = 0.581633
step 2781, loss = 5.32561
step 2782, loss = 3.78363
step 2783, loss = 4.13154
step 2784, loss = 3.72129
step 2785, loss = 3.58584
step 2786, loss = 3.68344
step 2787, loss = 3.52622
step 2788, loss = 3.81167
step 2789, loss = 3.50649
step 2790, loss = 3.46358
training 2791, accuracy = 0.049645
step 2791, loss = 3.49423
step 2792, loss = 3.44679
step 2793, loss = 3.39869
step 2794, loss = 3.50725
step 2795, loss = 3.39677
step 2796, loss = 3.41417
step 2797, loss = 3.40085
step 2798, loss = 3.37411
step 2799, loss = 3.48347
step 2800, loss = 3.47183
training 2801, accuracy = 0.164706
step 2801, loss = 3.37658
step 2802, loss = 3.32272
step 2803, loss = 3.39666
step 2804, loss = 3.3137
step 2805, loss = 3.77524
step 2806, loss = 3.579
step 2807, loss = 3.72082
step 2808, loss = 3.34491
step 2809, loss = 3.42862
step 2810, loss = 3.31086
training 2811, accuracy = 0.262774
step 2811, loss = 3.5267
step 2812, loss = 3.33569
step 2813, loss = 3.48678
step 2814, loss = 3.44885
step 2815, loss = 3.37244
step 2816, loss = 3.32959
step 2817, loss = 3.50548
step 2818, loss = 3.51023
step 2819, loss = 3.30599
step 2820, loss = 3.36811
training 2821, accuracy = 0.205128
step 2821, loss = 3.35927
step 2822, loss = 3.3901
step 2823, loss = 3.30413
step 2824, loss = 3.37208
step 2825, loss = 3.53168
step 2826, loss = 3.40573
step 2827, loss = 3.4238
step 2828, loss = 3.41334
step 2829, loss = 3.46
step 2830, loss = 3.47459
training 2831, accuracy = 0.473684
step 2831, loss = 3.40574
step 2832, loss = 3.51708
step 2833, loss = 3.32457
step 2834, loss = 3.50835
step 2835, loss = 3.37239
step 2836, loss = 3.28993
step 2837, loss = 3.41288
step 2838, loss = 3.30315
step 2839, loss = 3.41856
step 2840, loss = 3.32703
training 2841, accuracy = 0.531915
step 2841, loss = 3.47354
step 2842, loss = 3.42988
step 2843, loss = 3.28787
step 2844, loss = 3.35679
step 2845, loss = 3.23886
step 2846, loss = 3.38677
step 2847, loss = 3.35768
step 2848, loss = 3.78858
step 2849, loss = 3.73965
step 2850, loss = 3.54192
training 2851, accuracy = 0.720430
step 2851, loss = 3.31385
step 2852, loss = 3.53417
step 2853, loss = 3.27734
step 2854, loss = 3.28801
step 2855, loss = 3.21726
step 2856, loss = 3.22238
step 2857, loss = 3.38327
step 2858, loss = 3.42894
step 2859, loss = 3.25072
step 2860, loss = 3.32815
training 2861, accuracy = 0.791667
step 2861, loss = 3.31186
step 2862, loss = 3.52799
step 2863, loss = 3.31674
step 2864, loss = 3.37592
step 2865, loss = 3.24738
step 2866, loss = 3.24469
step 2867, loss = 3.19202
step 2868, loss = 3.35484
step 2869, loss = 3.28011
step 2870, loss = 3.28944
training 2871, accuracy = 0.000000
step 2871, loss = 3.79399
step 2872, loss = 3.87306
step 2873, loss = 4.29081
step 2874, loss = 3.3987
step 2875, loss = 3.23638
step 2876, loss = 3.39292
step 2877, loss = 3.2151
step 2878, loss = 3.48048
step 2879, loss = 3.20388
step 2880, loss = 3.49889
training 2881, accuracy = 0.236842
step 2881, loss = 3.22723
step 2882, loss = 3.32413
step 2883, loss = 3.27252
step 2884, loss = 3.32727
step 2885, loss = 3.31625
step 2886, loss = 3.18707
step 2887, loss = 3.15077
step 2888, loss = 3.32004
step 2889, loss = 3.1862
step 2890, loss = 3.16211
training 2891, accuracy = 0.579710
step 2891, loss = 3.20957
step 2892, loss = 3.25985
step 2893, loss = 3.20914
step 2894, loss = 3.27996
step 2895, loss = 3.25725
step 2896, loss = 3.25043
step 2897, loss = 3.27497
step 2898, loss = 3.12104
step 2899, loss = 3.29542
step 2900, loss = 3.13705
training 2901, accuracy = 0.125000
step 2901, loss = 3.13888
step 2902, loss = 3.29367
step 2903, loss = 3.45094
step 2904, loss = 3.26763
step 2905, loss = 3.25685
step 2906, loss = 5.36314
step 2907, loss = 3.70806
step 2908, loss = 3.4474
step 2909, loss = 3.60189
step 2910, loss = 3.31242
training 2911, accuracy = 0.511628
step 2911, loss = 3.63198
step 2912, loss = 3.44859
step 2913, loss = 3.68138
step 2914, loss = 3.34483
step 2915, loss = 3.83194
step 2916, loss = 3.19152
step 2917, loss = 3.44788
step 2918, loss = 3.59919
step 2919, loss = 3.32515
step 2920, loss = 3.28188
training 2921, accuracy = 0.058824
step 2921, loss = 3.13073
step 2922, loss = 3.10782
step 2923, loss = 3.11945
step 2924, loss = 3.10076
step 2925, loss = 3.18472
step 2926, loss = 3.16136
step 2927, loss = 3.07109
step 2928, loss = 3.49829
step 2929, loss = 3.31345
step 2930, loss = 3.13477
training 2931, accuracy = 0.364341
step 2931, loss = 3.13827
step 2932, loss = 3.1307
step 2933, loss = 3.14767
step 2934, loss = 3.3204
step 2935, loss = 3.06624
step 2936, loss = 3.21925
step 2937, loss = 3.23676
step 2938, loss = 3.14401
step 2939, loss = 3.18752
step 2940, loss = 3.10517
training 2941, accuracy = 0.758065
step 2941, loss = 3.04545
step 2942, loss = 3.24985
step 2943, loss = 3.01322
step 2944, loss = 3.06713
step 2945, loss = 3.11798
step 2946, loss = 3.12613
step 2947, loss = 3.02403
step 2948, loss = 3.04491
step 2949, loss = 3.09325
step 2950, loss = 3.11796
training 2951, accuracy = 0.071429
step 2951, loss = 3.13328
step 2952, loss = 4.17189
step 2953, loss = 3.14418
step 2954, loss = 3.15745
step 2955, loss = 3.12142
step 2956, loss = 3.13832
step 2957, loss = 3.24191
step 2958, loss = 3.15273
step 2959, loss = 3.1276
step 2960, loss = 3.11599
training 2961, accuracy = 0.226415
step 2961, loss = 3.06817
step 2962, loss = 3.13334
step 2963, loss = 2.99913
step 2964, loss = 3.09636
step 2965, loss = 3.06986
step 2966, loss = 3.27035
step 2967, loss = 3.00079
step 2968, loss = 2.99473
step 2969, loss = 3.23653
step 2970, loss = 3.16316
training 2971, accuracy = 0.155172
step 2971, loss = 3.06321
step 2972, loss = 3.00741
step 2973, loss = 3.03792
step 2974, loss = 3.00133
step 2975, loss = 3.01956
step 2976, loss = 2.98913
step 2977, loss = 3.17686
step 2978, loss = 2.97039
step 2979, loss = 3.01068
step 2980, loss = 3.12614
training 2981, accuracy = 0.051948
step 2981, loss = 3.27355
step 2982, loss = 3.04871
step 2983, loss = 3.05252
step 2984, loss = 2.99772
step 2985, loss = 3.07577
step 2986, loss = 3.13039
step 2987, loss = 3.02677
step 2988, loss = 3.04475
step 2989, loss = 3.09493
step 2990, loss = 2.99366
training 2991, accuracy = 0.515723
step 2991, loss = 3.04667
step 2992, loss = 3.02584
step 2993, loss = 3.01607
step 2994, loss = 3.165
step 2995, loss = 4.30638
step 2996, loss = 3.19552
step 2997, loss = 3.02723
step 2998, loss = 2.97408
step 2999, loss = 2.93908
step 3000, loss = 3.05071
training 3001, accuracy = 0.489362
step 3001, loss = 2.97241
step 3002, loss = 3.10487
step 3003, loss = 4.12208
step 3004, loss = 3.57651
step 3005, loss = 3.15346
step 3006, loss = 3.5231
step 3007, loss = 3.12353
step 3008, loss = 3.30743
step 3009, loss = 3.04659
step 3010, loss = 3.11003
training 3011, accuracy = 0.330189
step 3011, loss = 2.93577
step 3012, loss = 2.95711
step 3013, loss = 3.21093
step 3014, loss = 3.08722
step 3015, loss = 3.12244
step 3016, loss = 2.934
step 3017, loss = 3.09952
step 3018, loss = 3.05484
step 3019, loss = 3.0818
step 3020, loss = 3.15605
training 3021, accuracy = 0.162162
step 3021, loss = 2.95791
step 3022, loss = 2.93645
step 3023, loss = 2.98675
step 3024, loss = 3.05331
step 3025, loss = 2.97349
step 3026, loss = 3.0871
step 3027, loss = 2.9273
step 3028, loss = 3.07946
step 3029, loss = 3.14558
step 3030, loss = 2.92473
training 3031, accuracy = 0.125000
step 3031, loss = 2.90736
step 3032, loss = 2.94384
step 3033, loss = 3.00863
step 3034, loss = 2.86191
step 3035, loss = 2.98522
step 3036, loss = 2.87764
step 3037, loss = 2.84939
step 3038, loss = 2.94587
step 3039, loss = 3.06077
step 3040, loss = 3.11651
training 3041, accuracy = 0.190476
step 3041, loss = 3.28515
step 3042, loss = 2.93498
step 3043, loss = 2.90069
step 3044, loss = 2.89963
step 3045, loss = 2.88696
step 3046, loss = 2.98896
step 3047, loss = 3.34045
step 3048, loss = 3.00423
step 3049, loss = 2.88304
step 3050, loss = 3.60648
training 3051, accuracy = 0.380952
step 3051, loss = 3.36658
step 3052, loss = 3.16024
step 3053, loss = 2.90688
step 3054, loss = 3.03177
step 3055, loss = 2.89899
step 3056, loss = 3.00741
step 3057, loss = 3.01113
step 3058, loss = 3.15523
step 3059, loss = 3.38452
step 3060, loss = 3.08574
training 3061, accuracy = 0.117647
step 3061, loss = 3.14192
step 3062, loss = 3.00009
step 3063, loss = 2.90605
step 3064, loss = 2.88697
step 3065, loss = 2.83737
step 3066, loss = 3.11875
step 3067, loss = 2.84272
step 3068, loss = 3.02755
step 3069, loss = 2.80721
step 3070, loss = 3.00047
training 3071, accuracy = 0.102041
step 3071, loss = 2.83455
step 3072, loss = 2.92031
step 3073, loss = 2.81638
step 3074, loss = 2.79585
step 3075, loss = 2.80498
step 3076, loss = 3.00657
step 3077, loss = 2.89394
step 3078, loss = 2.80483
step 3079, loss = 2.85545
step 3080, loss = 2.94275
training 3081, accuracy = 0.596154
step 3081, loss = 2.83812
step 3082, loss = 2.81854
step 3083, loss = 2.816
step 3084, loss = 2.86285
step 3085, loss = 3.22629
step 3086, loss = 2.86164
step 3087, loss = 2.90263
step 3088, loss = 3.18908
step 3089, loss = 2.95891
step 3090, loss = 3.4539
training 3091, accuracy = 0.540541
step 3091, loss = 3.01749
step 3092, loss = 2.9554
step 3093, loss = 2.80255
step 3094, loss = 2.94358
step 3095, loss = 2.92408
step 3096, loss = 2.81451
step 3097, loss = 2.94468
step 3098, loss = 2.89503
step 3099, loss = 3.00598
step 3100, loss = 2.79314
training 3101, accuracy = 0.290698
step 3101, loss = 2.867
step 3102, loss = 2.84238
step 3103, loss = 2.75798
step 3104, loss = 2.83695
step 3105, loss = 2.77714
step 3106, loss = 2.87091
step 3107, loss = 2.7483
step 3108, loss = 2.77754
step 3109, loss = 2.86807
step 3110, loss = 2.83269
training 3111, accuracy = 0.463415
step 3111, loss = 2.8425
step 3112, loss = 3.07723
step 3113, loss = 2.77796
step 3114, loss = 2.77371
step 3115, loss = 2.74795
step 3116, loss = 2.78662
step 3117, loss = 2.79774
step 3118, loss = 2.75066
step 3119, loss = 2.79751
step 3120, loss = 2.89524
training 3121, accuracy = 0.096774
step 3121, loss = 2.7105
step 3122, loss = 2.90418
step 3123, loss = 2.93758
step 3124, loss = 2.73996
step 3125, loss = 2.86073
step 3126, loss = 2.85242
step 3127, loss = 2.89711
step 3128, loss = 2.80847
step 3129, loss = 2.91935
step 3130, loss = 2.75912
training 3131, accuracy = 0.581395
step 3131, loss = 2.72395
step 3132, loss = 2.79118
step 3133, loss = 2.85456
step 3134, loss = 2.74474
step 3135, loss = 2.7961
step 3136, loss = 2.71072
step 3137, loss = 2.98702
step 3138, loss = 2.98169
step 3139, loss = 2.89572
step 3140, loss = 2.75055
training 3141, accuracy = 0.672566
step 3141, loss = 2.70369
step 3142, loss = 2.92255
step 3143, loss = 2.77095
step 3144, loss = 3.09726
step 3145, loss = 3.00577
step 3146, loss = 3.03916
step 3147, loss = 2.77946
step 3148, loss = 2.95949
step 3149, loss = 3.01422
step 3150, loss = 2.69044
training 3151, accuracy = 0.157480
step 3151, loss = 2.9321
step 3152, loss = 2.71753
step 3153, loss = 2.69023
step 3154, loss = 2.99009
step 3155, loss = 2.75712
step 3156, loss = 2.79413
step 3157, loss = 2.69167
step 3158, loss = 2.76008
step 3159, loss = 2.89806
step 3160, loss = 2.75904
training 3161, accuracy = 0.200000
step 3161, loss = 2.63856
step 3162, loss = 2.69559
step 3163, loss = 2.6318
step 3164, loss = 2.76387
step 3165, loss = 2.8748
step 3166, loss = 3.06497
step 3167, loss = 2.63937
step 3168, loss = 2.72769
step 3169, loss = 2.74906
step 3170, loss = 2.80233
training 3171, accuracy = 0.035714
step 3171, loss = 2.85179
step 3172, loss = 2.85036
step 3173, loss = 2.63645
step 3174, loss = 2.8454
step 3175, loss = 3.43207
step 3176, loss = 2.96167
step 3177, loss = 2.78402
step 3178, loss = 2.73344
step 3179, loss = 2.64428
step 3180, loss = 2.60701
training 3181, accuracy = 0.194611
step 3181, loss = 2.72983
step 3182, loss = 2.61464
step 3183, loss = 2.70825
step 3184, loss = 2.73393
step 3185, loss = 3.26078
step 3186, loss = 3.36853
step 3187, loss = 5.71587
step 3188, loss = 2.77581
step 3189, loss = 3.07975
step 3190, loss = 2.9878
training 3191, accuracy = 0.134921
step 3191, loss = 4.09567
step 3192, loss = 3.26143
step 3193, loss = 2.80473
step 3194, loss = 2.91344
step 3195, loss = 2.94653
step 3196, loss = 2.79288
step 3197, loss = 2.75051
step 3198, loss = 2.65922
step 3199, loss = 2.72511
step 3200, loss = 2.721
training 3201, accuracy = 0.300000
step 3201, loss = 2.70333
step 3202, loss = 2.64222
step 3203, loss = 2.93095
step 3204, loss = 2.89127
step 3205, loss = 2.83527
step 3206, loss = 2.65231
step 3207, loss = 2.6853
step 3208, loss = 2.7121
step 3209, loss = 2.58212
step 3210, loss = 4.42464
training 3211, accuracy = 0.066667
step 3211, loss = 3.63943
step 3212, loss = 3.14618
step 3213, loss = 2.83307
step 3214, loss = 2.8599
step 3215, loss = 3.03518
step 3216, loss = 2.64192
step 3217, loss = 3.22181
step 3218, loss = 2.86824
step 3219, loss = 2.87367
step 3220, loss = 2.70694
training 3221, accuracy = 0.573034
step 3221, loss = 2.85572
step 3222, loss = 3.07203
step 3223, loss = 2.61106
step 3224, loss = 2.60546
step 3225, loss = 2.61175
step 3226, loss = 2.54504
step 3227, loss = 2.62533
step 3228, loss = 2.54833
step 3229, loss = 2.56563
step 3230, loss = 2.75547
training 3231, accuracy = 0.491803
step 3231, loss = 2.61524
step 3232, loss = 2.56375
step 3233, loss = 2.92322
step 3234, loss = 2.55439
step 3235, loss = 2.55271
step 3236, loss = 2.61867
step 3237, loss = 2.61411
step 3238, loss = 2.62941
step 3239, loss = 2.66384
step 3240, loss = 2.5393
training 3241, accuracy = 0.190244
step 3241, loss = 2.66959
step 3242, loss = 2.59099
step 3243, loss = 2.66558
step 3244, loss = 2.62895
step 3245, loss = 2.57412
step 3246, loss = 3.5054
step 3247, loss = 3.19927
step 3248, loss = 2.57606
step 3249, loss = 2.90163
step 3250, loss = 2.62213
training 3251, accuracy = 0.256545
step 3251, loss = 2.93414
step 3252, loss = 2.63145
step 3253, loss = 2.60243
step 3254, loss = 2.70559
step 3255, loss = 2.5846
step 3256, loss = 2.71053
step 3257, loss = 3.16247
step 3258, loss = 2.68066
step 3259, loss = 2.68906
step 3260, loss = 2.69393
training 3261, accuracy = 0.376623
step 3261, loss = 3.47795
step 3262, loss = 2.96973
step 3263, loss = 2.96111
step 3264, loss = 2.63543
step 3265, loss = 2.73568
step 3266, loss = 2.58975
step 3267, loss = 2.64206
step 3268, loss = 2.56903
step 3269, loss = 2.60406
step 3270, loss = 2.51632
training 3271, accuracy = 0.392473
step 3271, loss = 2.53656
step 3272, loss = 2.55714
step 3273, loss = 2.64091
step 3274, loss = 2.52683
step 3275, loss = 4.26412
step 3276, loss = 2.98084
step 3277, loss = 2.68541
step 3278, loss = 3.17212
step 3279, loss = 3.07622
step 3280, loss = 2.65233
training 3281, accuracy = 0.277311
step 3281, loss = 2.49862
step 3282, loss = 2.89123
step 3283, loss = 2.52587
step 3284, loss = 4.07516
step 3285, loss = 3.07542
step 3286, loss = 2.63921
step 3287, loss = 3.11221
step 3288, loss = 2.66134
step 3289, loss = 2.75702
step 3290, loss = 2.51436
training 3291, accuracy = 0.496774
step 3291, loss = 2.68623
step 3292, loss = 2.40409
step 3293, loss = 2.50645
step 3294, loss = 2.61095
step 3295, loss = 2.45731
step 3296, loss = 2.58008
step 3297, loss = 2.51853
step 3298, loss = 2.46519
step 3299, loss = 2.49515
step 3300, loss = 2.45677
training 3301, accuracy = 0.731343
step 3301, loss = 2.46993
step 3302, loss = 2.49686
step 3303, loss = 2.43817
step 3304, loss = 2.43003
step 3305, loss = 2.69937
step 3306, loss = 2.78893
step 3307, loss = 2.61787
step 3308, loss = 2.57571
step 3309, loss = 2.75437
step 3310, loss = 2.6812
training 3311, accuracy = 0.204301
step 3311, loss = 2.64103
step 3312, loss = 2.48178
step 3313, loss = 2.54059
step 3314, loss = 2.6732
step 3315, loss = 2.75485
step 3316, loss = 2.62226
step 3317, loss = 2.51334
step 3318, loss = 2.44537
step 3319, loss = 2.47696
step 3320, loss = 2.57201
training 3321, accuracy = 0.287671
step 3321, loss = 2.52384
step 3322, loss = 2.45473
step 3323, loss = 2.65034
step 3324, loss = 2.48537
step 3325, loss = 2.44972
step 3326, loss = 2.42899
step 3327, loss = 2.53575
step 3328, loss = 2.47847
step 3329, loss = 2.49119
step 3330, loss = 2.4508
training 3331, accuracy = 0.291139
step 3331, loss = 2.40344
step 3332, loss = 2.45608
step 3333, loss = 2.56182
step 3334, loss = 2.51322
step 3335, loss = 2.5587
step 3336, loss = 2.50509
step 3337, loss = 2.37659
step 3338, loss = 2.44395
step 3339, loss = 2.42579
step 3340, loss = 2.45362
training 3341, accuracy = 0.047170
step 3341, loss = 2.4593
step 3342, loss = 2.40147
step 3343, loss = 2.48818
step 3344, loss = 2.64611
step 3345, loss = 2.37113
step 3346, loss = 2.42483
step 3347, loss = 2.3954
step 3348, loss = 2.48622
step 3349, loss = 2.51035
step 3350, loss = 2.38768
training 3351, accuracy = 0.614286
step 3351, loss = 2.42753
step 3352, loss = 3.66544
step 3353, loss = 3.80394
step 3354, loss = 2.67713
step 3355, loss = 2.59855
step 3356, loss = 2.47318
step 3357, loss = 2.44353
step 3358, loss = 2.74852
step 3359, loss = 2.67873
step 3360, loss = 2.62671
training 3361, accuracy = 0.175439
step 3361, loss = 2.54473
step 3362, loss = 2.50908
step 3363, loss = 2.38115
step 3364, loss = 2.53294
step 3365, loss = 2.7858
step 3366, loss = 2.73002
step 3367, loss = 2.5559
step 3368, loss = 2.4914
step 3369, loss = 2.5123
step 3370, loss = 2.51181
training 3371, accuracy = 0.477612
step 3371, loss = 2.45006
step 3372, loss = 2.49196
step 3373, loss = 2.43828
step 3374, loss = 2.3809
step 3375, loss = 2.54328
step 3376, loss = 2.61974
step 3377, loss = 2.49247
step 3378, loss = 2.4415
step 3379, loss = 2.35834
step 3380, loss = 2.44937
training 3381, accuracy = 0.302083
step 3381, loss = 2.38328
step 3382, loss = 2.3983
step 3383, loss = 2.4937
step 3384, loss = 2.40471
step 3385, loss = 2.5117
step 3386, loss = 2.42206
step 3387, loss = 2.38137
step 3388, loss = 4.16282
step 3389, loss = 2.79456
step 3390, loss = 3.12991
training 3391, accuracy = 0.045455
step 3391, loss = 3.16804
step 3392, loss = 2.68489
step 3393, loss = 2.51982
step 3394, loss = 2.35754
step 3395, loss = 2.43464
step 3396, loss = 2.30113
step 3397, loss = 4.24074
step 3398, loss = 3.08064
step 3399, loss = 2.84741
step 3400, loss = 2.34935
training 3401, accuracy = 0.284644
step 3401, loss = 2.5302
step 3402, loss = 2.39903
step 3403, loss = 2.54672
step 3404, loss = 2.3003
step 3405, loss = 2.401
step 3406, loss = 2.408
step 3407, loss = 3.02293
step 3408, loss = 2.5779
step 3409, loss = 2.38445
step 3410, loss = 2.57987
training 3411, accuracy = 0.000000
step 3411, loss = 2.38216
step 3412, loss = 2.44211
step 3413, loss = 2.56495
step 3414, loss = 2.47892
step 3415, loss = 2.73593
step 3416, loss = 2.37928
step 3417, loss = 2.3089
step 3418, loss = 2.37745
step 3419, loss = 2.3988
step 3420, loss = 2.43287
training 3421, accuracy = 0.146667
step 3421, loss = 2.37789
step 3422, loss = 2.4088
step 3423, loss = 2.39647
step 3424, loss = 2.36558
step 3425, loss = 2.59891
step 3426, loss = 2.75723
step 3427, loss = 2.4477
step 3428, loss = 2.40218
step 3429, loss = 2.42081
step 3430, loss = 2.39978
training 3431, accuracy = 0.146067
step 3431, loss = 2.47957
step 3432, loss = 2.31377
step 3433, loss = 2.26379
step 3434, loss = 2.28011
step 3435, loss = 2.41653
step 3436, loss = 2.33246
step 3437, loss = 2.40593
step 3438, loss = 2.28151
step 3439, loss = 2.58334
step 3440, loss = 2.38411
training 3441, accuracy = 0.835938
step 3441, loss = 2.36297
step 3442, loss = 2.40265
step 3443, loss = 2.36453
step 3444, loss = 2.62524
step 3445, loss = 2.37749
step 3446, loss = 2.22876
step 3447, loss = 2.2924
step 3448, loss = 2.43579
step 3449, loss = 2.38245
step 3450, loss = 2.38653
training 3451, accuracy = 0.333333
step 3451, loss = 2.44647
step 3452, loss = 2.38856
step 3453, loss = 2.87866
step 3454, loss = 2.36194
step 3455, loss = 2.31771
step 3456, loss = 2.28929
step 3457, loss = 2.43693
step 3458, loss = 2.39588
step 3459, loss = 2.31276
step 3460, loss = 2.38313
training 3461, accuracy = 0.840000
step 3461, loss = 2.42845
step 3462, loss = 2.26232
step 3463, loss = 2.25045
step 3464, loss = 2.48028
step 3465, loss = 2.29977
step 3466, loss = 2.35839
step 3467, loss = 2.30837
step 3468, loss = 2.26748
step 3469, loss = 2.36806
step 3470, loss = 2.327
training 3471, accuracy = 0.911765
step 3471, loss = 2.27457
step 3472, loss = 2.32559
step 3473, loss = 2.37429
step 3474, loss = 2.60117
step 3475, loss = 2.36371
step 3476, loss = 2.23426
step 3477, loss = 3.14858
step 3478, loss = 2.44329
step 3479, loss = 2.42454
step 3480, loss = 2.70399
training 3481, accuracy = 0.337838
step 3481, loss = 2.50388
step 3482, loss = 2.52963
step 3483, loss = 2.58495
step 3484, loss = 2.46333
step 3485, loss = 2.48406
step 3486, loss = 2.31841
step 3487, loss = 2.20782
step 3488, loss = 2.33903
step 3489, loss = 2.28976
step 3490, loss = 2.33582
training 3491, accuracy = 0.547945
step 3491, loss = 2.23015
step 3492, loss = 2.26078
step 3493, loss = 2.31175
step 3494, loss = 2.29614
step 3495, loss = 2.26125
step 3496, loss = 2.34314
step 3497, loss = 2.32555
step 3498, loss = 2.26994
step 3499, loss = 2.28001
step 3500, loss = 2.22544
training 3501, accuracy = 0.068027
step 3501, loss = 2.35286
step 3502, loss = 2.23529
step 3503, loss = 2.18915
step 3504, loss = 2.37121
step 3505, loss = 2.30096
step 3506, loss = 2.20441
step 3507, loss = 2.18278
step 3508, loss = 2.30874
step 3509, loss = 2.3057
step 3510, loss = 2.21351
training 3511, accuracy = 0.056522
step 3511, loss = 2.29462
step 3512, loss = 2.20098
step 3513, loss = 2.17573
step 3514, loss = 2.20739
step 3515, loss = 2.30606
step 3516, loss = 2.22875
step 3517, loss = 2.29871
step 3518, loss = 2.17245
step 3519, loss = 2.56071
step 3520, loss = 2.24559
training 3521, accuracy = 0.241071
step 3521, loss = 2.35912
step 3522, loss = 2.26882
step 3523, loss = 2.14337
step 3524, loss = 2.32137
step 3525, loss = 2.22834
step 3526, loss = 2.34405
step 3527, loss = 2.22324
step 3528, loss = 2.21687
step 3529, loss = 2.17072
step 3530, loss = 4.01152
training 3531, accuracy = 0.333333
step 3531, loss = 2.96881
step 3532, loss = 4.12527
step 3533, loss = 2.91032
step 3534, loss = 2.65235
step 3535, loss = 2.35232
step 3536, loss = 2.48671
step 3537, loss = 2.31823
step 3538, loss = 2.2999
step 3539, loss = 2.29027
step 3540, loss = 2.18887
training 3541, accuracy = 0.649718
step 3541, loss = 3.2241
step 3542, loss = 2.40614
step 3543, loss = 2.41213
step 3544, loss = 2.17364
step 3545, loss = 2.92542
step 3546, loss = 2.28326
step 3547, loss = 2.34663
step 3548, loss = 2.2064
step 3549, loss = 2.19937
step 3550, loss = 2.20365
training 3551, accuracy = 0.095238
step 3551, loss = 2.31797
step 3552, loss = 2.23953
step 3553, loss = 2.27418
step 3554, loss = 2.19913
step 3555, loss = 2.20388
step 3556, loss = 2.11021
step 3557, loss = 2.15779
step 3558, loss = 2.15611
step 3559, loss = 2.14953
step 3560, loss = 2.57738
training 3561, accuracy = 0.363636
step 3561, loss = 2.32903
step 3562, loss = 2.10645
step 3563, loss = 2.13618
step 3564, loss = 2.1291
step 3565, loss = 2.28626
step 3566, loss = 2.32745
step 3567, loss = 2.13384
step 3568, loss = 2.28913
step 3569, loss = 3.25695
step 3570, loss = 2.36118
training 3571, accuracy = 0.757962
step 3571, loss = 2.54744
step 3572, loss = 2.4163
step 3573, loss = 2.32945
step 3574, loss = 2.70564
step 3575, loss = 2.30539
step 3576, loss = 2.26416
step 3577, loss = 2.18586
step 3578, loss = 2.27334
step 3579, loss = 2.09979
step 3580, loss = 2.1233
training 3581, accuracy = 0.283237
step 3581, loss = 2.15792
step 3582, loss = 2.23975
step 3583, loss = 2.26851
step 3584, loss = 2.45466
step 3585, loss = 2.12113
step 3586, loss = 2.23609
step 3587, loss = 2.29311
step 3588, loss = 2.08578
step 3589, loss = 2.2537
step 3590, loss = 2.11263
training 3591, accuracy = 0.551351
step 3591, loss = 2.16752
step 3592, loss = 2.29709
step 3593, loss = 2.18657
step 3594, loss = 2.11546
step 3595, loss = 2.11786
step 3596, loss = 2.11882
step 3597, loss = 2.06376
step 3598, loss = 2.14182
step 3599, loss = 2.14317
step 3600, loss = 2.13235
training 3601, accuracy = 0.342857
step 3601, loss = 2.18649
step 3602, loss = 2.29842
step 3603, loss = 2.09793
step 3604, loss = 2.07007
step 3605, loss = 2.09805
step 3606, loss = 2.08434
step 3607, loss = 2.20717
step 3608, loss = 2.87154
step 3609, loss = 2.97953
step 3610, loss = 2.15541
training 3611, accuracy = 0.557377
step 3611, loss = 2.34814
step 3612, loss = 2.38655
step 3613, loss = 2.10801
step 3614, loss = 2.25558
step 3615, loss = 2.19922
step 3616, loss = 2.1044
step 3617, loss = 2.17009
step 3618, loss = 2.07431
step 3619, loss = 2.35258
step 3620, loss = 2.09635
training 3621, accuracy = 0.067164
step 3621, loss = 2.18624
step 3622, loss = 2.18887
step 3623, loss = 2.08586
step 3624, loss = 2.09169
step 3625, loss = 2.22777
step 3626, loss = 2.14114
step 3627, loss = 2.22017
step 3628, loss = 2.04956
step 3629, loss = 2.17001
step 3630, loss = 2.09547
training 3631, accuracy = 0.308943
step 3631, loss = 2.43491
step 3632, loss = 2.27261
step 3633, loss = 2.12197
step 3634, loss = 2.16028
step 3635, loss = 2.24311
step 3636, loss = 2.27961
step 3637, loss = 2.08442
step 3638, loss = 2.19887
step 3639, loss = 2.05839
step 3640, loss = 2.14519
training 3641, accuracy = 0.213483
step 3641, loss = 2.05816
step 3642, loss = 2.29128
step 3643, loss = 2.09267
step 3644, loss = 2.06499
step 3645, loss = 2.1525
step 3646, loss = 2.15188
step 3647, loss = 2.07909
step 3648, loss = 2.27777
step 3649, loss = 2.25534
step 3650, loss = 2.01589
training 3651, accuracy = 0.118519
step 3651, loss = 2.07949
step 3652, loss = 2.21061
step 3653, loss = 1.9918
step 3654, loss = 2.24543
step 3655, loss = 2.06186
step 3656, loss = 2.12221
step 3657, loss = 2.16358
step 3658, loss = 2.06924
step 3659, loss = 2.1118
step 3660, loss = 2.06749
training 3661, accuracy = 0.357143
step 3661, loss = 2.5029
step 3662, loss = 2.10099
step 3663, loss = 2.2203
step 3664, loss = 2.19387
step 3665, loss = 2.10979
step 3666, loss = 2.11192
step 3667, loss = 2.09003
step 3668, loss = 2.07567
step 3669, loss = 1.98134
step 3670, loss = 2.01683
training 3671, accuracy = 0.097345
step 3671, loss = 2.2163
step 3672, loss = 2.02471
step 3673, loss = 2.06203
step 3674, loss = 2.11671
step 3675, loss = 2.12742
step 3676, loss = 2.0247
step 3677, loss = 2.20536
step 3678, loss = 2.09483
step 3679, loss = 2.00775
step 3680, loss = 2.0368
training 3681, accuracy = 0.057554
step 3681, loss = 2.09067
step 3682, loss = 2.00904
step 3683, loss = 2.04357
step 3684, loss = 2.4267
step 3685, loss = 2.02382
step 3686, loss = 2.30148
step 3687, loss = 1.98833
step 3688, loss = 2.19922
step 3689, loss = 2.02306
step 3690, loss = 2.04894
training 3691, accuracy = 0.102564
step 3691, loss = 1.89859
step 3692, loss = 2.08658
step 3693, loss = 2.01769
step 3694, loss = 2.04902
step 3695, loss = 1.99689
step 3696, loss = 2.48293
step 3697, loss = 2.34638
step 3698, loss = 2.08786
step 3699, loss = 2.09757
step 3700, loss = 2.05579
training 3701, accuracy = 0.550000
step 3701, loss = 2.04688
step 3702, loss = 2.05825
step 3703, loss = 2.20381
step 3704, loss = 1.96631
step 3705, loss = 2.04849
step 3706, loss = 2.16573
step 3707, loss = 2.5878
step 3708, loss = 2.28233
step 3709, loss = 2.17156
step 3710, loss = 1.97239
training 3711, accuracy = 0.094737
step 3711, loss = 2.05478
step 3712, loss = 2.03912
step 3713, loss = 2.04177
step 3714, loss = 2.02702
step 3715, loss = 2.12862
step 3716, loss = 2.01177
step 3717, loss = 1.97302
step 3718, loss = 2.02285
step 3719, loss = 2.13056
step 3720, loss = 2.05123
training 3721, accuracy = 0.295918
step 3721, loss = 1.9704
step 3722, loss = 2.02942
step 3723, loss = 2.14928
step 3724, loss = 2.07475
step 3725, loss = 1.99962
step 3726, loss = 1.97038
step 3727, loss = 2.11012
step 3728, loss = 2.02953
step 3729, loss = 2.00735
step 3730, loss = 2.76582
training 3731, accuracy = 0.269231
step 3731, loss = 1.99231
step 3732, loss = 2.26377
step 3733, loss = 2.13499
step 3734, loss = 1.93968
step 3735, loss = 2.01969
step 3736, loss = 1.96215
step 3737, loss = 1.94074
step 3738, loss = 2.07749
step 3739, loss = 2.14114
step 3740, loss = 1.98934
training 3741, accuracy = 0.426471
step 3741, loss = 1.99079
step 3742, loss = 2.08023
step 3743, loss = 2.43431
step 3744, loss = 2.12111
step 3745, loss = 1.94553
step 3746, loss = 1.92696
step 3747, loss = 2.0629
step 3748, loss = 2.05733
step 3749, loss = 2.16353
step 3750, loss = 2.13798
training 3751, accuracy = 0.460000
step 3751, loss = 2.12952
step 3752, loss = 1.91828
step 3753, loss = 1.92709
step 3754, loss = 2.0777
step 3755, loss = 1.92636
step 3756, loss = 2.00624
step 3757, loss = 1.95217
step 3758, loss = 1.98344
step 3759, loss = 2.06
step 3760, loss = 2.20275
training 3761, accuracy = 0.177778
step 3761, loss = 1.96427
step 3762, loss = 2.03002
step 3763, loss = 2.14841
step 3764, loss = 1.95802
step 3765, loss = 1.99434
step 3766, loss = 2.07619
step 3767, loss = 2.12485
step 3768, loss = 2.02562
step 3769, loss = 2.03212
step 3770, loss = 1.96275
training 3771, accuracy = 0.609023
step 3771, loss = 2.09534
step 3772, loss = 1.97
step 3773, loss = 2.06538
step 3774, loss = 1.94927
step 3775, loss = 1.96657
step 3776, loss = 1.9257
step 3777, loss = 2.00618
step 3778, loss = 1.89299
step 3779, loss = 2.32304
step 3780, loss = 2.00025
training 3781, accuracy = 0.679487
step 3781, loss = 1.94706
step 3782, loss = 2.10566
step 3783, loss = 1.95182
step 3784, loss = 1.9716
step 3785, loss = 1.97399
step 3786, loss = 1.90835
step 3787, loss = 2.16658
step 3788, loss = 1.92973
step 3789, loss = 2.03877
step 3790, loss = 2.35948
training 3791, accuracy = 0.482759
step 3791, loss = 2.51093
step 3792, loss = 2.06266
step 3793, loss = 1.88036
step 3794, loss = 1.96355
step 3795, loss = 2.0167
step 3796, loss = 1.92289
step 3797, loss = 2.02495
step 3798, loss = 2.14333
step 3799, loss = 2.31184
step 3800, loss = 1.9691
training 3801, accuracy = 0.090253
step 3801, loss = 2.3861
step 3802, loss = 2.19722
step 3803, loss = 1.88996
step 3804, loss = 1.88759
step 3805, loss = 1.93691
step 3806, loss = 2.19439
step 3807, loss = 1.89404
step 3808, loss = 2.13997
step 3809, loss = 2.64343
step 3810, loss = 2.1304
training 3811, accuracy = 0.442623
step 3811, loss = 1.97737
step 3812, loss = 2.00896
step 3813, loss = 2.01129
step 3814, loss = 2.07971
step 3815, loss = 1.94413
step 3816, loss = 1.93391
step 3817, loss = 2.48098
step 3818, loss = 2.29788
step 3819, loss = 2.03507
step 3820, loss = 2.0328
training 3821, accuracy = 0.217391
step 3821, loss = 1.95818
step 3822, loss = 2.01496
step 3823, loss = 2.16279
step 3824, loss = 1.91136
step 3825, loss = 1.92594
step 3826, loss = 1.91781
step 3827, loss = 1.89709
step 3828, loss = 2.01749
step 3829, loss = 1.97305
step 3830, loss = 1.88958
training 3831, accuracy = 0.706897
step 3831, loss = 1.96625
step 3832, loss = 1.83482
step 3833, loss = 1.90724
step 3834, loss = 1.91235
step 3835, loss = 1.90515
step 3836, loss = 1.98131
step 3837, loss = 1.87732
step 3838, loss = 1.82643
step 3839, loss = 1.93977
step 3840, loss = 1.9047
training 3841, accuracy = 0.666667
step 3841, loss = 1.97103
step 3842, loss = 1.8469
step 3843, loss = 1.87347
step 3844, loss = 1.84043
step 3845, loss = 1.88386
step 3846, loss = 1.89644
step 3847, loss = 1.84836
step 3848, loss = 1.90888
step 3849, loss = 1.90716
step 3850, loss = 1.92155
training 3851, accuracy = 0.097087
step 3851, loss = 1.89765
step 3852, loss = 2.07105
step 3853, loss = 2.03662
step 3854, loss = 1.92513
step 3855, loss = 1.85543
step 3856, loss = 1.91809
step 3857, loss = 1.99424
step 3858, loss = 1.84368
step 3859, loss = 1.80085
step 3860, loss = 1.83463
training 3861, accuracy = 0.356784
step 3861, loss = 1.93558
step 3862, loss = 2.45428
step 3863, loss = 2.70474
step 3864, loss = 2.48464
step 3865, loss = 2.61009
step 3866, loss = 1.87984
step 3867, loss = 2.15283
step 3868, loss = 1.84567
step 3869, loss = 2.1289
step 3870, loss = 1.96205
training 3871, accuracy = 0.181818
step 3871, loss = 1.9155
step 3872, loss = 1.82414
step 3873, loss = 1.88104
step 3874, loss = 1.87619
step 3875, loss = 1.84376
step 3876, loss = 1.98515
step 3877, loss = 1.81782
step 3878, loss = 1.88795
step 3879, loss = 1.93832
step 3880, loss = 1.94351
training 3881, accuracy = 0.387097
step 3881, loss = 1.93656
step 3882, loss = 1.84158
step 3883, loss = 1.88522
step 3884, loss = 1.85612
step 3885, loss = 1.9327
step 3886, loss = 2.02199
step 3887, loss = 1.81827
step 3888, loss = 1.82504
step 3889, loss = 1.77911
step 3890, loss = 1.91743
training 3891, accuracy = 0.240000
step 3891, loss = 1.99672
step 3892, loss = 1.83975
step 3893, loss = 2.07912
step 3894, loss = 1.8606
step 3895, loss = 1.87574
step 3896, loss = 1.96905
step 3897, loss = 2.0629
step 3898, loss = 1.92357
step 3899, loss = 1.79145
step 3900, loss = 1.95153
training 3901, accuracy = 0.296296
step 3901, loss = 1.80538
step 3902, loss = 1.91915
step 3903, loss = 1.87123
step 3904, loss = 1.92364
step 3905, loss = 1.96228
step 3906, loss = 1.89796
step 3907, loss = 1.92627
step 3908, loss = 1.85179
step 3909, loss = 1.92919
step 3910, loss = 1.80778
training 3911, accuracy = 0.051852
step 3911, loss = 1.94292
step 3912, loss = 1.812
step 3913, loss = 1.88388
step 3914, loss = 2.03136
step 3915, loss = 1.80691
step 3916, loss = 1.80237
step 3917, loss = 1.84393
step 3918, loss = 1.76623
step 3919, loss = 1.78488
step 3920, loss = 1.81062
training 3921, accuracy = 0.472050
step 3921, loss = 1.82563
step 3922, loss = 1.72918
step 3923, loss = 1.99642
step 3924, loss = 1.95729
step 3925, loss = 2.18804
step 3926, loss = 1.84784
step 3927, loss = 1.84595
step 3928, loss = 1.80428
step 3929, loss = 1.75413
step 3930, loss = 1.90269
training 3931, accuracy = 0.092308
step 3931, loss = 1.92399
step 3932, loss = 2.01532
step 3933, loss = 1.81841
step 3934, loss = 1.83278
step 3935, loss = 1.83377
step 3936, loss = 1.8622
step 3937, loss = 1.84526
step 3938, loss = 1.92352
step 3939, loss = 2.22412
step 3940, loss = 1.94545
training 3941, accuracy = 0.179487
step 3941, loss = 1.79696
step 3942, loss = 1.89253
step 3943, loss = 1.76804
step 3944, loss = 1.93107
step 3945, loss = 1.81419
step 3946, loss = 2.12841
step 3947, loss = 1.76956
step 3948, loss = 1.94074
step 3949, loss = 1.79104
step 3950, loss = 1.76114
training 3951, accuracy = 0.447514
step 3951, loss = 1.844
step 3952, loss = 1.74223
step 3953, loss = 1.85641
step 3954, loss = 1.80232
step 3955, loss = 1.83455
step 3956, loss = 1.78368
step 3957, loss = 1.88364
step 3958, loss = 2.07655
step 3959, loss = 1.82448
step 3960, loss = 1.82569
training 3961, accuracy = 0.217949
step 3961, loss = 1.8032
step 3962, loss = 1.91679
step 3963, loss = 1.84571
step 3964, loss = 1.82361
step 3965, loss = 1.80991
step 3966, loss = 1.72775
step 3967, loss = 1.79365
step 3968, loss = 1.7523
step 3969, loss = 1.76624
step 3970, loss = 1.80391
training 3971, accuracy = 0.060606
step 3971, loss = 1.76056
step 3972, loss = 1.87286
step 3973, loss = 1.80375
step 3974, loss = 1.91626
step 3975, loss = 1.76094
step 3976, loss = 1.87366
step 3977, loss = 1.94083
step 3978, loss = 1.94445
step 3979, loss = 1.93262
step 3980, loss = 1.76292
training 3981, accuracy = 0.242236
step 3981, loss = 1.75319
step 3982, loss = 1.88065
step 3983, loss = 1.92399
step 3984, loss = 1.74101
step 3985, loss = 1.72937
step 3986, loss = 1.78366
step 3987, loss = 1.80817
step 3988, loss = 1.71025
step 3989, loss = 1.76713
step 3990, loss = 1.81167
training 3991, accuracy = 0.279412
step 3991, loss = 1.93973
step 3992, loss = 1.96488
step 3993, loss = 1.74622
step 3994, loss = 1.80096
step 3995, loss = 1.73266
step 3996, loss = 1.78387
step 3997, loss = 1.73227
step 3998, loss = 1.72876
step 3999, loss = 1.70039
step 4000, loss = 1.77553
training 4001, accuracy = 0.453846
step 4001, loss = 1.87954
step 4002, loss = 1.77256
step 4003, loss = 1.7776
step 4004, loss = 1.93923
step 4005, loss = 1.83615
step 4006, loss = 1.93478
step 4007, loss = 1.74491
step 4008, loss = 1.9403
step 4009, loss = 1.92799
step 4010, loss = 1.74198
training 4011, accuracy = 0.000000
step 4011, loss = 1.69053
step 4012, loss = 1.80877
step 4013, loss = 1.93536
step 4014, loss = 1.87694
step 4015, loss = 1.98164
step 4016, loss = 1.77581
step 4017, loss = 1.86771
step 4018, loss = 1.7434
step 4019, loss = 1.8446
step 4020, loss = 1.8116
training 4021, accuracy = 0.871429
step 4021, loss = 1.74707
step 4022, loss = 1.8487
step 4023, loss = 1.74083
step 4024, loss = 2.1001
step 4025, loss = 2.98715
step 4026, loss = 1.75339
step 4027, loss = 1.93166
step 4028, loss = 1.97835
step 4029, loss = 1.73061
step 4030, loss = 1.90964
training 4031, accuracy = 0.232558
step 4031, loss = 1.72618
step 4032, loss = 1.74569
step 4033, loss = 1.74894
step 4034, loss = 1.67521
step 4035, loss = 1.85306
step 4036, loss = 1.71264
step 4037, loss = 1.73676
step 4038, loss = 1.7282
step 4039, loss = 1.79212
step 4040, loss = 1.79024
training 4041, accuracy = 0.154639
step 4041, loss = 1.79701
step 4042, loss = 2.02403
step 4043, loss = 1.70422
step 4044, loss = 1.78692
step 4045, loss = 2.20307
step 4046, loss = 2.03644
step 4047, loss = 1.83678
step 4048, loss = 1.85256
step 4049, loss = 1.77771
step 4050, loss = 3.99743
training 4051, accuracy = 0.384615
step 4051, loss = 2.02186
step 4052, loss = 2.09748
step 4053, loss = 2.36412
step 4054, loss = 2.65921
step 4055, loss = 1.76593
step 4056, loss = 1.68523
step 4057, loss = 1.7037
step 4058, loss = 1.77792
step 4059, loss = 1.83416
step 4060, loss = 2.44339
training 4061, accuracy = 0.950000
step 4061, loss = 1.76184
step 4062, loss = 1.76963
step 4063, loss = 1.88544
step 4064, loss = 2.04501
step 4065, loss = 1.78205
step 4066, loss = 1.87247
step 4067, loss = 2.18716
step 4068, loss = 1.68457
step 4069, loss = 1.70121
step 4070, loss = 1.85497
training 4071, accuracy = 0.516667
step 4071, loss = 1.71187
step 4072, loss = 1.74704
step 4073, loss = 1.85398
step 4074, loss = 1.74257
step 4075, loss = 1.73308
step 4076, loss = 1.67323
step 4077, loss = 1.65641
step 4078, loss = 1.75135
step 4079, loss = 1.67372
step 4080, loss = 1.7312
training 4081, accuracy = 0.025316
step 4081, loss = 1.79168
step 4082, loss = 1.65638
step 4083, loss = 1.60041
step 4084, loss = 1.75949
step 4085, loss = 1.83811
step 4086, loss = 1.81189
step 4087, loss = 1.65321
step 4088, loss = 1.81474
step 4089, loss = 1.88486
step 4090, loss = 1.90159
training 4091, accuracy = 0.315789
step 4091, loss = 1.86014
step 4092, loss = 1.68739
step 4093, loss = 1.67636
step 4094, loss = 3.63302
step 4095, loss = 2.05255
step 4096, loss = 2.02038
step 4097, loss = 2.29055
step 4098, loss = 2.1827
step 4099, loss = 1.86375
step 4100, loss = 2.07161
training 4101, accuracy = 0.033333
step 4101, loss = 1.65803
step 4102, loss = 1.79927
step 4103, loss = 1.74842
step 4104, loss = 1.79793
step 4105, loss = 1.79189
step 4106, loss = 1.71323
step 4107, loss = 1.72531
step 4108, loss = 2.14323
step 4109, loss = 1.78703
step 4110, loss = 1.78876
training 4111, accuracy = 0.250000
step 4111, loss = 1.79517
step 4112, loss = 1.76639
step 4113, loss = 1.70908
step 4114, loss = 1.80347
step 4115, loss = 1.6504
step 4116, loss = 1.76771
step 4117, loss = 1.8071
step 4118, loss = 1.72193
step 4119, loss = 1.69146
step 4120, loss = 1.76751
training 4121, accuracy = 0.205128
step 4121, loss = 1.80223
step 4122, loss = 2.00905
step 4123, loss = 1.92114
step 4124, loss = 1.69848
step 4125, loss = 1.64334
step 4126, loss = 1.77636
step 4127, loss = 1.86255
step 4128, loss = 1.64704
step 4129, loss = 1.76605
step 4130, loss = 1.68909
training 4131, accuracy = 0.732673
step 4131, loss = 1.85586
step 4132, loss = 1.85552
step 4133, loss = 1.6917
step 4134, loss = 1.66689
step 4135, loss = 1.63847
step 4136, loss = 1.66523
step 4137, loss = 1.65149
step 4138, loss = 1.63302
step 4139, loss = 1.74105
step 4140, loss = 1.61426
training 4141, accuracy = 0.244444
step 4141, loss = 1.67752
step 4142, loss = 1.67344
step 4143, loss = 1.77193
step 4144, loss = 1.67545
step 4145, loss = 1.70067
step 4146, loss = 1.76471
step 4147, loss = 1.75472
step 4148, loss = 1.74895
step 4149, loss = 1.84716
step 4150, loss = 1.61953
training 4151, accuracy = 0.333333
step 4151, loss = 1.65374
step 4152, loss = 1.80518
step 4153, loss = 1.61847
step 4154, loss = 1.59073
step 4155, loss = 1.61332
step 4156, loss = 1.70614
step 4157, loss = 1.68359
step 4158, loss = 1.68393
step 4159, loss = 1.66696
step 4160, loss = 1.66059
training 4161, accuracy = 0.107143
step 4161, loss = 1.83792
step 4162, loss = 1.95844
step 4163, loss = 1.61479
step 4164, loss = 1.63632
step 4165, loss = 1.71957
step 4166, loss = 1.93571
step 4167, loss = 1.66037
step 4168, loss = 1.78856
step 4169, loss = 1.63538
step 4170, loss = 2.2229
training 4171, accuracy = 0.066667
step 4171, loss = 1.79376
step 4172, loss = 1.65536
step 4173, loss = 1.69808
step 4174, loss = 1.6664
step 4175, loss = 1.71225
step 4176, loss = 1.5841
step 4177, loss = 1.60031
step 4178, loss = 1.78098
step 4179, loss = 1.60285
step 4180, loss = 1.77692
training 4181, accuracy = 0.214286
step 4181, loss = 1.61173
step 4182, loss = 1.67323
step 4183, loss = 1.64509
step 4184, loss = 1.72522
step 4185, loss = 1.68757
step 4186, loss = 1.75396
step 4187, loss = 1.65792
step 4188, loss = 1.59776
step 4189, loss = 1.7356
step 4190, loss = 1.61102
training 4191, accuracy = 0.409091
step 4191, loss = 1.6103
step 4192, loss = 1.64773
step 4193, loss = 1.6045
step 4194, loss = 1.71434
step 4195, loss = 1.61129
step 4196, loss = 1.6281
step 4197, loss = 1.61809
step 4198, loss = 1.67022
step 4199, loss = 1.67042
step 4200, loss = 1.58717
training 4201, accuracy = 0.736559
step 4201, loss = 1.59129
step 4202, loss = 1.58016
step 4203, loss = 1.69309
step 4204, loss = 1.65626
step 4205, loss = 1.76375
step 4206, loss = 1.66584
step 4207, loss = 1.58453
step 4208, loss = 1.72929
step 4209, loss = 1.55588
step 4210, loss = 1.77948
training 4211, accuracy = 0.702128
step 4211, loss = 1.64183
step 4212, loss = 1.5645
step 4213, loss = 2.05278
step 4214, loss = 2.23675
step 4215, loss = 1.76862
step 4216, loss = 1.65711
step 4217, loss = 1.60127
step 4218, loss = 1.67569
step 4219, loss = 1.75267
step 4220, loss = 1.67263
training 4221, accuracy = 0.271739
step 4221, loss = 1.745
step 4222, loss = 3.28188
step 4223, loss = 2.45909
step 4224, loss = 2.07216
step 4225, loss = 1.71444
step 4226, loss = 2.63793
step 4227, loss = 2.4411
step 4228, loss = 1.78963
step 4229, loss = 1.76021
step 4230, loss = 1.90972
training 4231, accuracy = 0.126316
step 4231, loss = 1.72537
step 4232, loss = 1.71357
step 4233, loss = 1.80601
step 4234, loss = 1.62993
step 4235, loss = 1.61254
step 4236, loss = 1.83288
step 4237, loss = 1.95319
step 4238, loss = 1.62727
step 4239, loss = 1.56112
step 4240, loss = 1.5208
training 4241, accuracy = 0.183976
step 4241, loss = 1.81421
step 4242, loss = 2.089
step 4243, loss = 2.40334
step 4244, loss = 1.71435
step 4245, loss = 1.57628
step 4246, loss = 1.96431
step 4247, loss = 1.63334
step 4248, loss = 1.62075
step 4249, loss = 1.63024
step 4250, loss = 1.52282
training 4251, accuracy = 0.848485
step 4251, loss = 1.53712
step 4252, loss = 1.5439
step 4253, loss = 1.60273
step 4254, loss = 1.52993
step 4255, loss = 1.51803
step 4256, loss = 1.68289
step 4257, loss = 2.13778
step 4258, loss = 1.75983
step 4259, loss = 1.77125
step 4260, loss = 1.65958
training 4261, accuracy = 0.320000
step 4261, loss = 1.81631
step 4262, loss = 1.65822
step 4263, loss = 2.11837
step 4264, loss = 1.87322
step 4265, loss = 1.84187
step 4266, loss = 1.73717
step 4267, loss = 1.71668
step 4268, loss = 1.62774
step 4269, loss = 1.6441
step 4270, loss = 1.67698
training 4271, accuracy = 0.081967
step 4271, loss = 1.78733
step 4272, loss = 1.53381
step 4273, loss = 1.81565
step 4274, loss = 1.69056
step 4275, loss = 1.48663
step 4276, loss = 1.60303
step 4277, loss = 1.48894
step 4278, loss = 1.57065
step 4279, loss = 1.50786
step 4280, loss = 1.59205
training 4281, accuracy = 0.035714
step 4281, loss = 1.59899
step 4282, loss = 1.56055
step 4283, loss = 1.54969
step 4284, loss = 1.57772
step 4285, loss = 1.52784
step 4286, loss = 1.53321
step 4287, loss = 1.53517
step 4288, loss = 1.53814
step 4289, loss = 2.44607
step 4290, loss = 1.62301
training 4291, accuracy = 0.347826
step 4291, loss = 1.86097
step 4292, loss = 1.56265
step 4293, loss = 1.73104
step 4294, loss = 1.48126
step 4295, loss = 1.5453
step 4296, loss = 1.63206
step 4297, loss = 1.61437
step 4298, loss = 1.51393
step 4299, loss = 1.53178
step 4300, loss = 1.49623
training 4301, accuracy = 0.268595
step 4301, loss = 1.62755
step 4302, loss = 1.49273
step 4303, loss = 1.5911
step 4304, loss = 1.59489
step 4305, loss = 1.91192
step 4306, loss = 1.62958
step 4307, loss = 1.64163
step 4308, loss = 1.67369
step 4309, loss = 1.60693
step 4310, loss = 1.65816
training 4311, accuracy = 0.557692
step 4311, loss = 1.60649
step 4312, loss = 1.66922
step 4313, loss = 1.59081
step 4314, loss = 2.1701
step 4315, loss = 1.52568
step 4316, loss = 1.69048
step 4317, loss = 1.52252
step 4318, loss = 1.67504
step 4319, loss = 1.51107
step 4320, loss = 1.62083
training 4321, accuracy = 0.000000
step 4321, loss = 1.51517
step 4322, loss = 1.61734
step 4323, loss = 1.47673
step 4324, loss = 1.53378
step 4325, loss = 1.6449
step 4326, loss = 1.52674
step 4327, loss = 1.52495
step 4328, loss = 1.5247
step 4329, loss = 1.89936
step 4330, loss = 1.49479
training 4331, accuracy = 0.713333
step 4331, loss = 1.47074
step 4332, loss = 1.5715
step 4333, loss = 1.44344
step 4334, loss = 1.46176
step 4335, loss = 1.51461
step 4336, loss = 1.54739
step 4337, loss = 1.55705
step 4338, loss = 1.48072
step 4339, loss = 1.58207
step 4340, loss = 1.56658
training 4341, accuracy = 0.500000
step 4341, loss = 1.71248
step 4342, loss = 1.55936
step 4343, loss = 1.52628
step 4344, loss = 1.48065
step 4345, loss = 1.7692
step 4346, loss = 1.46937
step 4347, loss = 1.63321
step 4348, loss = 1.49
step 4349, loss = 1.62073
step 4350, loss = 1.62116
training 4351, accuracy = 0.426230
step 4351, loss = 1.94498
step 4352, loss = 1.48887
step 4353, loss = 2.01766
step 4354, loss = 1.79794
step 4355, loss = 1.83118
step 4356, loss = 1.63575
step 4357, loss = 1.447
step 4358, loss = 1.46286
step 4359, loss = 1.56298
step 4360, loss = 1.53484
training 4361, accuracy = 0.489796
step 4361, loss = 1.53334
step 4362, loss = 1.49178
step 4363, loss = 1.52659
step 4364, loss = 1.77595
step 4365, loss = 1.72698
step 4366, loss = 1.61536
step 4367, loss = 1.54658
step 4368, loss = 1.73441
step 4369, loss = 1.51052
step 4370, loss = 1.44766
training 4371, accuracy = 0.137795
step 4371, loss = 1.48269
step 4372, loss = 1.45616
step 4373, loss = 1.48453
step 4374, loss = 1.4694
step 4375, loss = 1.53141
step 4376, loss = 1.45879
step 4377, loss = 1.82177
step 4378, loss = 1.57847
step 4379, loss = 1.51103
step 4380, loss = 1.48273
training 4381, accuracy = 0.318182
step 4381, loss = 1.47478
step 4382, loss = 1.44072
step 4383, loss = 1.63413
step 4384, loss = 1.57409
step 4385, loss = 1.5968
step 4386, loss = 1.47205
step 4387, loss = 1.48551
step 4388, loss = 1.49333
step 4389, loss = 1.46577
step 4390, loss = 1.48999
training 4391, accuracy = 0.185714
step 4391, loss = 1.60143
step 4392, loss = 1.47914
step 4393, loss = 1.47968
step 4394, loss = 1.4264
step 4395, loss = 1.49145
step 4396, loss = 3.1924
step 4397, loss = 1.5711
step 4398, loss = 1.81656
step 4399, loss = 1.70276
step 4400, loss = 1.68411
training 4401, accuracy = 0.522523
step 4401, loss = 1.71567
step 4402, loss = 1.67084
step 4403, loss = 1.67519
step 4404, loss = 1.49494
step 4405, loss = 1.43104
step 4406, loss = 1.50772
step 4407, loss = 1.45087
step 4408, loss = 1.43051
step 4409, loss = 1.54573
step 4410, loss = 1.49809
training 4411, accuracy = 0.824074
step 4411, loss = 1.49147
step 4412, loss = 1.54101
step 4413, loss = 1.49173
step 4414, loss = 1.52588
step 4415, loss = 1.41351
step 4416, loss = 1.55695
step 4417, loss = 1.49124
step 4418, loss = 1.56755
step 4419, loss = 1.54958
step 4420, loss = 1.44896
training 4421, accuracy = 0.293750
step 4421, loss = 1.60686
step 4422, loss = 1.65619
step 4423, loss = 1.49411
step 4424, loss = 1.85884
step 4425, loss = 1.47276
step 4426, loss = 1.64006
step 4427, loss = 1.78024
step 4428, loss = 1.60265
step 4429, loss = 1.41786
step 4430, loss = 1.45592
training 4431, accuracy = 0.909722
step 4431, loss = 1.5475
step 4432, loss = 1.42251
step 4433, loss = 1.47241
step 4434, loss = 1.71549
step 4435, loss = 1.66161
step 4436, loss = 1.9554
step 4437, loss = 1.8686
step 4438, loss = 1.41311
step 4439, loss = 1.45638
step 4440, loss = 1.97318
training 4441, accuracy = 0.142857
step 4441, loss = 1.51276
step 4442, loss = 1.60987
step 4443, loss = 1.77089
step 4444, loss = 1.45297
step 4445, loss = 1.45808
step 4446, loss = 1.52537
step 4447, loss = 1.51743
step 4448, loss = 1.53485
step 4449, loss = 1.56951
step 4450, loss = 2.51858
training 4451, accuracy = 0.500000
step 4451, loss = 2.3552
step 4452, loss = 2.07513
step 4453, loss = 1.52012
step 4454, loss = 1.64122
step 4455, loss = 1.92895
step 4456, loss = 1.8021
step 4457, loss = 1.49243
step 4458, loss = 1.50897
step 4459, loss = 1.53969
step 4460, loss = 1.55308
training 4461, accuracy = 0.095238
step 4461, loss = 1.43989
step 4462, loss = 1.53587
step 4463, loss = 1.49113
step 4464, loss = 1.46685
step 4465, loss = 1.52838
step 4466, loss = 1.48296
step 4467, loss = 1.44237
step 4468, loss = 1.54533
step 4469, loss = 1.55673
step 4470, loss = 1.46512
training 4471, accuracy = 0.038462
step 4471, loss = 1.4127
step 4472, loss = 1.47659
step 4473, loss = 1.39686
step 4474, loss = 1.40693
step 4475, loss = 1.68613
step 4476, loss = 1.49286
step 4477, loss = 1.38959
step 4478, loss = 1.45421
step 4479, loss = 1.53722
step 4480, loss = 1.43876
training 4481, accuracy = 0.134752
step 4481, loss = 1.47836
step 4482, loss = 1.4209
step 4483, loss = 1.47834
step 4484, loss = 1.51918
step 4485, loss = 1.48356
step 4486, loss = 1.56656
step 4487, loss = 1.764
step 4488, loss = 1.41952
step 4489, loss = 1.47616
step 4490, loss = 1.38792
training 4491, accuracy = 0.118182
step 4491, loss = 1.47633
step 4492, loss = 1.52221
step 4493, loss = 1.39779
step 4494, loss = 1.38542
step 4495, loss = 1.43763
step 4496, loss = 1.37609
step 4497, loss = 1.41687
step 4498, loss = 1.44962
step 4499, loss = 1.52557
step 4500, loss = 1.52713
training 4501, accuracy = 0.166667
step 4501, loss = 1.50297
step 4502, loss = 1.40993
step 4503, loss = 1.54634
step 4504, loss = 1.58452
step 4505, loss = 1.48867
step 4506, loss = 1.39075
step 4507, loss = 1.50138
step 4508, loss = 1.63019
step 4509, loss = 1.69742
step 4510, loss = 1.47026
training 4511, accuracy = 0.190000
step 4511, loss = 1.35511
step 4512, loss = 1.39431
step 4513, loss = 1.43199
step 4514, loss = 1.38495
step 4515, loss = 1.37249
step 4516, loss = 1.49765
step 4517, loss = 1.50467
step 4518, loss = 1.4793
step 4519, loss = 2.03278
step 4520, loss = 1.79106
training 4521, accuracy = 0.253731
step 4521, loss = 1.7277
step 4522, loss = 1.30854
step 4523, loss = 1.43967
step 4524, loss = 1.64673
step 4525, loss = 1.67612
step 4526, loss = 1.47839
step 4527, loss = 1.36676
step 4528, loss = 1.51733
step 4529, loss = 1.67055
step 4530, loss = 1.44732
training 4531, accuracy = 0.811966
step 4531, loss = 1.44794
step 4532, loss = 1.32738
step 4533, loss = 1.5254
step 4534, loss = 1.50856
step 4535, loss = 1.4288
step 4536, loss = 1.37381
step 4537, loss = 1.37994
step 4538, loss = 1.45609
step 4539, loss = 1.49319
step 4540, loss = 1.44161
training 4541, accuracy = 0.120879
step 4541, loss = 1.37918
step 4542, loss = 1.35299
step 4543, loss = 1.51494
step 4544, loss = 1.37641
step 4545, loss = 1.50477
step 4546, loss = 1.36142
step 4547, loss = 1.59258
step 4548, loss = 1.48776
step 4549, loss = 1.49104
step 4550, loss = 1.36154
training 4551, accuracy = 0.088235
step 4551, loss = 1.52385
step 4552, loss = 1.38374
step 4553, loss = 1.44754
step 4554, loss = 1.57094
step 4555, loss = 1.40978
step 4556, loss = 1.37962
step 4557, loss = 1.36313
step 4558, loss = 1.49136
step 4559, loss = 1.43604
step 4560, loss = 1.45454
training 4561, accuracy = 0.755319
step 4561, loss = 1.37104
step 4562, loss = 1.41853
step 4563, loss = 1.48503
step 4564, loss = 1.51214
step 4565, loss = 1.32432
step 4566, loss = 1.39088
step 4567, loss = 1.32662
step 4568, loss = 1.70939
step 4569, loss = 1.39156
step 4570, loss = 1.66959
training 4571, accuracy = 0.142857
step 4571, loss = 1.38725
step 4572, loss = 1.43219
step 4573, loss = 1.41973
step 4574, loss = 1.36318
step 4575, loss = 1.47424
step 4576, loss = 1.48852
step 4577, loss = 1.37113
step 4578, loss = 1.45893
step 4579, loss = 1.48966
step 4580, loss = 1.3819
training 4581, accuracy = 0.882812
step 4581, loss = 1.43125
step 4582, loss = 1.4174
step 4583, loss = 1.59245
step 4584, loss = 1.31851
step 4585, loss = 1.6761
step 4586, loss = 1.84648
step 4587, loss = 1.37002
step 4588, loss = 1.32968
step 4589, loss = 1.38993
step 4590, loss = 1.47221
training 4591, accuracy = 0.513889
step 4591, loss = 1.37621
step 4592, loss = 1.32393
step 4593, loss = 1.37109
step 4594, loss = 1.34334
step 4595, loss = 1.47754
step 4596, loss = 1.42603
step 4597, loss = 1.39648
step 4598, loss = 1.41079
step 4599, loss = 1.47269
step 4600, loss = 1.3587
training 4601, accuracy = 0.153846
step 4601, loss = 2.08321
step 4602, loss = 1.67041
step 4603, loss = 1.4495
step 4604, loss = 1.5919
step 4605, loss = 1.58574
step 4606, loss = 1.46716
step 4607, loss = 1.54572
step 4608, loss = 1.81796
step 4609, loss = 1.36452
step 4610, loss = 1.4015
training 4611, accuracy = 0.262411
step 4611, loss = 1.48903
step 4612, loss = 1.47227
step 4613, loss = 1.41979
step 4614, loss = 1.35238
step 4615, loss = 1.58765
step 4616, loss = 1.80078
step 4617, loss = 1.45119
step 4618, loss = 1.35663
step 4619, loss = 1.42828
step 4620, loss = 1.41657
training 4621, accuracy = 0.072072
step 4621, loss = 1.53918
step 4622, loss = 1.37517
step 4623, loss = 1.31885
step 4624, loss = 1.33336
step 4625, loss = 1.45524
step 4626, loss = 1.36056
step 4627, loss = 1.46428
step 4628, loss = 1.42579
step 4629, loss = 1.5094
step 4630, loss = 1.33541
training 4631, accuracy = 0.416667
step 4631, loss = 1.34747
step 4632, loss = 1.28997
step 4633, loss = 1.45477
step 4634, loss = 1.38109
step 4635, loss = 1.3593
step 4636, loss = 1.34563
step 4637, loss = 1.40441
step 4638, loss = 1.3156
step 4639, loss = 1.54013
step 4640, loss = 1.50596
training 4641, accuracy = 0.638298
step 4641, loss = 1.5474
step 4642, loss = 1.27014
step 4643, loss = 1.62119
step 4644, loss = 1.3475
step 4645, loss = 1.32431
step 4646, loss = 1.28463
step 4647, loss = 1.35676
step 4648, loss = 1.28277
step 4649, loss = 1.61153
step 4650, loss = 1.37294
training 4651, accuracy = 0.038760
step 4651, loss = 1.33905
step 4652, loss = 1.36761
step 4653, loss = 1.38153
step 4654, loss = 1.41446
step 4655, loss = 1.29226
step 4656, loss = 1.40287
step 4657, loss = 1.39923
step 4658, loss = 1.42162
step 4659, loss = 1.30625
step 4660, loss = 1.97118
training 4661, accuracy = 0.333333
step 4661, loss = 1.34156
step 4662, loss = 1.33271
step 4663, loss = 1.5173
step 4664, loss = 1.41199
step 4665, loss = 1.42515
step 4666, loss = 1.3883
step 4667, loss = 1.35329
step 4668, loss = 1.29531
step 4669, loss = 1.30264
step 4670, loss = 1.43373
training 4671, accuracy = 0.845070
step 4671, loss = 1.39172
step 4672, loss = 1.30742
step 4673, loss = 1.34049
step 4674, loss = 1.42004
step 4675, loss = 1.28202
step 4676, loss = 1.41209
step 4677, loss = 1.28516
step 4678, loss = 1.39698
step 4679, loss = 1.28937
step 4680, loss = 1.35383
training 4681, accuracy = 0.161017
step 4681, loss = 1.31467
step 4682, loss = 1.3295
step 4683, loss = 1.41934
step 4684, loss = 1.2733
step 4685, loss = 1.37226
step 4686, loss = 1.40624
step 4687, loss = 1.25699
step 4688, loss = 1.78037
step 4689, loss = 1.3509
step 4690, loss = 1.54585
training 4691, accuracy = 0.236842
step 4691, loss = 1.44393
step 4692, loss = 1.52232
step 4693, loss = 1.41506
step 4694, loss = 1.33361
step 4695, loss = 1.41918
step 4696, loss = 1.35395
step 4697, loss = 1.27334
step 4698, loss = 1.3433
step 4699, loss = 1.53921
step 4700, loss = 1.49804
training 4701, accuracy = 0.027778
step 4701, loss = 1.3546
step 4702, loss = 1.25948
step 4703, loss = 1.46679
step 4704, loss = 1.37183
step 4705, loss = 1.30345
step 4706, loss = 1.43379
step 4707, loss = 1.41759
step 4708, loss = 1.404
step 4709, loss = 1.31581
step 4710, loss = 1.49575
training 4711, accuracy = 0.122449
step 4711, loss = 1.3009
step 4712, loss = 1.29613
step 4713, loss = 1.82775
step 4714, loss = 1.29149
step 4715, loss = 1.37035
step 4716, loss = 1.47997
step 4717, loss = 1.45031
step 4718, loss = 1.68687
step 4719, loss = 1.42032
step 4720, loss = 1.27006
training 4721, accuracy = 0.729358
step 4721, loss = 1.37857
step 4722, loss = 1.2706
step 4723, loss = 1.27566
step 4724, loss = 1.44098
step 4725, loss = 1.40892
step 4726, loss = 2.79248
step 4727, loss = 1.7505
step 4728, loss = 1.63197
step 4729, loss = 1.35032
step 4730, loss = 1.6379
training 4731, accuracy = 0.280000
step 4731, loss = 1.42013
step 4732, loss = 1.37561
step 4733, loss = 1.25951
step 4734, loss = 1.26653
step 4735, loss = 1.36838
step 4736, loss = 1.41671
step 4737, loss = 1.33488
step 4738, loss = 1.3487
step 4739, loss = 1.30541
step 4740, loss = 1.27069
training 4741, accuracy = 0.636364
step 4741, loss = 1.39379
step 4742, loss = 1.38861
step 4743, loss = 1.40357
step 4744, loss = 1.38615
step 4745, loss = 1.41725
step 4746, loss = 1.32348
step 4747, loss = 1.24582
step 4748, loss = 1.27562
step 4749, loss = 1.34924
step 4750, loss = 1.41238
training 4751, accuracy = 0.285714
step 4751, loss = 2.01478
step 4752, loss = 1.59284
step 4753, loss = 1.39413
step 4754, loss = 1.63155
step 4755, loss = 1.35876
step 4756, loss = 1.25045
step 4757, loss = 1.51744
step 4758, loss = 1.34773
step 4759, loss = 1.47652
step 4760, loss = 1.33614
training 4761, accuracy = 0.097345
step 4761, loss = 1.42458
step 4762, loss = 1.36383
step 4763, loss = 1.33111
step 4764, loss = 1.43631
step 4765, loss = 1.26585
step 4766, loss = 1.30584
step 4767, loss = 1.31109
step 4768, loss = 1.24979
step 4769, loss = 1.36374
step 4770, loss = 1.45972
training 4771, accuracy = 0.196078
step 4771, loss = 1.24262
step 4772, loss = 1.64063
step 4773, loss = 1.85205
step 4774, loss = 1.48463
step 4775, loss = 1.34198
step 4776, loss = 1.26529
step 4777, loss = 1.4693
step 4778, loss = 1.56051
step 4779, loss = 1.20191
step 4780, loss = 1.37112
training 4781, accuracy = 0.344828
step 4781, loss = 1.25553
step 4782, loss = 1.3242
step 4783, loss = 1.32348
step 4784, loss = 1.3334
step 4785, loss = 1.31766
step 4786, loss = 1.2719
step 4787, loss = 1.20462
step 4788, loss = 1.50316
step 4789, loss = 1.34422
step 4790, loss = 1.26171
training 4791, accuracy = 0.084337
step 4791, loss = 1.33703
step 4792, loss = 1.34379
step 4793, loss = 1.25452
step 4794, loss = 1.35549
step 4795, loss = 1.39889
step 4796, loss = 1.32519
step 4797, loss = 2.07629
step 4798, loss = 1.64145
step 4799, loss = 1.5408
step 4800, loss = 1.38153
training 4801, accuracy = 0.559633
step 4801, loss = 1.41996
step 4802, loss = 1.39665
step 4803, loss = 1.28831
step 4804, loss = 1.33891
step 4805, loss = 1.4963
step 4806, loss = 1.66257
step 4807, loss = 1.2598
step 4808, loss = 2.19445
step 4809, loss = 1.7004
step 4810, loss = 1.75501
training 4811, accuracy = 0.088235
step 4811, loss = 1.50433
step 4812, loss = 1.51021
step 4813, loss = 1.20338
step 4814, loss = 1.32984
step 4815, loss = 1.38894
step 4816, loss = 1.3532
step 4817, loss = 1.37179
step 4818, loss = 1.38124
step 4819, loss = 1.78026
step 4820, loss = 1.29855
training 4821, accuracy = 0.303797
step 4821, loss = 1.135
step 4822, loss = 1.52643
step 4823, loss = 1.24447
step 4824, loss = 1.42444
step 4825, loss = 1.24972
step 4826, loss = 1.37192
step 4827, loss = 1.35999
step 4828, loss = 1.19936
step 4829, loss = 1.59529
step 4830, loss = 1.31923
training 4831, accuracy = 0.548780
step 4831, loss = 1.20524
step 4832, loss = 1.34171
step 4833, loss = 1.36419
step 4834, loss = 1.39386
step 4835, loss = 1.51908
step 4836, loss = 1.30999
step 4837, loss = 1.25451
step 4838, loss = 1.37328
step 4839, loss = 1.6076
step 4840, loss = 1.348
training 4841, accuracy = 0.163934
step 4841, loss = 1.29502
step 4842, loss = 1.27963
step 4843, loss = 1.27379
step 4844, loss = 1.21759
step 4845, loss = 1.21469
step 4846, loss = 1.2781
step 4847, loss = 1.45887
step 4848, loss = 1.48331
step 4849, loss = 1.28699
step 4850, loss = 1.69648
training 4851, accuracy = 0.142857
step 4851, loss = 1.33395
step 4852, loss = 1.2318
step 4853, loss = 1.23396
step 4854, loss = 1.43592
step 4855, loss = 1.20559
step 4856, loss = 1.23467
step 4857, loss = 1.20727
step 4858, loss = 1.53558
step 4859, loss = 1.50308
step 4860, loss = 1.28685
training 4861, accuracy = 0.230769
step 4861, loss = 1.44113
step 4862, loss = 1.35416
step 4863, loss = 1.20493
step 4864, loss = 1.23003
step 4865, loss = 1.41117
step 4866, loss = 1.63879
step 4867, loss = 1.5562
step 4868, loss = 1.39051
step 4869, loss = 1.17986
step 4870, loss = 1.47431
training 4871, accuracy = 0.070175
step 4871, loss = 1.62205
step 4872, loss = 1.19614
step 4873, loss = 1.29659
step 4874, loss = 1.25761
step 4875, loss = 1.28801
step 4876, loss = 1.32876
step 4877, loss = 1.24765
step 4878, loss = 1.35641
step 4879, loss = 1.37686
step 4880, loss = 1.22051
training 4881, accuracy = 0.141104
step 4881, loss = 1.30908
step 4882, loss = 1.40561
step 4883, loss = 1.26369
step 4884, loss = 1.3498
step 4885, loss = 1.27486
step 4886, loss = 2.04887
step 4887, loss = 1.29647
step 4888, loss = 1.20436
step 4889, loss = 1.24057
step 4890, loss = 1.30546
training 4891, accuracy = 0.298077
step 4891, loss = 1.33728
step 4892, loss = 1.24279
step 4893, loss = 1.9163
step 4894, loss = 1.3701
step 4895, loss = 1.34194
step 4896, loss = 1.22824
step 4897, loss = 1.39959
step 4898, loss = 1.23891
step 4899, loss = 1.22804
step 4900, loss = 1.21549
training 4901, accuracy = 0.123596
step 4901, loss = 1.42559
step 4902, loss = 1.52638
step 4903, loss = 1.22307
step 4904, loss = 1.39593
step 4905, loss = 1.28261
step 4906, loss = 1.18448
step 4907, loss = 1.20621
step 4908, loss = 1.27913
step 4909, loss = 1.24779
step 4910, loss = 1.36649
training 4911, accuracy = 0.000000
step 4911, loss = 1.25506
step 4912, loss = 2.68006
step 4913, loss = 2.16864
step 4914, loss = 1.57997
step 4915, loss = 1.57447
step 4916, loss = 1.30864
step 4917, loss = 1.19203
step 4918, loss = 1.48325
step 4919, loss = 1.23668
step 4920, loss = 1.44463
training 4921, accuracy = 0.930233
step 4921, loss = 1.30688
step 4922, loss = 1.22913
step 4923, loss = 1.22749
step 4924, loss = 1.19595
step 4925, loss = 1.19434
step 4926, loss = 1.32884
step 4927, loss = 1.32333
step 4928, loss = 1.41365
step 4929, loss = 1.20807
step 4930, loss = 1.36553
training 4931, accuracy = 0.087719
step 4931, loss = 1.23094
step 4932, loss = 1.23002
step 4933, loss = 1.2284
step 4934, loss = 1.18909
step 4935, loss = 1.23445
step 4936, loss = 1.219
step 4937, loss = 1.21179
step 4938, loss = 1.26066
step 4939, loss = 1.32148
step 4940, loss = 1.17908
training 4941, accuracy = 0.607143
step 4941, loss = 1.49451
step 4942, loss = 1.29859
step 4943, loss = 1.18105
step 4944, loss = 1.21357
step 4945, loss = 1.19863
step 4946, loss = 1.16351
step 4947, loss = 1.3042
step 4948, loss = 1.15606
step 4949, loss = 1.31077
step 4950, loss = 1.29161
training 4951, accuracy = 0.125000
step 4951, loss = 1.26797
step 4952, loss = 1.26611
step 4953, loss = 1.31426
step 4954, loss = 1.22113
step 4955, loss = 1.26021
step 4956, loss = 1.30248
step 4957, loss = 1.42808
step 4958, loss = 1.2433
step 4959, loss = 1.1816
step 4960, loss = 1.31756
training 4961, accuracy = 0.253968
step 4961, loss = 1.19165
step 4962, loss = 1.21425
step 4963, loss = 1.24144
step 4964, loss = 1.23015
step 4965, loss = 1.26338
step 4966, loss = 1.27754
step 4967, loss = 1.25503
step 4968, loss = 1.15133
step 4969, loss = 1.26136
step 4970, loss = 1.15436
training 4971, accuracy = 0.127778
step 4971, loss = 1.24302
step 4972, loss = 1.22438
step 4973, loss = 1.3051
step 4974, loss = 1.29635
step 4975, loss = 1.18981
step 4976, loss = 1.44079
step 4977, loss = 1.21407
step 4978, loss = 1.14084
step 4979, loss = 1.2582
step 4980, loss = 1.16405
training 4981, accuracy = 0.828571
step 4981, loss = 1.17737
step 4982, loss = 1.28813
step 4983, loss = 1.3298
step 4984, loss = 1.23828
step 4985, loss = 1.17596
step 4986, loss = 1.32512
step 4987, loss = 1.28468
step 4988, loss = 1.53245
step 4989, loss = 1.28496
step 4990, loss = 1.31365
training 4991, accuracy = 0.152778
step 4991, loss = 1.30813
step 4992, loss = 1.22357
step 4993, loss = 1.17022
step 4994, loss = 2.08985
step 4995, loss = 1.47176
step 4996, loss = 1.22335
step 4997, loss = 1.47372
step 4998, loss = 1.47133
step 4999, loss = 1.17457
step 5000, loss = 1.37523
training 5001, accuracy = 0.019608
step 5001, loss = 1.13121
step 5002, loss = 1.17284
step 5003, loss = 1.25697
step 5004, loss = 1.25005
step 5005, loss = 1.21073
step 5006, loss = 1.17318
step 5007, loss = 1.28444
step 5008, loss = 1.14562
step 5009, loss = 1.23602
step 5010, loss = 1.27008
training 5011, accuracy = 0.295455
step 5011, loss = 1.16475
step 5012, loss = 1.24814
step 5013, loss = 1.20093
step 5014, loss = 1.15162
step 5015, loss = 1.13526
step 5016, loss = 1.14333
step 5017, loss = 1.27617
step 5018, loss = 1.33999
step 5019, loss = 1.37682
step 5020, loss = 1.52718
training 5021, accuracy = 0.437500
step 5021, loss = 1.15921
step 5022, loss = 1.13107
step 5023, loss = 1.18868
step 5024, loss = 1.26953
step 5025, loss = 1.20162
step 5026, loss = 1.37476
step 5027, loss = 1.11825
step 5028, loss = 1.15744
step 5029, loss = 1.1916
step 5030, loss = 1.42088
training 5031, accuracy = 0.354839
step 5031, loss = 1.45484
step 5032, loss = 1.19233
step 5033, loss = 1.232
step 5034, loss = 1.22276
step 5035, loss = 1.22789
step 5036, loss = 1.1942
step 5037, loss = 1.32075
step 5038, loss = 1.10667
step 5039, loss = 1.16098
step 5040, loss = 1.31377
training 5041, accuracy = 0.187500
step 5041, loss = 1.16697
step 5042, loss = 1.28347
step 5043, loss = 1.17041
step 5044, loss = 1.19233
step 5045, loss = 1.18765
step 5046, loss = 1.36191
step 5047, loss = 1.56619
step 5048, loss = 1.25784
step 5049, loss = 1.20206
step 5050, loss = 1.13476
training 5051, accuracy = 0.113208
step 5051, loss = 1.19988
step 5052, loss = 1.14639
step 5053, loss = 1.11941
step 5054, loss = 1.17251
step 5055, loss = 1.2282
step 5056, loss = 1.31365
step 5057, loss = 1.15101
step 5058, loss = 1.14055
step 5059, loss = 1.31231
step 5060, loss = 1.26321
training 5061, accuracy = 0.716981
step 5061, loss = 1.13702
step 5062, loss = 1.19048
step 5063, loss = 1.19886
step 5064, loss = 1.59115
step 5065, loss = 1.28644
step 5066, loss = 1.15888
step 5067, loss = 1.32133
step 5068, loss = 1.26694
step 5069, loss = 1.16805
step 5070, loss = 1.12871
training 5071, accuracy = 0.010050
step 5071, loss = 1.28085
step 5072, loss = 1.22591
step 5073, loss = 1.18193
step 5074, loss = 1.15167
step 5075, loss = 1.19052
step 5076, loss = 1.30587
step 5077, loss = 1.14082
step 5078, loss = 1.13493
step 5079, loss = 1.17733
step 5080, loss = 1.18534
training 5081, accuracy = 0.069307
step 5081, loss = 1.23559
step 5082, loss = 1.15353
step 5083, loss = 1.15297
step 5084, loss = 1.15129
step 5085, loss = 1.10422
step 5086, loss = 1.12163
step 5087, loss = 1.10198
step 5088, loss = 1.30116
step 5089, loss = 1.1355
step 5090, loss = 1.16199
training 5091, accuracy = 0.313869
step 5091, loss = 1.17305
step 5092, loss = 1.3277
step 5093, loss = 1.18261
step 5094, loss = 1.88524
step 5095, loss = 1.24909
step 5096, loss = 1.19427
step 5097, loss = 1.37364
step 5098, loss = 1.48108
step 5099, loss = 1.14316
step 5100, loss = 1.74125
training 5101, accuracy = 0.531250
step 5101, loss = 1.13832
step 5102, loss = 1.18199
step 5103, loss = 1.19676
step 5104, loss = 1.30624
step 5105, loss = 1.10976
step 5106, loss = 1.12965
step 5107, loss = 1.28672
step 5108, loss = 1.18989
step 5109, loss = 1.10372
step 5110, loss = 1.13217
training 5111, accuracy = 0.173653
step 5111, loss = 1.24323
step 5112, loss = 1.11133
step 5113, loss = 1.23542
step 5114, loss = 1.22624
step 5115, loss = 1.11274
step 5116, loss = 1.15083
step 5117, loss = 1.16505
step 5118, loss = 1.01502
step 5119, loss = 1.09269
step 5120, loss = 1.37754
training 5121, accuracy = 0.500000
step 5121, loss = 1.14207
step 5122, loss = 1.19309
step 5123, loss = 2.05908
step 5124, loss = 1.21099
step 5125, loss = 1.59011
step 5126, loss = 1.4244
step 5127, loss = 1.24284
step 5128, loss = 1.16875
step 5129, loss = 1.0878
step 5130, loss = 1.17804
training 5131, accuracy = 0.311828
step 5131, loss = 1.14929
step 5132, loss = 1.25318
step 5133, loss = 1.36381
step 5134, loss = 1.40585
step 5135, loss = 1.12586
step 5136, loss = 1.12284
step 5137, loss = 1.26208
step 5138, loss = 1.14634
step 5139, loss = 1.12314
step 5140, loss = 1.15322
training 5141, accuracy = 0.468468
step 5141, loss = 1.11032
step 5142, loss = 1.21186
step 5143, loss = 1.2183
step 5144, loss = 1.21831
step 5145, loss = 1.07839
step 5146, loss = 1.29388
step 5147, loss = 1.12074
step 5148, loss = 1.11514
step 5149, loss = 1.22543
step 5150, loss = 1.18984
training 5151, accuracy = 0.250000
step 5151, loss = 1.19422
step 5152, loss = 1.33151
step 5153, loss = 1.34475
step 5154, loss = 1.16455
step 5155, loss = 1.12355
step 5156, loss = 1.19455
step 5157, loss = 1.11696
step 5158, loss = 1.15626
step 5159, loss = 1.05566
step 5160, loss = 1.25242
training 5161, accuracy = 0.457627
step 5161, loss = 1.07446
step 5162, loss = 1.10691
step 5163, loss = 1.11528
step 5164, loss = 1.21265
step 5165, loss = 1.2504
step 5166, loss = 1.09091
step 5167, loss = 1.16981
step 5168, loss = 1.22682
step 5169, loss = 1.10049
step 5170, loss = 1.07878
training 5171, accuracy = 0.412556
step 5171, loss = 1.06379
step 5172, loss = 1.11815
step 5173, loss = 1.17698
step 5174, loss = 1.24027
step 5175, loss = 1.10127
step 5176, loss = 1.07561
step 5177, loss = 1.34371
step 5178, loss = 1.26974
step 5179, loss = 1.16959
step 5180, loss = 1.19583
training 5181, accuracy = 0.271186
step 5181, loss = 1.34609
step 5182, loss = 1.06625
step 5183, loss = 1.13154
step 5184, loss = 1.25717
step 5185, loss = 1.13887
step 5186, loss = 1.09365
step 5187, loss = 1.08784
step 5188, loss = 1.20516
step 5189, loss = 1.17537
step 5190, loss = 1.15036
training 5191, accuracy = 0.640000
step 5191, loss = 1.05449
step 5192, loss = 1.1602
step 5193, loss = 1.05825
step 5194, loss = 1.12182
step 5195, loss = 1.27687
step 5196, loss = 1.1318
step 5197, loss = 1.16813
step 5198, loss = 1.19853
step 5199, loss = 1.12272
step 5200, loss = 1.21705
training 5201, accuracy = 0.396552
step 5201, loss = 1.14461
step 5202, loss = 1.14338
step 5203, loss = 1.23182
step 5204, loss = 1.11558
step 5205, loss = 1.12374
step 5206, loss = 1.0733
step 5207, loss = 1.15854
step 5208, loss = 1.22525
step 5209, loss = 1.05343
step 5210, loss = 1.0683
training 5211, accuracy = 0.744898
step 5211, loss = 1.16559
step 5212, loss = 1.2595
step 5213, loss = 1.68834
step 5214, loss = 1.39713
step 5215, loss = 1.15641
step 5216, loss = 1.13183
step 5217, loss = 1.07714
step 5218, loss = 1.30317
step 5219, loss = 1.15668
step 5220, loss = 1.05282
training 5221, accuracy = 0.746781
step 5221, loss = 1.11716
step 5222, loss = 1.12707
step 5223, loss = 1.16966
step 5224, loss = 1.14222
step 5225, loss = 1.06373
step 5226, loss = 1.13343
step 5227, loss = 1.0703
step 5228, loss = 1.03686
step 5229, loss = 2.12604
step 5230, loss = 1.25322
training 5231, accuracy = 0.578947
step 5231, loss = 1.45723
step 5232, loss = 1.06801
step 5233, loss = 1.17304
step 5234, loss = 1.13509
step 5235, loss = 1.22182
step 5236, loss = 1.13037
step 5237, loss = 1.13787
step 5238, loss = 1.07659
step 5239, loss = 1.1569
step 5240, loss = 1.03757
training 5241, accuracy = 0.703846
step 5241, loss = 1.16129
step 5242, loss = 1.68766
step 5243, loss = 1.12791
step 5244, loss = 1.20034
step 5245, loss = 1.37468
step 5246, loss = 1.36651
step 5247, loss = 1.26168
step 5248, loss = 1.26179
step 5249, loss = 1.17505
step 5250, loss = 1.33804
training 5251, accuracy = 0.476190
step 5251, loss = 1.32966
step 5252, loss = 1.53329
step 5253, loss = 1.15741
step 5254, loss = 1.22392
step 5255, loss = 1.2357
step 5256, loss = 1.26128
step 5257, loss = 1.14911
step 5258, loss = 1.115
step 5259, loss = 1.10631
step 5260, loss = 1.11203
training 5261, accuracy = 0.851064
step 5261, loss = 1.15646
step 5262, loss = 1.05776
step 5263, loss = 1.11504
step 5264, loss = 1.24301
step 5265, loss = 1.34428
step 5266, loss = 1.04254
step 5267, loss = 1.06177
step 5268, loss = 1.15167
step 5269, loss = 1.07447
step 5270, loss = 1.07491
training 5271, accuracy = 0.333333
step 5271, loss = 1.15103
step 5272, loss = 1.06465
step 5273, loss = 1.1708
step 5274, loss = 1.13008
step 5275, loss = 1.05101
step 5276, loss = 1.07541
step 5277, loss = 1.18478
step 5278, loss = 1.09705
step 5279, loss = 1.06779
step 5280, loss = 1.02817
training 5281, accuracy = 0.491736
step 5281, loss = 1.0672
step 5282, loss = 1.0903
step 5283, loss = 0.995584
step 5284, loss = 1.16226
step 5285, loss = 1.2318
step 5286, loss = 1.10448
step 5287, loss = 1.19944
step 5288, loss = 1.03958
step 5289, loss = 2.05706
step 5290, loss = 1.08279
training 5291, accuracy = 0.237226
step 5291, loss = 1.47802
step 5292, loss = 1.81114
step 5293, loss = 1.24106
step 5294, loss = 1.15061
step 5295, loss = 1.03352
step 5296, loss = 1.10303
step 5297, loss = 1.18018
step 5298, loss = 1.01844
step 5299, loss = 1.09219
step 5300, loss = 1.15411
training 5301, accuracy = 0.169231
step 5301, loss = 1.09854
step 5302, loss = 1.03833
step 5303, loss = 1.10637
step 5304, loss = 1.09645
step 5305, loss = 1.40043
step 5306, loss = 1.6404
step 5307, loss = 1.09505
step 5308, loss = 1.04662
step 5309, loss = 1.06379
step 5310, loss = 1.11882
training 5311, accuracy = 0.521739
step 5311, loss = 1.21841
step 5312, loss = 1.20405
step 5313, loss = 1.16637
step 5314, loss = 1.07101
step 5315, loss = 1.01982
step 5316, loss = 1.10991
step 5317, loss = 1.12843
step 5318, loss = 1.15917
step 5319, loss = 1.0271
step 5320, loss = 1.01358
training 5321, accuracy = 0.665339
step 5321, loss = 1.62369
step 5322, loss = 1.51474
step 5323, loss = 1.16191
step 5324, loss = 1.25909
step 5325, loss = 1.28497
step 5326, loss = 1.00201
step 5327, loss = 1.10417
step 5328, loss = 1.01929
step 5329, loss = 1.1063
step 5330, loss = 1.08846
training 5331, accuracy = 0.200000
step 5331, loss = 1.01913
step 5332, loss = 1.101
step 5333, loss = 1.08136
step 5334, loss = 1.37673
step 5335, loss = 1.27319
step 5336, loss = 1.31279
step 5337, loss = 1.07527
step 5338, loss = 1.304
step 5339, loss = 0.995868
step 5340, loss = 1.16137
training 5341, accuracy = 0.125000
step 5341, loss = 1.30494
step 5342, loss = 1.18106
step 5343, loss = 1.06185
step 5344, loss = 1.09264
step 5345, loss = 1.18169
step 5346, loss = 1.09931
step 5347, loss = 1.15738
step 5348, loss = 1.09687
step 5349, loss = 1.0818
step 5350, loss = 1.15403
training 5351, accuracy = 0.437500
step 5351, loss = 1.02784
step 5352, loss = 1.10941
step 5353, loss = 0.989009
step 5354, loss = 1.14712
step 5355, loss = 1.09729
step 5356, loss = 1.05997
step 5357, loss = 1.02367
step 5358, loss = 1.06264
step 5359, loss = 2.03776
step 5360, loss = 1.20933
training 5361, accuracy = 0.486239
step 5361, loss = 1.05734
step 5362, loss = 1.16344
step 5363, loss = 1.03769
step 5364, loss = 1.03544
step 5365, loss = 1.02106
step 5366, loss = 1.12679
step 5367, loss = 1.14507
step 5368, loss = 1.02699
step 5369, loss = 1.12054
step 5370, loss = 1.31922
training 5371, accuracy = 0.357143
step 5371, loss = 1.04556
step 5372, loss = 1.04476
step 5373, loss = 1.0927
step 5374, loss = 1.00671
step 5375, loss = 1.09417
step 5376, loss = 1.01536
step 5377, loss = 1.36204
step 5378, loss = 1.0854
step 5379, loss = 1.16196
step 5380, loss = 1.18752
training 5381, accuracy = 0.029412
step 5381, loss = 1.14009
step 5382, loss = 1.16992
step 5383, loss = 1.19889
step 5384, loss = 1.05012
step 5385, loss = 0.995678
step 5386, loss = 0.981288
step 5387, loss = 1.04201
step 5388, loss = 1.05557
step 5389, loss = 1.14784
step 5390, loss = 1.15034
training 5391, accuracy = 0.102041
step 5391, loss = 1.06499
step 5392, loss = 1.02487
step 5393, loss = 1.07565
step 5394, loss = 1.00899
step 5395, loss = 1.09264
step 5396, loss = 1.03188
step 5397, loss = 0.989026
step 5398, loss = 1.13239
step 5399, loss = 1.14926
step 5400, loss = 1.39687
training 5401, accuracy = 0.333333
step 5401, loss = 1.10799
step 5402, loss = 1.38923
step 5403, loss = 1.21951
step 5404, loss = 1.18684
step 5405, loss = 1.2289
step 5406, loss = 1.04582
step 5407, loss = 1.08086
step 5408, loss = 1.15519
step 5409, loss = 1.11969
step 5410, loss = 1.21965
training 5411, accuracy = 0.000000
step 5411, loss = 1.20403
step 5412, loss = 1.10027
step 5413, loss = 1.04941
step 5414, loss = 1.09905
step 5415, loss = 1.05647
step 5416, loss = 1.70081
step 5417, loss = 1.28906
step 5418, loss = 1.27234
step 5419, loss = 1.19959
step 5420, loss = 1.15389
training 5421, accuracy = 0.144144
step 5421, loss = 1.14983
step 5422, loss = 1.04582
step 5423, loss = 1.06585
step 5424, loss = 1.03625
step 5425, loss = 1.08789
step 5426, loss = 1.27726
step 5427, loss = 1.01987
step 5428, loss = 0.97779
step 5429, loss = 1.10566
step 5430, loss = 1.05505
training 5431, accuracy = 0.126437
step 5431, loss = 1.0829
step 5432, loss = 1.00724
step 5433, loss = 0.983819
step 5434, loss = 1.15074
step 5435, loss = 1.02845
step 5436, loss = 0.97791
step 5437, loss = 1.00802
step 5438, loss = 1.45843
step 5439, loss = 0.997963
step 5440, loss = 1.12693
training 5441, accuracy = 0.466667
step 5441, loss = 1.02937
step 5442, loss = 1.17772
step 5443, loss = 0.958617
step 5444, loss = 1.10977
step 5445, loss = 1.0884
step 5446, loss = 1.02597
step 5447, loss = 1.08873
step 5448, loss = 1.08295
step 5449, loss = 1.03681
step 5450, loss = 1.13333
training 5451, accuracy = 0.425532
step 5451, loss = 0.976869
step 5452, loss = 1.14171
step 5453, loss = 1.11437
step 5454, loss = 1.03089
step 5455, loss = 1.38083
step 5456, loss = 1.47722
step 5457, loss = 1.1432
step 5458, loss = 1.01851
step 5459, loss = 1.01545
step 5460, loss = 1.12449
training 5461, accuracy = 0.243590
step 5461, loss = 1.04915
step 5462, loss = 1.06323
step 5463, loss = 0.997424
step 5464, loss = 1.14365
step 5465, loss = 1.05403
step 5466, loss = 1.06846
step 5467, loss = 0.981584
step 5468, loss = 0.961998
step 5469, loss = 1.03435
step 5470, loss = 0.999973
training 5471, accuracy = 0.844311
step 5471, loss = 1.11181
step 5472, loss = 0.966916
step 5473, loss = 1.04158
step 5474, loss = 1.07998
step 5475, loss = 1.50227
step 5476, loss = 2.18901
step 5477, loss = 1.21504
step 5478, loss = 1.00536
step 5479, loss = 1.33059
step 5480, loss = 1.22235
training 5481, accuracy = 0.109589
step 5481, loss = 1.2202
step 5482, loss = 1.04456
step 5483, loss = 0.979771
step 5484, loss = 1.10817
step 5485, loss = 1.03494
step 5486, loss = 1.10773
step 5487, loss = 0.984624
step 5488, loss = 1.01085
step 5489, loss = 0.957846
step 5490, loss = 0.980393
training 5491, accuracy = 0.502326
step 5491, loss = 1.09615
step 5492, loss = 0.97739
step 5493, loss = 1.01615
step 5494, loss = 0.991817
step 5495, loss = 1.06718
step 5496, loss = 1.01886
step 5497, loss = 1.01221
step 5498, loss = 1.08599
step 5499, loss = 1.36392
step 5500, loss = 0.972282
training 5501, accuracy = 0.834783
step 5501, loss = 1.02536
step 5502, loss = 1.2177
step 5503, loss = 1.03051
step 5504, loss = 1.25889
step 5505, loss = 0.959095
step 5506, loss = 1.05623
step 5507, loss = 1.39023
step 5508, loss = 1.15298
step 5509, loss = 1.18771
step 5510, loss = 1.19847
training 5511, accuracy = 0.750000
step 5511, loss = 1.11619
step 5512, loss = 0.99113
step 5513, loss = 1.14495
step 5514, loss = 0.979698
step 5515, loss = 1.08547
step 5516, loss = 1.04051
step 5517, loss = 1.18422
step 5518, loss = 0.962023
step 5519, loss = 1.04325
step 5520, loss = 1.04062
training 5521, accuracy = 0.693878
step 5521, loss = 1.35529
step 5522, loss = 0.980341
step 5523, loss = 1.03342
step 5524, loss = 1.1297
step 5525, loss = 1.03074
step 5526, loss = 1.05857
step 5527, loss = 1.04774
step 5528, loss = 1.02399
step 5529, loss = 0.999443
step 5530, loss = 0.988807
training 5531, accuracy = 0.289256
step 5531, loss = 1.07706
step 5532, loss = 0.981725
step 5533, loss = 1.03081
step 5534, loss = 0.968136
step 5535, loss = 1.07728
step 5536, loss = 1.045
step 5537, loss = 1.86341
step 5538, loss = 1.46405
step 5539, loss = 1.32052
step 5540, loss = 1.11781
training 5541, accuracy = 0.740741
step 5541, loss = 1.18515
step 5542, loss = 1.08526
step 5543, loss = 0.999378
step 5544, loss = 0.993754
step 5545, loss = 1.19404
step 5546, loss = 0.951291
step 5547, loss = 0.989345
step 5548, loss = 0.94967
step 5549, loss = 1.16752
step 5550, loss = 0.972862
training 5551, accuracy = 0.000000
step 5551, loss = 0.96092
step 5552, loss = 1.01131
step 5553, loss = 0.960589
step 5554, loss = 0.978083
step 5555, loss = 0.994868
step 5556, loss = 1.09963
step 5557, loss = 0.948949
step 5558, loss = 1.17719
step 5559, loss = 0.979562
step 5560, loss = 0.974499
training 5561, accuracy = 0.727811
step 5561, loss = 0.965925
step 5562, loss = 1.03863
step 5563, loss = 1.23406
step 5564, loss = 1.00322
step 5565, loss = 0.959255
step 5566, loss = 1.06953
step 5567, loss = 0.955946
step 5568, loss = 0.96048
step 5569, loss = 0.996899
step 5570, loss = 0.942368
training 5571, accuracy = 0.236842
step 5571, loss = 1.21453
step 5572, loss = 0.952355
step 5573, loss = 0.930058
step 5574, loss = 0.968782
step 5575, loss = 0.977917
step 5576, loss = 0.996585
step 5577, loss = 0.921769
step 5578, loss = 0.928238
step 5579, loss = 0.93111
step 5580, loss = 1.02885
training 5581, accuracy = 0.241379
step 5581, loss = 0.989157
step 5582, loss = 3.63344
step 5583, loss = 1.37312
step 5584, loss = 1.14472
step 5585, loss = 1.09241
step 5586, loss = 1.27051
step 5587, loss = 1.05671
step 5588, loss = 0.960593
step 5589, loss = 1.10232
step 5590, loss = 1.07122
training 5591, accuracy = 0.474453
step 5591, loss = 0.965396
step 5592, loss = 1.22936
step 5593, loss = 0.952675
step 5594, loss = 0.948198
step 5595, loss = 1.00702
step 5596, loss = 0.936005
step 5597, loss = 0.95901
step 5598, loss = 1.01659
step 5599, loss = 1.04487
step 5600, loss = 1.05811
training 5601, accuracy = 0.077778
step 5601, loss = 1.09698
step 5602, loss = 1.01388
step 5603, loss = 1.13497
step 5604, loss = 1.00907
step 5605, loss = 0.957933
step 5606, loss = 0.990227
step 5607, loss = 1.09623
step 5608, loss = 0.949333
step 5609, loss = 1.0079
step 5610, loss = 1.09407
training 5611, accuracy = 0.360000
step 5611, loss = 1.09805
step 5612, loss = 0.963349
step 5613, loss = 1.09068
step 5614, loss = 1.08606
step 5615, loss = 0.928667
step 5616, loss = 0.948893
step 5617, loss = 0.943486
step 5618, loss = 1.04668
step 5619, loss = 0.984976
step 5620, loss = 1.09009
training 5621, accuracy = 0.215686
step 5621, loss = 1.02082
step 5622, loss = 1.32765
step 5623, loss = 1.59154
step 5624, loss = 1.26733
step 5625, loss = 1.01451
step 5626, loss = 1.35208
step 5627, loss = 0.935667
step 5628, loss = 0.993811
step 5629, loss = 0.966289
step 5630, loss = 1.5229
training 5631, accuracy = 0.461538
step 5631, loss = 0.938677
step 5632, loss = 0.952194
step 5633, loss = 0.990553
step 5634, loss = 1.05359
step 5635, loss = 0.964715
step 5636, loss = 0.930073
step 5637, loss = 0.964179
step 5638, loss = 0.947114
step 5639, loss = 1.0233
step 5640, loss = 1.03493
training 5641, accuracy = 0.274510
step 5641, loss = 0.936773
step 5642, loss = 1.02238
step 5643, loss = 1.06978
step 5644, loss = 0.992334
step 5645, loss = 0.951809
step 5646, loss = 1.0911
step 5647, loss = 1.0184
step 5648, loss = 0.930623
step 5649, loss = 0.987949
step 5650, loss = 0.962138
training 5651, accuracy = 0.063063
step 5651, loss = 1.24787
step 5652, loss = 1.39833
step 5653, loss = 0.976787
step 5654, loss = 1.04456
step 5655, loss = 1.11466
step 5656, loss = 0.938147
step 5657, loss = 0.983414
step 5658, loss = 0.960835
step 5659, loss = 0.985315
step 5660, loss = 0.964679
training 5661, accuracy = 0.603774
step 5661, loss = 0.965597
step 5662, loss = 1.7679
step 5663, loss = 0.996035
step 5664, loss = 0.946093
step 5665, loss = 1.19862
step 5666, loss = 0.9134
step 5667, loss = 0.954305
step 5668, loss = 0.955799
step 5669, loss = 1.21692
step 5670, loss = 0.977789
training 5671, accuracy = 0.294643
step 5671, loss = 1.55284
step 5672, loss = 1.0835
step 5673, loss = 1.15288
step 5674, loss = 1.08695
step 5675, loss = 0.987308
step 5676, loss = 1.02295
step 5677, loss = 1.13818
step 5678, loss = 0.938984
step 5679, loss = 1.05905
step 5680, loss = 0.992864
training 5681, accuracy = 0.921875
step 5681, loss = 0.928543
step 5682, loss = 0.877446
step 5683, loss = 1.33119
step 5684, loss = 1.11205
step 5685, loss = 0.974585
step 5686, loss = 1.02545
step 5687, loss = 1.04333
step 5688, loss = 0.929901
step 5689, loss = 0.981378
step 5690, loss = 1.25193
training 5691, accuracy = 0.272727
step 5691, loss = 1.09779
step 5692, loss = 1.06575
step 5693, loss = 0.942993
step 5694, loss = 0.979975
step 5695, loss = 0.997977
step 5696, loss = 0.906265
step 5697, loss = 0.963432
step 5698, loss = 0.916932
step 5699, loss = 1.08613
step 5700, loss = 0.976403
training 5701, accuracy = 0.682927
step 5701, loss = 0.959155
step 5702, loss = 1.00674
step 5703, loss = 1.05565
step 5704, loss = 0.896329
step 5705, loss = 1.126
step 5706, loss = 0.938044
step 5707, loss = 1.03057
step 5708, loss = 1.0352
step 5709, loss = 1.04262
step 5710, loss = 1.00918
training 5711, accuracy = 0.717949
step 5711, loss = 0.946654
step 5712, loss = 1.08259
step 5713, loss = 1.00984
step 5714, loss = 0.941876
step 5715, loss = 0.908981
step 5716, loss = 0.9678
step 5717, loss = 0.912414
step 5718, loss = 0.978618
step 5719, loss = 0.964924
step 5720, loss = 0.894811
training 5721, accuracy = 0.381166
step 5721, loss = 1.00624
step 5722, loss = 1.40752
step 5723, loss = 0.966407
step 5724, loss = 0.926482
step 5725, loss = 0.938482
step 5726, loss = 1.00727
step 5727, loss = 1.01636
step 5728, loss = 0.934441
step 5729, loss = 0.986544
step 5730, loss = 0.891763
training 5731, accuracy = 0.269231
step 5731, loss = 0.916637
step 5732, loss = 1.09352
step 5733, loss = 1.17297
step 5734, loss = 0.910751
step 5735, loss = 0.897149
step 5736, loss = 0.952559
step 5737, loss = 0.900234
step 5738, loss = 0.958901
step 5739, loss = 0.944553
step 5740, loss = 0.906573
training 5741, accuracy = 0.230366
step 5741, loss = 0.90196
step 5742, loss = 1.66613
step 5743, loss = 1.22643
step 5744, loss = 0.907589
step 5745, loss = 1.15603
step 5746, loss = 1.03791
step 5747, loss = 1.22345
step 5748, loss = 0.952451
step 5749, loss = 1.23837
step 5750, loss = 0.923723
training 5751, accuracy = 0.811594
step 5751, loss = 0.984552
step 5752, loss = 0.91783
step 5753, loss = 0.91539
step 5754, loss = 1.00565
step 5755, loss = 1.10002
step 5756, loss = 0.911504
step 5757, loss = 0.987854
step 5758, loss = 0.979561
step 5759, loss = 0.99538
step 5760, loss = 1.18706
training 5761, accuracy = 0.166667
step 5761, loss = 0.910814
step 5762, loss = 0.90707
step 5763, loss = 0.972056
step 5764, loss = 0.882212
step 5765, loss = 0.944496
step 5766, loss = 1.03542
step 5767, loss = 0.896474
step 5768, loss = 0.936625
step 5769, loss = 1.23189
step 5770, loss = 0.925422
training 5771, accuracy = 0.598131
step 5771, loss = 1.00318
step 5772, loss = 0.916116
step 5773, loss = 1.14603
step 5774, loss = 1.05781
step 5775, loss = 0.881417
step 5776, loss = 0.942357
step 5777, loss = 0.879186
step 5778, loss = 1.06044
step 5779, loss = 1.06558
step 5780, loss = 0.983729
training 5781, accuracy = 0.230769
step 5781, loss = 0.913521
step 5782, loss = 0.918028
step 5783, loss = 1.00552
step 5784, loss = 1.15045
step 5785, loss = 1.1047
step 5786, loss = 1.21407
step 5787, loss = 1.03606
step 5788, loss = 0.91884
step 5789, loss = 1.13809
step 5790, loss = 1.25544
training 5791, accuracy = 0.518519
step 5791, loss = 0.910786
step 5792, loss = 0.923911
step 5793, loss = 1.08861
step 5794, loss = 0.918143
step 5795, loss = 0.892168
step 5796, loss = 0.869557
step 5797, loss = 0.892595
step 5798, loss = 1.15689
step 5799, loss = 0.877832
step 5800, loss = 0.931249
training 5801, accuracy = 0.327103
step 5801, loss = 1.10787
step 5802, loss = 0.88595
step 5803, loss = 0.989617
step 5804, loss = 0.985735
step 5805, loss = 0.858551
step 5806, loss = 0.991948
step 5807, loss = 0.920883
step 5808, loss = 0.918797
step 5809, loss = 0.921438
step 5810, loss = 0.90657
training 5811, accuracy = 0.339286
step 5811, loss = 1.25902
step 5812, loss = 1.03028
step 5813, loss = 1.10093
step 5814, loss = 0.867761
step 5815, loss = 1.19079
step 5816, loss = 0.870393
step 5817, loss = 0.934071
step 5818, loss = 1.15066
step 5819, loss = 0.87751
step 5820, loss = 0.906911
training 5821, accuracy = 0.218978
step 5821, loss = 1.15979
step 5822, loss = 0.982128
step 5823, loss = 0.924972
step 5824, loss = 0.887543
step 5825, loss = 0.906554
step 5826, loss = 0.971884
step 5827, loss = 0.915685
step 5828, loss = 0.940443
step 5829, loss = 0.923482
step 5830, loss = 2.16424
training 5831, accuracy = 0.294118
step 5831, loss = 1.48286
step 5832, loss = 1.58578
step 5833, loss = 1.16158
step 5834, loss = 0.89578
step 5835, loss = 0.900502
step 5836, loss = 1.09415
step 5837, loss = 1.00998
step 5838, loss = 2.86789
step 5839, loss = 0.986379
step 5840, loss = 1.27738
training 5841, accuracy = 0.404255
step 5841, loss = 1.05912
step 5842, loss = 0.989188
step 5843, loss = 0.953843
step 5844, loss = 1.2126
step 5845, loss = 0.840748
step 5846, loss = 0.931491
step 5847, loss = 1.04368
step 5848, loss = 0.927584
step 5849, loss = 0.915285
step 5850, loss = 0.976251
training 5851, accuracy = 0.294118
step 5851, loss = 1.63497
step 5852, loss = 0.884726
step 5853, loss = 1.00358
step 5854, loss = 1.06873
step 5855, loss = 1.10089
step 5856, loss = 0.905231
step 5857, loss = 0.844029
step 5858, loss = 1.07996
step 5859, loss = 1.02605
step 5860, loss = 0.879517
training 5861, accuracy = 0.664975
step 5861, loss = 1.03771
step 5862, loss = 1.01094
step 5863, loss = 0.850699
step 5864, loss = 0.859295
step 5865, loss = 1.78315
step 5866, loss = 1.04794
step 5867, loss = 1.17398
step 5868, loss = 1.0439
step 5869, loss = 0.888203
step 5870, loss = 0.995691
training 5871, accuracy = 0.310345
step 5871, loss = 1.0221
step 5872, loss = 1.01627
step 5873, loss = 1.06114
step 5874, loss = 0.90349
step 5875, loss = 0.861812
step 5876, loss = 0.897342
step 5877, loss = 0.910078
step 5878, loss = 0.921607
step 5879, loss = 0.880735
step 5880, loss = 1.05297
training 5881, accuracy = 0.694444
step 5881, loss = 0.876548
step 5882, loss = 0.943089
step 5883, loss = 0.947124
step 5884, loss = 0.899743
step 5885, loss = 0.906963
step 5886, loss = 0.873253
step 5887, loss = 0.935139
step 5888, loss = 0.832143
step 5889, loss = 0.917592
step 5890, loss = 0.873565
training 5891, accuracy = 0.192053
step 5891, loss = 0.869814
step 5892, loss = 0.911919
step 5893, loss = 0.913364
step 5894, loss = 0.861443
step 5895, loss = 0.872519
step 5896, loss = 0.964924
step 5897, loss = 0.873357
step 5898, loss = 1.23144
step 5899, loss = 0.925372
step 5900, loss = 0.942721
training 5901, accuracy = 0.362637
step 5901, loss = 0.8429
step 5902, loss = 0.920926
step 5903, loss = 0.914419
step 5904, loss = 0.930337
step 5905, loss = 0.852449
step 5906, loss = 1.2686
step 5907, loss = 0.986837
step 5908, loss = 0.995828
step 5909, loss = 0.987146
step 5910, loss = 0.827692
training 5911, accuracy = 0.012158
step 5911, loss = 0.938642
step 5912, loss = 0.96594
step 5913, loss = 1.4326
step 5914, loss = 1.01359
step 5915, loss = 1.04513
step 5916, loss = 0.969605
step 5917, loss = 1.07234
step 5918, loss = 0.955007
step 5919, loss = 0.993421
step 5920, loss = 0.936536
training 5921, accuracy = 0.175258
step 5921, loss = 1.01944
step 5922, loss = 0.958457
step 5923, loss = 0.843481
step 5924, loss = 1.06666
step 5925, loss = 1.24888
step 5926, loss = 0.866027
step 5927, loss = 1.07423
step 5928, loss = 1.13701
step 5929, loss = 0.967556
step 5930, loss = 0.995522
training 5931, accuracy = 0.206186
step 5931, loss = 0.89141
step 5932, loss = 0.905134
step 5933, loss = 0.84058
step 5934, loss = 0.890981
step 5935, loss = 1.01916
step 5936, loss = 0.897696
step 5937, loss = 0.830963
step 5938, loss = 1.3163
step 5939, loss = 0.997604
step 5940, loss = 0.854399
training 5941, accuracy = 0.177340
step 5941, loss = 0.854832
step 5942, loss = 0.863158
step 5943, loss = 0.950877
step 5944, loss = 0.847144
step 5945, loss = 0.838429
step 5946, loss = 0.956163
step 5947, loss = 0.922483
step 5948, loss = 1.28556
step 5949, loss = 0.902017
step 5950, loss = 1.32406
training 5951, accuracy = 0.333333
step 5951, loss = 1.20134
step 5952, loss = 0.904084
step 5953, loss = 0.872328
step 5954, loss = 0.983364
step 5955, loss = 0.85569
step 5956, loss = 0.892886
step 5957, loss = 0.883722
step 5958, loss = 1.57815
step 5959, loss = 0.935433
step 5960, loss = 0.827971
training 5961, accuracy = 0.006289
step 5961, loss = 1.0099
step 5962, loss = 0.897899
step 5963, loss = 0.850734
step 5964, loss = 0.850955
step 5965, loss = 1.00363
step 5966, loss = 0.831864
step 5967, loss = 0.824768
step 5968, loss = 0.860243
step 5969, loss = 1.00366
step 5970, loss = 0.996585
training 5971, accuracy = 0.754098
step 5971, loss = 0.921822
step 5972, loss = 3.13215
step 5973, loss = 0.978071
step 5974, loss = 1.25775
step 5975, loss = 1.10663
step 5976, loss = 1.16422
step 5977, loss = 1.01737
step 5978, loss = 0.964987
step 5979, loss = 0.912246
step 5980, loss = 0.849303
training 5981, accuracy = 0.834356
step 5981, loss = 0.90597
step 5982, loss = 1.04787
step 5983, loss = 0.969602
step 5984, loss = 1.10181
step 5985, loss = 0.924466
step 5986, loss = 0.871025
step 5987, loss = 0.996146
step 5988, loss = 0.867544
step 5989, loss = 0.845002
step 5990, loss = 0.862472
training 5991, accuracy = 0.803371
step 5991, loss = 0.982865
step 5992, loss = 0.953075
step 5993, loss = 0.851621
step 5994, loss = 0.911323
step 5995, loss = 0.914049
step 5996, loss = 1.23363
step 5997, loss = 0.826809
step 5998, loss = 0.917468
step 5999, loss = 0.960627
step 6000, loss = 0.884858
training 6001, accuracy = 0.435294
step 6001, loss = 1.22525
step 6002, loss = 1.15182
step 6003, loss = 1.05454
step 6004, loss = 0.992001
step 6005, loss = 0.912975
step 6006, loss = 0.891482
step 6007, loss = 0.933034
step 6008, loss = 0.962228
step 6009, loss = 1.07662
step 6010, loss = 0.802954
training 6011, accuracy = 0.116981
step 6011, loss = 0.906788
step 6012, loss = 1.51416
step 6013, loss = 0.872875
step 6014, loss = 0.852278
step 6015, loss = 0.908589
step 6016, loss = 1.29949
step 6017, loss = 0.843772
step 6018, loss = 0.904883
step 6019, loss = 0.915098
step 6020, loss = 0.874469
training 6021, accuracy = 0.369231
step 6021, loss = 1.01374
step 6022, loss = 0.821701
step 6023, loss = 0.947242
step 6024, loss = 0.958222
step 6025, loss = 0.804446
step 6026, loss = 0.947507
step 6027, loss = 1.9318
step 6028, loss = 1.57229
step 6029, loss = 1.27675
step 6030, loss = 0.842739
training 6031, accuracy = 0.286624
step 6031, loss = 0.899036
step 6032, loss = 0.830959
step 6033, loss = 0.852371
step 6034, loss = 0.852801
step 6035, loss = 0.873483
step 6036, loss = 0.826615
step 6037, loss = 0.973428
step 6038, loss = 0.988366
step 6039, loss = 0.796684
step 6040, loss = 0.911998
training 6041, accuracy = 0.330189
step 6041, loss = 0.827409
step 6042, loss = 0.845922
step 6043, loss = 1.09166
step 6044, loss = 0.859642
step 6045, loss = 1.19886
step 6046, loss = 0.888622
step 6047, loss = 0.788079
step 6048, loss = 0.791669
step 6049, loss = 0.82744
step 6050, loss = 0.832373
training 6051, accuracy = 0.307143
step 6051, loss = 0.869962
step 6052, loss = 0.872657
step 6053, loss = 0.899961
step 6054, loss = 0.919486
step 6055, loss = 0.827293
step 6056, loss = 1.20359
step 6057, loss = 1.09898
step 6058, loss = 0.817964
step 6059, loss = 1.76262
step 6060, loss = 0.8981
training 6061, accuracy = 0.840000
step 6061, loss = 0.929503
step 6062, loss = 0.847351
step 6063, loss = 0.914648
step 6064, loss = 0.827708
step 6065, loss = 0.948661
step 6066, loss = 1.1543
step 6067, loss = 0.801329
step 6068, loss = 0.996354
step 6069, loss = 0.948119
step 6070, loss = 0.8297
training 6071, accuracy = 0.356725
step 6071, loss = 0.829784
step 6072, loss = 0.910336
step 6073, loss = 0.871739
step 6074, loss = 0.952114
step 6075, loss = 0.835039
step 6076, loss = 0.878113
step 6077, loss = 0.852737
step 6078, loss = 0.883187
step 6079, loss = 0.830055
step 6080, loss = 0.799482
training 6081, accuracy = 0.625000
step 6081, loss = 0.798412
step 6082, loss = 0.870286
step 6083, loss = 0.923343
step 6084, loss = 1.59881
step 6085, loss = 0.929487
step 6086, loss = 0.920358
step 6087, loss = 0.826591
step 6088, loss = 0.822101
step 6089, loss = 0.806767
step 6090, loss = 0.802715
training 6091, accuracy = 0.784000
step 6091, loss = 0.838097
step 6092, loss = 1.03027
step 6093, loss = 0.932499
step 6094, loss = 0.93599
step 6095, loss = 0.834631
step 6096, loss = 0.855417
step 6097, loss = 0.865325
step 6098, loss = 0.827572
step 6099, loss = 0.867703
step 6100, loss = 0.820676
training 6101, accuracy = 0.487179
step 6101, loss = 0.863977
step 6102, loss = 0.882242
step 6103, loss = 0.826873
step 6104, loss = 0.89587
step 6105, loss = 0.866108
step 6106, loss = 0.856074
step 6107, loss = 0.847368
step 6108, loss = 1.74413
step 6109, loss = 2.21702
step 6110, loss = 1.39749
training 6111, accuracy = 0.530612
step 6111, loss = 0.8329
step 6112, loss = 0.828696
step 6113, loss = 0.893162
step 6114, loss = 1.0117
step 6115, loss = 1.04404
step 6116, loss = 0.876248
step 6117, loss = 0.938159
step 6118, loss = 0.854961
step 6119, loss = 0.997401
step 6120, loss = 0.806502
training 6121, accuracy = 0.821918
step 6121, loss = 0.835221
step 6122, loss = 0.921411
step 6123, loss = 0.883421
step 6124, loss = 0.9479
step 6125, loss = 0.814932
step 6126, loss = 0.929649
step 6127, loss = 0.824461
step 6128, loss = 0.80899
step 6129, loss = 0.945398
step 6130, loss = 0.860733
training 6131, accuracy = 0.657658
step 6131, loss = 0.921632
step 6132, loss = 0.901311
step 6133, loss = 0.850138
step 6134, loss = 0.876609
step 6135, loss = 0.835758
step 6136, loss = 0.808007
step 6137, loss = 0.801274
step 6138, loss = 0.8078
step 6139, loss = 0.84256
step 6140, loss = 0.773795
training 6141, accuracy = 0.135678
step 6141, loss = 0.874037
step 6142, loss = 0.84134
step 6143, loss = 0.804774
step 6144, loss = 0.863768
step 6145, loss = 0.701379
step 6146, loss = 0.878069
step 6147, loss = 0.769128
step 6148, loss = 0.840265
step 6149, loss = 0.834524
step 6150, loss = 0.855196
training 6151, accuracy = 0.253165
step 6151, loss = 0.86183
step 6152, loss = 0.817913
step 6153, loss = 0.795971
step 6154, loss = 0.88151
step 6155, loss = 0.867678
step 6156, loss = 0.861031
step 6157, loss = 0.942234
step 6158, loss = 0.954121
step 6159, loss = 0.868608
step 6160, loss = 0.904217
training 6161, accuracy = 0.288462
step 6161, loss = 0.829859
step 6162, loss = 0.805251
step 6163, loss = 0.838224
step 6164, loss = 0.952248
step 6165, loss = 0.864414
step 6166, loss = 0.901013
step 6167, loss = 0.895807
step 6168, loss = 1.16191
step 6169, loss = 1.07256
step 6170, loss = 1.49449
training 6171, accuracy = 0.684211
step 6171, loss = 0.728789
step 6172, loss = 1.10992
step 6173, loss = 0.894005
step 6174, loss = 0.911285
step 6175, loss = 0.863218
step 6176, loss = 0.899289
step 6177, loss = 0.833912
step 6178, loss = 0.871663
step 6179, loss = 0.829509
step 6180, loss = 0.807513
training 6181, accuracy = 0.422857
step 6181, loss = 0.834475
step 6182, loss = 0.845925
step 6183, loss = 1.21264
step 6184, loss = 0.970666
step 6185, loss = 0.835293
step 6186, loss = 0.821229
step 6187, loss = 0.779151
step 6188, loss = 0.818042
step 6189, loss = 1.001
step 6190, loss = 0.824334
training 6191, accuracy = 0.544643
step 6191, loss = 0.849793
step 6192, loss = 0.867069
step 6193, loss = 0.79885
step 6194, loss = 0.927593
step 6195, loss = 0.816319
step 6196, loss = 0.762022
step 6197, loss = 0.799937
step 6198, loss = 0.771932
step 6199, loss = 0.903302
step 6200, loss = 0.865406
training 6201, accuracy = 0.383721
step 6201, loss = 0.775799
step 6202, loss = 0.878744
step 6203, loss = 0.929628
step 6204, loss = 0.76223
step 6205, loss = 1.7696
step 6206, loss = 1.67629
step 6207, loss = 1.58905
step 6208, loss = 0.928402
step 6209, loss = 0.924319
step 6210, loss = 0.930452
training 6211, accuracy = 0.833333
step 6211, loss = 1.11046
step 6212, loss = 0.769625
step 6213, loss = 0.781204
step 6214, loss = 0.805968
step 6215, loss = 1.05771
step 6216, loss = 1.16349
step 6217, loss = 0.784207
step 6218, loss = 1.01602
step 6219, loss = 0.925045
step 6220, loss = 0.873696
training 6221, accuracy = 1.000000
step 6221, loss = 0.876677
step 6222, loss = 0.881417
step 6223, loss = 0.803629
step 6224, loss = 0.838803
step 6225, loss = 0.806407
step 6226, loss = 0.870317
step 6227, loss = 0.723173
step 6228, loss = 0.82173
step 6229, loss = 0.826585
step 6230, loss = 0.872449
training 6231, accuracy = 0.923077
step 6231, loss = 0.91257
step 6232, loss = 0.74745
step 6233, loss = 0.809305
step 6234, loss = 0.744773
step 6235, loss = 0.776483
step 6236, loss = 0.78455
step 6237, loss = 0.846615
step 6238, loss = 0.855089
step 6239, loss = 1.11475
step 6240, loss = 0.937737
training 6241, accuracy = 0.339806
step 6241, loss = 0.950031
step 6242, loss = 0.823621
step 6243, loss = 0.798586
step 6244, loss = 0.790113
step 6245, loss = 0.862376
step 6246, loss = 0.763898
step 6247, loss = 0.817604
step 6248, loss = 1.13916
step 6249, loss = 1.36984
step 6250, loss = 0.856871
training 6251, accuracy = 0.666667
step 6251, loss = 0.750246
step 6252, loss = 0.901656
step 6253, loss = 0.991139
step 6254, loss = 0.901665
step 6255, loss = 0.87671
step 6256, loss = 0.755952
step 6257, loss = 0.876041
step 6258, loss = 0.755773
step 6259, loss = 0.865875
step 6260, loss = 0.804085
training 6261, accuracy = 0.162338
step 6261, loss = 0.825805
step 6262, loss = 0.748515
step 6263, loss = 0.786212
step 6264, loss = 0.87447
step 6265, loss = 0.737802
step 6266, loss = 0.735379
step 6267, loss = 0.770048
step 6268, loss = 0.891767
step 6269, loss = 0.856901
step 6270, loss = 0.746028
training 6271, accuracy = 0.463855
step 6271, loss = 0.765719
step 6272, loss = 0.773207
step 6273, loss = 0.771525
step 6274, loss = 0.912234
step 6275, loss = 0.784026
step 6276, loss = 0.908146
step 6277, loss = 0.883485
step 6278, loss = 0.893488
step 6279, loss = 0.808533
step 6280, loss = 0.784708
training 6281, accuracy = 0.904412
step 6281, loss = 1.05471
step 6282, loss = 0.718921
step 6283, loss = 0.777459
step 6284, loss = 0.898629
step 6285, loss = 0.875619
step 6286, loss = 0.780453
step 6287, loss = 1.33423
step 6288, loss = 0.958259
step 6289, loss = 1.00954
step 6290, loss = 0.993261
training 6291, accuracy = 0.727273
step 6291, loss = 0.758246
step 6292, loss = 0.787925
step 6293, loss = 0.864718
step 6294, loss = 0.818167
step 6295, loss = 0.89015
step 6296, loss = 0.781508
step 6297, loss = 0.827862
step 6298, loss = 0.787325
step 6299, loss = 0.769976
step 6300, loss = 0.743579
training 6301, accuracy = 0.329114
step 6301, loss = 0.846403
step 6302, loss = 0.797556
step 6303, loss = 0.774423
step 6304, loss = 0.85888
step 6305, loss = 0.809222
step 6306, loss = 1.13009
step 6307, loss = 0.85404
step 6308, loss = 0.878234
step 6309, loss = 0.803965
step 6310, loss = 0.849231
training 6311, accuracy = 1.000000
step 6311, loss = 0.749491
step 6312, loss = 0.723481
step 6313, loss = 0.789653
step 6314, loss = 0.834321
step 6315, loss = 0.819478
step 6316, loss = 0.858624
step 6317, loss = 0.790582
step 6318, loss = 0.969021
step 6319, loss = 0.797684
step 6320, loss = 0.852953
training 6321, accuracy = 1.000000
step 6321, loss = 0.80485
step 6322, loss = 0.731169
step 6323, loss = 0.847271
step 6324, loss = 0.807872
step 6325, loss = 0.853605
step 6326, loss = 0.920664
step 6327, loss = 0.829007
step 6328, loss = 1.19945
step 6329, loss = 0.770585
step 6330, loss = 0.806489
training 6331, accuracy = 0.745098
step 6331, loss = 0.778808
step 6332, loss = 0.760009
step 6333, loss = 0.798821
step 6334, loss = 0.784526
step 6335, loss = 0.797619
step 6336, loss = 0.790981
step 6337, loss = 0.798779
step 6338, loss = 0.760402
step 6339, loss = 0.764265
step 6340, loss = 0.782049
training 6341, accuracy = 0.796748
step 6341, loss = 0.764099
step 6342, loss = 0.805885
step 6343, loss = 0.733486
step 6344, loss = 0.79884
step 6345, loss = 0.743274
step 6346, loss = 0.999547
step 6347, loss = 0.908652
step 6348, loss = 0.767436
step 6349, loss = 0.760343
step 6350, loss = 0.755086
training 6351, accuracy = 0.635294
step 6351, loss = 0.809614
step 6352, loss = 0.745113
step 6353, loss = 0.733009
step 6354, loss = 0.839711
step 6355, loss = 0.764431
step 6356, loss = 0.758887
step 6357, loss = 0.762278
step 6358, loss = 1.15556
step 6359, loss = 0.750557
step 6360, loss = 0.799692
training 6361, accuracy = 0.277778
step 6361, loss = 0.757714
step 6362, loss = 0.843797
step 6363, loss = 0.865692
step 6364, loss = 0.903807
step 6365, loss = 0.790795
step 6366, loss = 0.812659
testing 1, accuracy = 0.490196
testing 2, accuracy = 0.723757
testing 3, accuracy = 0.759494
testing 4, accuracy = 0.157895
testing 5, accuracy = 0.853333
testing 6, accuracy = 0.127358
testing 7, accuracy = 0.834437
testing 8, accuracy = 0.178082
testing 9, accuracy = 0.369048
testing 10, accuracy = 0.483871
testing 11, accuracy = 0.313253
testing 12, accuracy = 0.460526
testing 13, accuracy = 0.264706
testing 14, accuracy = 0.800000
testing 15, accuracy = 0.760684
testing 16, accuracy = 0.557377
testing 17, accuracy = 0.404494
testing 18, accuracy = 0.886700
testing 19, accuracy = 0.310000
testing 20, accuracy = 0.809524
testing 21, accuracy = 0.441964
testing 22, accuracy = 0.909091
testing 23, accuracy = 0.174905
testing 24, accuracy = 0.764706
testing 25, accuracy = 0.842105
testing 26, accuracy = 0.500000
testing 27, accuracy = 0.361702
testing 28, accuracy = 0.888889
testing 29, accuracy = 0.701754
testing 30, accuracy = 0.666667
testing 31, accuracy = 0.941176
testing 32, accuracy = 0.491935
testing 33, accuracy = 0.658291
testing 34, accuracy = 0.675676
testing 35, accuracy = 0.057851
testing 36, accuracy = 0.415929
testing 37, accuracy = 0.071895
testing 38, accuracy = 0.673203
testing 39, accuracy = 0.723529
testing 40, accuracy = 0.711409
testing 41, accuracy = 0.591346
testing 42, accuracy = 0.078261
testing 43, accuracy = 0.253807
testing 44, accuracy = 0.340000
testing 45, accuracy = 0.381215
testing 46, accuracy = 0.463415
testing 47, accuracy = 0.162162
testing 48, accuracy = 0.747253
testing 49, accuracy = 0.897959
testing 50, accuracy = 0.870130
testing 51, accuracy = 0.945946
testing 52, accuracy = 0.584270
testing 53, accuracy = 0.766467
testing 54, accuracy = 0.694118
testing 55, accuracy = 0.695652
testing 56, accuracy = 0.732143
testing 57, accuracy = 0.153153
testing 58, accuracy = 0.727273
testing 59, accuracy = 0.924731
testing 60, accuracy = 0.525641
testing 61, accuracy = 0.328767
testing 62, accuracy = 0.746479
testing 63, accuracy = 0.458716
testing 64, accuracy = 0.409938
testing 65, accuracy = 0.340909
testing 66, accuracy = 0.776650
testing 67, accuracy = 0.568182
testing 68, accuracy = 0.466667
testing 69, accuracy = 0.607477
testing 70, accuracy = 0.340580
testing 71, accuracy = 0.540000
testing 72, accuracy = 0.402174
testing 73, accuracy = 0.338235
testing 74, accuracy = 0.280000
testing 75, accuracy = 0.524590
testing 76, accuracy = 0.959184
testing 77, accuracy = 0.634615
testing 78, accuracy = 0.274194
testing 79, accuracy = 0.771084
testing 80, accuracy = 0.750000
testing 81, accuracy = 0.463415
testing 82, accuracy = 0.983051
testing 83, accuracy = 0.642202
testing 84, accuracy = 0.536232
testing 85, accuracy = 0.615385
testing 86, accuracy = 0.564103
testing 87, accuracy = 0.821429
testing 88, accuracy = 0.511905
testing 89, accuracy = 0.925000
testing 90, accuracy = 0.600000
testing 91, accuracy = 0.863014
testing 92, accuracy = 0.584211
testing 93, accuracy = 0.748031
testing 94, accuracy = 0.804270
testing 95, accuracy = 0.708075
testing 96, accuracy = 0.430657
testing 97, accuracy = 0.270142
testing 98, accuracy = 0.833333
testing 99, accuracy = 0.293578
testing 100, accuracy = 0.806452
testing 101, accuracy = 0.700787
testing 102, accuracy = 0.840000
testing 103, accuracy = 0.293651
testing 104, accuracy = 0.702970
testing 105, accuracy = 0.814815
testing 106, accuracy = 0.824324
testing 107, accuracy = 0.102041
testing 108, accuracy = 0.338710
testing 109, accuracy = 0.695652
testing 110, accuracy = 0.687500
testing 111, accuracy = 0.295652
testing 112, accuracy = 0.045113
testing 113, accuracy = 0.126437
testing 114, accuracy = 0.855263
testing 115, accuracy = 0.759259
testing 116, accuracy = 0.222222
testing 117, accuracy = 0.884615
testing 118, accuracy = 0.429907
testing 119, accuracy = 0.203125
testing 120, accuracy = 0.316327
testing 121, accuracy = 0.602740
testing 122, accuracy = 0.090909
testing 123, accuracy = 0.588710
testing 124, accuracy = 0.600000
testing 125, accuracy = 0.347222
testing 126, accuracy = 0.015686
testing 127, accuracy = 0.138728
testing 128, accuracy = 0.830000
testing 129, accuracy = 0.657895
testing 130, accuracy = 0.529412
testing 131, accuracy = 0.846154
testing 132, accuracy = 0.571429
testing 133, accuracy = 0.357664
testing 134, accuracy = 0.754967
testing 135, accuracy = 0.541667
testing 136, accuracy = 0.830189
testing 137, accuracy = 0.754717
testing 138, accuracy = 0.462185
testing 139, accuracy = 0.277778
testing 140, accuracy = 0.592105
testing 141, accuracy = 0.289617
testing 142, accuracy = 0.078571
testing 143, accuracy = 0.909091
testing 144, accuracy = 0.707317
testing 145, accuracy = 0.897436
testing 146, accuracy = 0.247706
testing 147, accuracy = 0.210000
testing 148, accuracy = 0.172619
testing 149, accuracy = 0.547445
testing 150, accuracy = 0.863636
testing 151, accuracy = 0.104651
testing 152, accuracy = 0.622807
testing 153, accuracy = 0.740260
testing 154, accuracy = 0.646552
testing 155, accuracy = 0.926829
testing 156, accuracy = 0.303371
testing 157, accuracy = 0.777778
testing 158, accuracy = 0.513089
testing 159, accuracy = 0.696078
testing 160, accuracy = 0.663551
testing 161, accuracy = 0.198630
testing 162, accuracy = 0.701493
testing 163, accuracy = 0.252336
testing 164, accuracy = 0.654545
testing 165, accuracy = 0.687500
testing 166, accuracy = 0.634146
testing 167, accuracy = 0.410714
testing 168, accuracy = 0.872727
testing 169, accuracy = 0.615385
testing 170, accuracy = 0.785714
testing 171, accuracy = 0.659341
testing 172, accuracy = 0.237624
testing 173, accuracy = 0.293210
testing 174, accuracy = 0.766520
testing 175, accuracy = 0.891892
testing 176, accuracy = 0.486486
testing 177, accuracy = 0.263158
testing 178, accuracy = 0.673469
testing 179, accuracy = 0.750000
testing 180, accuracy = 0.583333
testing 181, accuracy = 0.325397
testing 182, accuracy = 0.069307
testing 183, accuracy = 0.906250
testing 184, accuracy = 0.741667
testing 185, accuracy = 0.786047
testing 186, accuracy = 0.521277
testing 187, accuracy = 0.764706
testing 188, accuracy = 0.765625
testing 189, accuracy = 0.833333
testing 190, accuracy = 0.965517
testing 191, accuracy = 0.846154
testing 192, accuracy = 0.830357
testing 193, accuracy = 0.698276
testing 194, accuracy = 0.683544
testing 195, accuracy = 0.190000
testing 196, accuracy = 0.280000
testing 197, accuracy = 0.821782
testing 198, accuracy = 0.468254
testing 199, accuracy = 0.401709
testing 200, accuracy = 0.365385
testing 201, accuracy = 0.723618
testing 202, accuracy = 0.460000
testing 203, accuracy = 0.843137
testing 204, accuracy = 0.804469
testing 205, accuracy = 1.000000
testing 206, accuracy = 0.707447
testing 207, accuracy = 0.246914
testing 208, accuracy = 0.484848
testing 209, accuracy = 0.406250
testing 210, accuracy = 0.200000
testing 211, accuracy = 0.850467
testing 212, accuracy = 0.945946
testing 213, accuracy = 0.700000
testing 214, accuracy = 0.379032
testing 215, accuracy = 0.559055
testing 216, accuracy = 0.712963
testing 217, accuracy = 0.733333
testing 218, accuracy = 0.091743
testing 219, accuracy = 0.803738
testing 220, accuracy = 0.056000
testing 221, accuracy = 0.272727
testing 222, accuracy = 1.000000
testing 223, accuracy = 0.326531
testing 224, accuracy = 0.315789
testing 225, accuracy = 0.635514
testing 226, accuracy = 0.703704
testing 227, accuracy = 0.336000
testing 228, accuracy = 0.501901
testing 229, accuracy = 0.615385
testing 230, accuracy = 0.761194
testing 231, accuracy = 0.694915
testing 232, accuracy = 0.185567
testing 233, accuracy = 0.526882
testing 234, accuracy = 0.780488
testing 235, accuracy = 0.747664
testing 236, accuracy = 0.717557
testing 237, accuracy = 0.864078
testing 238, accuracy = 0.720000
testing 239, accuracy = 0.483871
testing 240, accuracy = 0.192308
testing 241, accuracy = 0.880597
testing 242, accuracy = 0.136054
testing 243, accuracy = 0.786260
testing 244, accuracy = 0.514706
testing 245, accuracy = 0.480000
testing 246, accuracy = 0.881579
testing 247, accuracy = 0.575000
testing 248, accuracy = 0.769231
testing 249, accuracy = 0.762431
testing 250, accuracy = 0.781457
testing 251, accuracy = 0.715596
testing 252, accuracy = 0.477273
testing 253, accuracy = 0.404762
testing 254, accuracy = 0.566372
testing 255, accuracy = 0.466019
testing 256, accuracy = 0.586207
testing 257, accuracy = 0.659574
testing 258, accuracy = 0.528090
testing 259, accuracy = 0.170455
testing 260, accuracy = 0.728477
testing 261, accuracy = 0.746479
testing 262, accuracy = 0.816327
testing 263, accuracy = 0.746269
testing 264, accuracy = 0.022099
testing 265, accuracy = 0.891304
testing 266, accuracy = 0.328571
testing 267, accuracy = 0.724324
testing 268, accuracy = 0.320809
testing 269, accuracy = 0.685345
testing 270, accuracy = 0.395210
testing 271, accuracy = 0.439024
testing 272, accuracy = 0.200000
testing 273, accuracy = 0.306050
testing 274, accuracy = 0.952381
testing 275, accuracy = 0.452128
testing 276, accuracy = 0.286458
testing 277, accuracy = 0.287770
testing 278, accuracy = 0.245098
testing 279, accuracy = 0.324324
testing 280, accuracy = 0.925000
testing 281, accuracy = 0.614458
testing 282, accuracy = 0.700855
testing 283, accuracy = 0.857143
testing 284, accuracy = 0.674641
testing 285, accuracy = 0.256881
testing 286, accuracy = 0.759494
testing 287, accuracy = 0.260331
testing 288, accuracy = 0.446970
testing 289, accuracy = 0.826087
testing 290, accuracy = 0.828877
testing 291, accuracy = 0.284483
testing 292, accuracy = 0.975610
testing 293, accuracy = 0.639535
testing 294, accuracy = 0.715909
testing 295, accuracy = 0.784615
testing 296, accuracy = 0.432432
testing 297, accuracy = 0.651685
testing 298, accuracy = 0.589404
testing 299, accuracy = 0.769231
testing 300, accuracy = 0.208333
testing 301, accuracy = 0.894737
testing 302, accuracy = 0.419355
testing 303, accuracy = 0.434783
testing 304, accuracy = 0.814815
testing 305, accuracy = 0.659574
testing 306, accuracy = 0.410714
testing 307, accuracy = 0.868613
testing 308, accuracy = 0.378641
testing 309, accuracy = 0.438017
testing 310, accuracy = 0.188525
testing 311, accuracy = 0.291429
testing 312, accuracy = 0.288770
testing 313, accuracy = 0.419643
testing 314, accuracy = 0.804054
testing 315, accuracy = 0.157534
testing 316, accuracy = 0.761194
testing 317, accuracy = 0.870370
testing 318, accuracy = 0.425532
testing 319, accuracy = 0.722772
testing 320, accuracy = 0.265403
testing 321, accuracy = 0.293578
testing 322, accuracy = 0.543860
testing 323, accuracy = 0.784722
testing 324, accuracy = 0.661616
testing 325, accuracy = 0.675393
testing 326, accuracy = 0.387755
testing 327, accuracy = 0.087156
testing 328, accuracy = 0.580645
testing 329, accuracy = 0.769231
testing 330, accuracy = 0.333333
testing 331, accuracy = 0.634146
testing 332, accuracy = 0.903846
testing 333, accuracy = 0.891566
testing 334, accuracy = 0.736264
testing 335, accuracy = 0.255814
testing 336, accuracy = 0.800000
testing 337, accuracy = 0.571429
testing 338, accuracy = 0.819277
testing 339, accuracy = 0.794872
testing 340, accuracy = 0.754717
testing 341, accuracy = 0.750000
testing 342, accuracy = 0.780488
testing 343, accuracy = 0.814516
testing 344, accuracy = 0.767442
testing 345, accuracy = 0.959184
testing 346, accuracy = 0.824561
testing 347, accuracy = 0.452381
testing 348, accuracy = 0.821429
testing 349, accuracy = 0.892857
testing 350, accuracy = 0.467890
testing 351, accuracy = 0.741176
testing 352, accuracy = 0.732394
testing 353, accuracy = 0.479167
testing 354, accuracy = 0.598291
testing 355, accuracy = 0.509434
testing 356, accuracy = 0.842105
testing 357, accuracy = 0.707547
testing 358, accuracy = 0.891304
testing 359, accuracy = 0.779874
testing 360, accuracy = 1.000000
testing 361, accuracy = 0.434783
testing 362, accuracy = 0.769231
testing 363, accuracy = 0.350000
testing 364, accuracy = 0.373737
testing 365, accuracy = 0.774775
testing 366, accuracy = 0.777778
testing 367, accuracy = 0.461290
testing 368, accuracy = 0.814815
testing 369, accuracy = 0.869565
testing 370, accuracy = 0.947368
testing 371, accuracy = 0.619048
testing 372, accuracy = 0.428571
testing 373, accuracy = 0.841584
testing 374, accuracy = 0.432203
testing 375, accuracy = 0.683544
testing 376, accuracy = 0.691244
testing 377, accuracy = 0.710744
testing 378, accuracy = 0.634146
testing 379, accuracy = 0.910000
testing 380, accuracy = 0.676056
testing 381, accuracy = 0.747573
testing 382, accuracy = 0.880795
testing 383, accuracy = 0.754717
testing 384, accuracy = 0.283898
testing 385, accuracy = 0.728571
testing 386, accuracy = 0.970588
testing 387, accuracy = 1.000000
testing 388, accuracy = 0.854839
testing 389, accuracy = 0.148936
testing 390, accuracy = 0.846939
testing 391, accuracy = 0.564706
testing 392, accuracy = 0.225000
testing 393, accuracy = 0.551282
testing 394, accuracy = 0.433962
testing 395, accuracy = 0.485714
testing 396, accuracy = 0.857143
testing 397, accuracy = 0.745902
testing 398, accuracy = 0.475410
testing 399, accuracy = 0.729927
testing 400, accuracy = 0.769737
testing 401, accuracy = 0.773723
testing 402, accuracy = 0.730159
testing 403, accuracy = 0.783784
testing 404, accuracy = 0.500000
testing 405, accuracy = 0.585714
testing 406, accuracy = 0.483333
testing 407, accuracy = 0.333333
testing 408, accuracy = 0.729885
testing 409, accuracy = 0.298851
testing 410, accuracy = 0.657277
testing 411, accuracy = 0.895522
testing 412, accuracy = 0.500000
testing 413, accuracy = 0.638462
testing 414, accuracy = 0.625850
testing 415, accuracy = 0.806452
testing 416, accuracy = 0.562500
testing 417, accuracy = 0.494253
testing 418, accuracy = 0.966102
testing 419, accuracy = 0.718310
testing 420, accuracy = 0.530612
testing 421, accuracy = 0.360544
testing 422, accuracy = 0.698630
testing 423, accuracy = 0.654545
testing 424, accuracy = 0.968750
testing 425, accuracy = 0.539683
testing 426, accuracy = 0.566667
testing 427, accuracy = 0.809524
testing 428, accuracy = 0.075145
testing 429, accuracy = 0.617188
testing 430, accuracy = 0.698020
testing 431, accuracy = 0.589744
testing 432, accuracy = 0.685185
testing 433, accuracy = 0.427350
testing 434, accuracy = 0.831461
testing 435, accuracy = 0.333333
testing 436, accuracy = 0.836879
testing 437, accuracy = 0.337500
testing 438, accuracy = 0.820225
testing 439, accuracy = 0.513889
testing 440, accuracy = 0.777778
testing 441, accuracy = 0.728889
testing 442, accuracy = 0.741722
testing 443, accuracy = 0.949367
testing 444, accuracy = 0.371429
testing 445, accuracy = 0.493333
testing 446, accuracy = 0.962963
testing 447, accuracy = 0.356383
testing 448, accuracy = 0.675978
testing 449, accuracy = 0.703125
testing 450, accuracy = 0.235294
testing 451, accuracy = 0.201550
testing 452, accuracy = 0.522727
testing 453, accuracy = 0.295082
testing 454, accuracy = 0.605042
testing 455, accuracy = 0.659794
testing 456, accuracy = 0.130000
testing 457, accuracy = 0.224299
testing 458, accuracy = 0.862745
testing 459, accuracy = 0.555556
testing 460, accuracy = 0.666667
testing 461, accuracy = 0.142857
testing 462, accuracy = 0.661972
testing 463, accuracy = 0.367347
testing 464, accuracy = 0.747126
testing 465, accuracy = 0.654545
testing 466, accuracy = 0.706383
testing 467, accuracy = 0.719298
testing 468, accuracy = 0.611702
testing 469, accuracy = 0.756757
testing 470, accuracy = 0.186916
testing 471, accuracy = 0.415385
testing 472, accuracy = 0.785714
testing 473, accuracy = 0.833333
testing 474, accuracy = 0.378151
testing 475, accuracy = 0.888889
testing 476, accuracy = 0.600000
testing 477, accuracy = 0.692982
testing 478, accuracy = 0.697143
testing 479, accuracy = 0.656904
testing 480, accuracy = 0.722628
testing 481, accuracy = 0.887324
testing 482, accuracy = 0.711111
testing 483, accuracy = 0.676471
testing 484, accuracy = 0.384615
testing 485, accuracy = 0.368421
testing 486, accuracy = 0.840637
testing 487, accuracy = 0.620690
testing 488, accuracy = 0.689655
testing 489, accuracy = 0.828947
testing 490, accuracy = 0.637931
testing 491, accuracy = 0.856410
testing 492, accuracy = 0.530612
testing 493, accuracy = 0.530806
testing 494, accuracy = 0.821429
testing 495, accuracy = 0.166667
testing 496, accuracy = 0.750000
testing 497, accuracy = 0.816092
testing 498, accuracy = 0.191083
testing 499, accuracy = 0.343590
testing 500, accuracy = 0.184211
